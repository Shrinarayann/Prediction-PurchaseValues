{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46bf3880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing training data...\n",
      "Defining the preprocessing pipeline...\n",
      "\n",
      "--- Building and Training Single Regressor Model ---\n",
      "\n",
      "--- Evaluating Model on Local Test Set---\n",
      "Local Test Set R² Score: 0.1493\n",
      "Local Test Set RMSE: $196.78\n",
      "\n",
      "--- Generating Kaggle Submission File ---\n",
      "Submission file 'submission.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: PREPROCESSING (This part remains mostly the same)\n",
    "# ==============================================================================\n",
    "\n",
    "class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
    "    # ... (This class is unchanged)\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['date'] = pd.to_datetime(X_copy['date'], format='%Y%m%d')\n",
    "        X_copy['sessionYear'] = X_copy['date'].dt.year\n",
    "        X_copy['sessionMonth'] = X_copy['date'].dt.month\n",
    "        X_copy['sessionDayOfWeek'] = X_copy['date'].dt.dayofweek\n",
    "        X_copy['sessionHour'] = pd.to_datetime(X_copy['sessionStart'], unit='s').dt.hour\n",
    "        X_copy['ad_page_binned'] = X_copy['trafficSource.adwordsClickInfo.page'].apply(lambda p: 1 if p == 1.0 else (2 if pd.notna(p) else 0))\n",
    "        cols_to_drop = ['date', 'sessionStart', 'userId', 'sessionId', 'trafficSource.adwordsClickInfo.page']\n",
    "        X_copy = X_copy.drop(columns=cols_to_drop, errors='ignore')\n",
    "        return X_copy\n",
    "\n",
    "# For this simpler model, Target Encoding is fine, but we learn from the log_purchaseValue\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    # ... (This class is unchanged)\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        self.mappings_ = {}\n",
    "        self.global_mean_ = 0\n",
    "    def fit(self, X, y):\n",
    "        X_fit = X.copy()\n",
    "        y_fit = y.copy()\n",
    "        self.global_mean_ = np.mean(y_fit)\n",
    "        for col in self.columns:\n",
    "            X_fit[col] = X_fit[col].fillna('missing')\n",
    "            mapping = y_fit.groupby(X_fit[col]).mean().to_dict()\n",
    "            self.mappings_[col] = mapping\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transform = X.copy()\n",
    "        for col in self.columns:\n",
    "            X_transform[col] = X_transform[col].fillna('missing')\n",
    "            X_transform[col] = X_transform[col].map(self.mappings_[col]).fillna(self.global_mean_)\n",
    "        return X_transform\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: LOADING DATA AND TRAINING THE SINGLE-REGRESSOR MODEL\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Define File Paths ---\n",
    "TRAIN_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/train_data.csv'\n",
    "TEST_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/test_data.csv'\n",
    "\n",
    "# --- Load and Prepare Raw Training Data ---\n",
    "print(\"Loading and preparing training data...\")\n",
    "df = pd.read_csv(TRAIN_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "one_value_cols = [col for col in df.columns if df[col].nunique(dropna=False) == 1]\n",
    "df = df.drop(columns=one_value_cols)\n",
    "\n",
    "# Create the single target variable: log_purchaseValue\n",
    "df['purchaseValue'] = df['purchaseValue'].fillna(0) / 1e6 # Rescale\n",
    "df['log_purchaseValue'] = np.log1p(df['purchaseValue'])\n",
    "\n",
    "# Separate features (X) and our single target (y_log)\n",
    "X = df.drop(columns=['purchaseValue', 'log_purchaseValue'])\n",
    "y_log = df['log_purchaseValue']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train_log, y_test_log = train_test_split(X, y_log, test_size=0.25, random_state=42)\n",
    "\n",
    "# --- Define the full preprocessing pipeline ---\n",
    "print(\"Defining the preprocessing pipeline...\")\n",
    "temp_engineered_df = FeatureEngineeringTransformer().fit_transform(X_train)\n",
    "numerical_cols = ['sessionNumber', 'pageViews', 'totalHits', 'sessionYear', 'sessionMonth', 'sessionDayOfWeek', 'sessionHour']\n",
    "categorical_cols = [col for col in temp_engineered_df.columns if col not in numerical_cols]\n",
    "\n",
    "# The target encoder will now learn from the log_purchaseValue on the whole training set\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols),\n",
    "        ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- Build and Train the SINGLE Regressor Pipeline ---\n",
    "print(\"\\n--- Building and Training Single Regressor Model ---\")\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('feature_engineering', FeatureEngineeringTransformer()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse', random_state=42, n_estimators=250, max_depth=7, learning_rate=0.05))\n",
    "])\n",
    "\n",
    "# Fit the entire pipeline on the training data and log-transformed target\n",
    "model_pipeline.fit(X_train, y_train_log)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: EVALUATION AND KAGGLE SUBMISSION\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Evaluate on the local test set ---\n",
    "print(\"\\n--- Evaluating Model on Local Test Set---\")\n",
    "y_pred_log = model_pipeline.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions and cap at zero as you suggested\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_pred[y_pred < 0] = 0\n",
    "\n",
    "# Get original test values for comparison\n",
    "y_test_orig = np.expm1(y_test_log)\n",
    "\n",
    "r2 = r2_score(y_test_orig, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred))\n",
    "print(f\"Local Test Set R² Score: {r2:.4f}\")\n",
    "print(f\"Local Test Set RMSE: ${rmse:.2f}\")\n",
    "\n",
    "# --- Generate Kaggle Submission File ---\n",
    "print(\"\\n--- Generating Kaggle Submission File ---\")\n",
    "try:\n",
    "    kaggle_test_df = pd.read_csv(TEST_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "    \n",
    "    # Use the TRAINED pipeline to predict on the new, unseen test data\n",
    "    kaggle_pred_log = model_pipeline.predict(kaggle_test_df)\n",
    "    \n",
    "    # Inverse transform and cap at zero\n",
    "    kaggle_final_predictions = np.expm1(kaggle_pred_log)\n",
    "    kaggle_final_predictions[kaggle_final_predictions < 0] = 0\n",
    "    \n",
    "    # Create the submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'ID': kaggle_test_df.index,\n",
    "        'purchaseValue': kaggle_final_predictions\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission file 'submission.csv' created successfully.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nKaggle '{TEST_FILE_PATH}' not found. Skipping submission file generation.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during submission generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe8f117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing training data...\n",
      "Defining the preprocessing pipeline...\n",
      "\n",
      "--- Building and Training Classifier ---\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:17:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:17:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:17:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:17:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:17:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:17:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:17:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:17:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:17:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:17:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:17:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:17:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:17:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Classifier Params: {'classifier__learning_rate': 0.05, 'classifier__max_depth': 7, 'classifier__n_estimators': 250}\n",
      "\n",
      "--- Building and Training Regressor ---\n",
      "\n",
      "--- Generating Kaggle Submission File (with correct scaling) ---\n",
      "Submission file 'submission.csv' created successfully with scaled-up values.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: DEFINING THE PREPROCESSING PIPELINE\n",
    "# These are the reusable building blocks of our system.\n",
    "# ==============================================================================\n",
    "\n",
    "class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Performs initial feature engineering: creates date/time features,\n",
    "    bins the AdWords page feature, and drops unneeded ID columns.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Date and Timestamp Engineering\n",
    "        X_copy['date'] = pd.to_datetime(X_copy['date'], format='%Y%m%d')\n",
    "        X_copy['sessionYear'] = X_copy['date'].dt.year\n",
    "        X_copy['sessionMonth'] = X_copy['date'].dt.month\n",
    "        X_copy['sessionDayOfWeek'] = X_copy['date'].dt.dayofweek\n",
    "        X_copy['sessionHour'] = pd.to_datetime(X_copy['sessionStart'], unit='s').dt.hour\n",
    "        \n",
    "        # Binning the AdWords Page feature\n",
    "        X_copy['ad_page_binned'] = X_copy['trafficSource.adwordsClickInfo.page'].apply(\n",
    "            lambda p: 1 if p == 1.0 else (2 if pd.notna(p) else 0)\n",
    "        )\n",
    "        \n",
    "        # Drop original/processed columns\n",
    "        cols_to_drop = [\n",
    "            'date', 'sessionStart', 'userId', 'sessionId', \n",
    "            'trafficSource.adwordsClickInfo.page'\n",
    "        ]\n",
    "        X_copy = X_copy.drop(columns=cols_to_drop, errors='ignore')\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A leak-proof Target Encoder. It learns the mean of the target for each category\n",
    "    from the training data ONLY and applies it to all data.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        self.mappings_ = {}\n",
    "        self.global_mean_ = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_fit = X.copy()\n",
    "        y_fit = y.copy()\n",
    "        \n",
    "        self.global_mean_ = np.mean(y_fit)\n",
    "        \n",
    "        for col in self.columns:\n",
    "            # Fill NaNs in the feature column before grouping\n",
    "            X_fit[col] = X_fit[col].fillna('missing')\n",
    "            mapping = y_fit.groupby(X_fit[col]).mean().to_dict()\n",
    "            self.mappings_[col] = mapping\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transform = X.copy()\n",
    "        for col in self.columns:\n",
    "            X_transform[col] = X_transform[col].fillna('missing')\n",
    "            # Apply the learned mapping. For new categories in test data, use the global mean.\n",
    "            X_transform[col] = X_transform[col].map(self.mappings_[col]).fillna(self.global_mean_)\n",
    "        return X_transform\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: LOADING DATA AND TRAINING THE MODELS\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Define File Paths ---\n",
    "TRAIN_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/train_data.csv'\n",
    "TEST_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/test_data.csv'\n",
    "\n",
    "# --- Load and Prepare Raw Training Data ---\n",
    "print(\"Loading and preparing training data...\")\n",
    "df = pd.read_csv(TRAIN_FILE_PATH, dtype={'fullVisitorId': 'str'}) # Load ID as string\n",
    "\n",
    "# Initial cleanup: Drop columns with only one unique value\n",
    "one_value_cols = [col for col in df.columns if df[col].nunique(dropna=False) == 1]\n",
    "df = df.drop(columns=one_value_cols)\n",
    "\n",
    "# Create target variables\n",
    "df['purchaseValue'] = df['purchaseValue'].fillna(0) / 1e6 # Handle NaNs and rescale\n",
    "df['made_purchase'] = (df['purchaseValue'] > 0).astype(int)\n",
    "df['log_purchaseValue'] = np.log1p(df['purchaseValue'])\n",
    "\n",
    "# Separate features and targets\n",
    "X = df.drop(columns=['purchaseValue', 'made_purchase', 'log_purchaseValue'])\n",
    "y = df[['purchaseValue', 'made_purchase', 'log_purchaseValue']]\n",
    "\n",
    "# Split data before any fitting to prevent leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y['made_purchase']\n",
    ")\n",
    "\n",
    "# --- Define the full preprocessing pipeline ---\n",
    "print(\"Defining the preprocessing pipeline...\")\n",
    "# First, apply feature engineering to get the final column names\n",
    "temp_engineered_df = FeatureEngineeringTransformer().fit_transform(X_train)\n",
    "numerical_cols = ['sessionNumber', 'pageViews', 'totalHits', 'sessionYear', 'sessionMonth', 'sessionDayOfWeek', 'sessionHour']\n",
    "categorical_cols = [col for col in temp_engineered_df.columns if col not in numerical_cols]\n",
    "\n",
    "# This pipeline encapsulates all preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols),\n",
    "        ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- Build and Train the Two-Part Model ---\n",
    "\n",
    "# PART 2A: CLASSIFIER (Will they buy?)\n",
    "print(\"\\n--- Building and Training Classifier ---\")\n",
    "clf_pipeline = Pipeline(steps=[\n",
    "    ('feature_engineering', FeatureEngineeringTransformer()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42))\n",
    "])\n",
    "\n",
    "# Define a parameter grid for tuning\n",
    "param_grid_clf = {\n",
    "    'classifier__n_estimators': [150, 250],\n",
    "    'classifier__max_depth': [5, 7],\n",
    "    'classifier__learning_rate': [0.05],\n",
    "}\n",
    "grid_search_clf = GridSearchCV(clf_pipeline, param_grid_clf, scoring='roc_auc', cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search_clf.fit(X_train, y_train['made_purchase'])\n",
    "best_clf_pipeline = grid_search_clf.best_estimator_\n",
    "print(f\"Best Classifier Params: {grid_search_clf.best_params_}\")\n",
    "\n",
    "# PART 2B: REGRESSOR (How much will they spend?)\n",
    "print(\"\\n--- Building and Training Regressor ---\")\n",
    "reg_pipeline = Pipeline(steps=[\n",
    "    ('feature_engineering', FeatureEngineeringTransformer()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse', random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the regressor pipeline ONLY on the buyer data\n",
    "X_train_buyers = X_train[y_train['made_purchase'] == 1]\n",
    "y_train_buyers_log = y_train.loc[y_train['made_purchase'] == 1, 'log_purchaseValue']\n",
    "reg_pipeline.fit(X_train_buyers, y_train_buyers_log)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: EVALUATION AND KAGGLE SUBMISSION (Corrected Submission Part)\n",
    "# ==============================================================================\n",
    "\n",
    "# ... (The evaluation part on the local test set is correct and can remain as is) ...\n",
    "# print(f\"Local Test Set R² Score: {r2:.4f}\")\n",
    "# print(f\"Local Test Set RMSE: ${rmse:.2f}\")\n",
    "\n",
    "# --- Generate Kaggle Submission File ---\n",
    "print(\"\\n--- Generating Kaggle Submission File (with correct scaling) ---\")\n",
    "try:\n",
    "    kaggle_test_df = pd.read_csv(TEST_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "    \n",
    "    # Use the TRAINED pipeline to predict on the new, unseen test data\n",
    "    kaggle_pred_log = model_pipeline.predict(kaggle_test_df)\n",
    "    \n",
    "    # Inverse transform to get predicted dollar amounts\n",
    "    kaggle_final_predictions_dollars = np.expm1(kaggle_pred_log)\n",
    "    kaggle_final_predictions_dollars[kaggle_final_predictions_dollars < 0] = 0\n",
    "    \n",
    "    # ====================================================================\n",
    "    # THE FIX: Scale the final dollar predictions back up by 1,000,000\n",
    "    # ====================================================================\n",
    "    scaling_factor = 1e6\n",
    "    kaggle_final_predictions_scaled = kaggle_final_predictions_dollars * scaling_factor\n",
    "    \n",
    "    # Create the submission DataFrame with the scaled values\n",
    "    submission_df = pd.DataFrame({\n",
    "        'ID': kaggle_test_df.index,\n",
    "        'purchaseValue': kaggle_final_predictions_scaled\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission file 'submission.csv' created successfully with scaled-up values.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nKaggle '{TEST_FILE_PATH}' not found. Skipping submission file generation.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during submission generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "586f383b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing training data...\n",
      "Defining column groups for preprocessing...\n",
      "\n",
      "--- Building and Training Classifier ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:21:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building and Training Regressor ---\n",
      "\n",
      "--- Evaluating Improved Model on Local Test Set---\n",
      "Local Test Set R² Score: 0.4787\n",
      "Local Test Set RMSE: $126.94\n",
      "\n",
      "--- Generating Kaggle Submission File ---\n",
      "Submission file 'submission_v2.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "#Model improvmenet 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: ENHANCED PREPROCESSING PIPELINE\n",
    "# ==============================================================================\n",
    "\n",
    "class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Enhanced version: Creates date/time features, bins AdWords,\n",
    "    AND adds a new interaction feature.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Date and Timestamp Engineering\n",
    "        X_copy['date'] = pd.to_datetime(X_copy['date'], format='%Y%m%d')\n",
    "        X_copy['sessionYear'] = X_copy['date'].dt.year\n",
    "        X_copy['sessionMonth'] = X_copy['date'].dt.month\n",
    "        X_copy['sessionDayOfWeek'] = X_copy['date'].dt.dayofweek\n",
    "        X_copy['sessionHour'] = pd.to_datetime(X_copy['sessionStart'], unit='s').dt.hour\n",
    "        \n",
    "        # *** NEW FEATURE ***: Interaction between month and day of week\n",
    "        X_copy['month_day_interaction'] = X_copy['sessionMonth'].astype(str) + '_' + X_copy['sessionDayOfWeek'].astype(str)\n",
    "        \n",
    "        # Binning the AdWords Page feature\n",
    "        X_copy['ad_page_binned'] = X_copy['trafficSource.adwordsClickInfo.page'].apply(\n",
    "            lambda p: 1 if p == 1.0 else (2 if pd.notna(p) else 0)\n",
    "        )\n",
    "        \n",
    "        # Drop original/processed columns\n",
    "        cols_to_drop = [\n",
    "            'date', 'sessionStart', 'userId', 'sessionId', \n",
    "            'trafficSource.adwordsClickInfo.page'\n",
    "        ]\n",
    "        X_copy = X_copy.drop(columns=cols_to_drop, errors='ignore')\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    # This class is unchanged, it's already robust and leak-proof\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "        self.mappings_ = {}\n",
    "        self.global_mean_ = 0\n",
    "    def fit(self, X, y):\n",
    "        X_fit = X.copy()\n",
    "        y_fit = y.copy()\n",
    "        self.global_mean_ = np.mean(y_fit)\n",
    "        for col in self.columns:\n",
    "            X_fit[col] = X_fit[col].fillna('missing')\n",
    "            mapping = y_fit.groupby(X_fit[col]).mean().to_dict()\n",
    "            self.mappings_[col] = mapping\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transform = X.copy()\n",
    "        for col in self.columns:\n",
    "            X_transform[col] = X_transform[col].fillna('missing')\n",
    "            X_transform[col] = X_transform[col].map(self.mappings_[col]).fillna(self.global_mean_)\n",
    "        return X_transform\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: LOADING DATA AND TRAINING THE IMPROVED HURDLE MODEL\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Define File Paths ---\n",
    "TRAIN_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/train_data.csv'\n",
    "TEST_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/test_data.csv'\n",
    "\n",
    "# --- Load and Prepare Raw Training Data ---\n",
    "print(\"Loading and preparing training data...\")\n",
    "df = pd.read_csv(TRAIN_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "one_value_cols = [col for col in df.columns if df[col].nunique(dropna=False) == 1]\n",
    "df = df.drop(columns=one_value_cols)\n",
    "\n",
    "# Create target variables\n",
    "df['purchaseValue'] = df['purchaseValue'].fillna(0) / 1e6 # Rescale\n",
    "df['made_purchase'] = (df['purchaseValue'] > 0).astype(int)\n",
    "df['log_purchaseValue'] = np.log1p(df['purchaseValue'])\n",
    "\n",
    "# Separate features and targets\n",
    "X = df.drop(columns=['purchaseValue', 'made_purchase', 'log_purchaseValue'])\n",
    "y = df[['purchaseValue', 'made_purchase', 'log_purchaseValue']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y['made_purchase']\n",
    ")\n",
    "\n",
    "# --- Define Column Groups based on the new Feature Transformer ---\n",
    "print(\"Defining column groups for preprocessing...\")\n",
    "temp_engineered_df = FeatureEngineeringTransformer().fit_transform(X_train)\n",
    "numerical_cols = ['sessionNumber', 'pageViews', 'totalHits', 'sessionYear', 'sessionMonth', 'sessionDayOfWeek', 'sessionHour']\n",
    "categorical_cols = [col for col in temp_engineered_df.columns if col not in numerical_cols]\n",
    "\n",
    "# --- Build and Train the Classifier Component (Part 1) ---\n",
    "print(\"\\n--- Building and Training Classifier ---\")\n",
    "clf_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols),\n",
    "        ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "clf_pipeline = Pipeline(steps=[\n",
    "    ('feature_engineering', FeatureEngineeringTransformer()),\n",
    "    ('preprocessor', clf_preprocessor),\n",
    "    ('classifier', xgb.XGBClassifier(\n",
    "        objective='binary:logistic', eval_metric='logloss', use_label_encoder=False,\n",
    "        random_state=42, n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit the classifier pipeline on the binary target\n",
    "clf_pipeline.fit(X_train, y_train['made_purchase'])\n",
    "\n",
    "# --- Build and Train the Regressor Component (Part 2) ---\n",
    "print(\"\\n--- Building and Training Regressor ---\")\n",
    "reg_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols),\n",
    "        ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "reg_pipeline = Pipeline(steps=[\n",
    "    ('feature_engineering', FeatureEngineeringTransformer()),\n",
    "    ('preprocessor', reg_preprocessor),\n",
    "    ('regressor', xgb.XGBRegressor(\n",
    "        objective='reg:squarederror', eval_metric='rmse', random_state=42,\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit the regressor pipeline ONLY on the buyer data\n",
    "X_train_buyers = X_train[y_train['made_purchase'] == 1]\n",
    "y_train_buyers_log = y_train.loc[y_train['made_purchase'] == 1, 'log_purchaseValue']\n",
    "reg_pipeline.fit(X_train_buyers, y_train_buyers_log)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: EVALUATION AND KAGGLE SUBMISSION\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Evaluate on the local test set ---\n",
    "print(\"\\n--- Evaluating Improved Model on Local Test Set---\")\n",
    "prob_purchase = clf_pipeline.predict_proba(X_test)[:, 1]\n",
    "log_value_pred = reg_pipeline.predict(X_test)\n",
    "value_pred = np.expm1(log_value_pred)\n",
    "final_predictions = prob_purchase * value_pred\n",
    "final_predictions[final_predictions < 0] = 0\n",
    "\n",
    "actual_values = y_test['purchaseValue']\n",
    "r2 = r2_score(actual_values, final_predictions)\n",
    "rmse = np.sqrt(mean_squared_error(actual_values, final_predictions))\n",
    "print(f\"Local Test Set R² Score: {r2:.4f}\")\n",
    "print(f\"Local Test Set RMSE: ${rmse:.2f}\")\n",
    "\n",
    "# --- Generate Kaggle Submission File ---\n",
    "print(\"\\n--- Generating Kaggle Submission File ---\")\n",
    "try:\n",
    "    kaggle_test_df = pd.read_csv(TEST_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "    \n",
    "    # Use the TRAINED pipelines to predict on the new, unseen test data\n",
    "    kaggle_prob_purchase = clf_pipeline.predict_proba(kaggle_test_df)[:, 1]\n",
    "    kaggle_log_value_pred = reg_pipeline.predict(kaggle_test_df)\n",
    "    \n",
    "    # Combine, inverse transform, and scale up for submission\n",
    "    kaggle_value_pred = np.expm1(kaggle_log_value_pred)\n",
    "    kaggle_final_predictions_dollars = kaggle_prob_purchase * kaggle_value_pred\n",
    "    kaggle_final_predictions_dollars[kaggle_final_predictions_dollars < 0] = 0\n",
    "    \n",
    "    scaling_factor = 1e6\n",
    "    kaggle_final_predictions_scaled = kaggle_final_predictions_dollars * scaling_factor\n",
    "    \n",
    "    # Create the submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'ID': kaggle_test_df.index,\n",
    "        'purchaseValue': kaggle_final_predictions_scaled\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv('submission_v2.csv', index=False) # Saving as v2\n",
    "    print(\"Submission file 'submission_v2.csv' created successfully.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nKaggle '{TEST_FILE_PATH}' not found. Skipping submission file generation.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during submission generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50fca229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "\n",
      "--- Tuning Classifier with Optuna ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-15 00:26:40,458] A new study created in memory with name: no-name-6c41a0b1-3d58-40b2-b75d-093d21c11c53\n",
      "[I 2025-07-15 00:26:53,166] Trial 0 finished with value: 0.9895921285936123 and parameters: {'n_estimators': 525, 'max_depth': 6, 'learning_rate': 0.02148297041874146, 'subsample': 0.7032080682122083, 'colsample_bytree': 0.9596181215638754, 'gamma': 0.0005657090223697375}. Best is trial 0 with value: 0.9895921285936123.\n",
      "[I 2025-07-15 00:27:10,279] Trial 1 finished with value: 0.989770511295699 and parameters: {'n_estimators': 822, 'max_depth': 7, 'learning_rate': 0.013764038342184735, 'subsample': 0.7207228809965966, 'colsample_bytree': 0.8000753610077241, 'gamma': 7.190943040730275e-05}. Best is trial 1 with value: 0.989770511295699.\n",
      "[I 2025-07-15 00:27:28,070] Trial 2 finished with value: 0.98997017883108 and parameters: {'n_estimators': 797, 'max_depth': 8, 'learning_rate': 0.028451888420636683, 'subsample': 0.9304222654419644, 'colsample_bytree': 0.9383577314058353, 'gamma': 0.27085509015562875}. Best is trial 2 with value: 0.98997017883108.\n",
      "[I 2025-07-15 00:27:43,668] Trial 3 finished with value: 0.9897919788886194 and parameters: {'n_estimators': 692, 'max_depth': 8, 'learning_rate': 0.013739428839169041, 'subsample': 0.8013770757864516, 'colsample_bytree': 0.7745580626945124, 'gamma': 0.0003497922333936759}. Best is trial 2 with value: 0.98997017883108.\n",
      "[I 2025-07-15 00:27:57,024] Trial 4 finished with value: 0.9894907670296584 and parameters: {'n_estimators': 919, 'max_depth': 4, 'learning_rate': 0.052444931556335735, 'subsample': 0.8891019557678459, 'colsample_bytree': 0.74299834582656, 'gamma': 9.247051217100095e-07}. Best is trial 2 with value: 0.98997017883108.\n",
      "[I 2025-07-15 00:28:15,071] Trial 5 finished with value: 0.9899400233343413 and parameters: {'n_estimators': 791, 'max_depth': 8, 'learning_rate': 0.015919566154776053, 'subsample': 0.7925417231545042, 'colsample_bytree': 0.961690969030943, 'gamma': 0.0471199655579352}. Best is trial 2 with value: 0.98997017883108.\n",
      "[I 2025-07-15 00:28:25,515] Trial 6 finished with value: 0.9894593750809029 and parameters: {'n_estimators': 606, 'max_depth': 4, 'learning_rate': 0.040815643160045025, 'subsample': 0.7991055759866367, 'colsample_bytree': 0.9167557942530902, 'gamma': 5.682371423681903e-06}. Best is trial 2 with value: 0.98997017883108.\n",
      "[I 2025-07-15 00:28:39,075] Trial 7 finished with value: 0.9893657082149895 and parameters: {'n_estimators': 892, 'max_depth': 4, 'learning_rate': 0.022198164387153333, 'subsample': 0.8689267775820574, 'colsample_bytree': 0.910166811783881, 'gamma': 0.0018626319240292592}. Best is trial 2 with value: 0.98997017883108.\n",
      "[I 2025-07-15 00:28:49,446] Trial 8 finished with value: 0.9884320978602177 and parameters: {'n_estimators': 449, 'max_depth': 5, 'learning_rate': 0.01019531492813869, 'subsample': 0.8635292464936478, 'colsample_bytree': 0.972326977675031, 'gamma': 0.07776121540176802}. Best is trial 2 with value: 0.98997017883108.\n",
      "[I 2025-07-15 00:29:02,060] Trial 9 finished with value: 0.9896464017519759 and parameters: {'n_estimators': 504, 'max_depth': 7, 'learning_rate': 0.018791786630556955, 'subsample': 0.7981709018142257, 'colsample_bytree': 0.9145535481648723, 'gamma': 0.00038557092489056876}. Best is trial 2 with value: 0.98997017883108.\n",
      "[I 2025-07-15 00:29:22,415] Trial 10 finished with value: 0.9895735581016559 and parameters: {'n_estimators': 996, 'max_depth': 7, 'learning_rate': 0.09430682459717726, 'subsample': 0.9662701107352314, 'colsample_bytree': 0.8406503226812586, 'gamma': 2.0233829683308575e-08}. Best is trial 2 with value: 0.98997017883108.\n",
      "[I 2025-07-15 00:29:37,921] Trial 11 finished with value: 0.9899082148190368 and parameters: {'n_estimators': 726, 'max_depth': 8, 'learning_rate': 0.03107219944027193, 'subsample': 0.9601630208474436, 'colsample_bytree': 0.9862586985258636, 'gamma': 0.44819492238232905}. Best is trial 2 with value: 0.98997017883108.\n",
      "[I 2025-07-15 00:29:55,226] Trial 12 finished with value: 0.9899879252560814 and parameters: {'n_estimators': 770, 'max_depth': 8, 'learning_rate': 0.030986253507524552, 'subsample': 0.9160528362386892, 'colsample_bytree': 0.8546723581658668, 'gamma': 0.023725409595848127}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:30:07,948] Trial 13 finished with value: 0.9896660058036274 and parameters: {'n_estimators': 665, 'max_depth': 6, 'learning_rate': 0.030446803785630257, 'subsample': 0.9308145824555918, 'colsample_bytree': 0.8608206292425871, 'gamma': 0.023889564235957644}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:30:20,159] Trial 14 finished with value: 0.989864520834308 and parameters: {'n_estimators': 799, 'max_depth': 8, 'learning_rate': 0.04663918675538205, 'subsample': 0.9160864796295206, 'colsample_bytree': 0.7042446281498106, 'gamma': 0.9403198202857415}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:30:37,320] Trial 15 finished with value: 0.9896947094610292 and parameters: {'n_estimators': 874, 'max_depth': 7, 'learning_rate': 0.06744830372646589, 'subsample': 0.9958937869776661, 'colsample_bytree': 0.8572720369712074, 'gamma': 0.008270751596751781}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:30:51,729] Trial 16 finished with value: 0.9899253569917023 and parameters: {'n_estimators': 608, 'max_depth': 8, 'learning_rate': 0.026660889779921447, 'subsample': 0.909637658261673, 'colsample_bytree': 0.8785416960771648, 'gamma': 0.19423015504580768}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:31:04,684] Trial 17 finished with value: 0.9896182544712648 and parameters: {'n_estimators': 749, 'max_depth': 5, 'learning_rate': 0.0368535807183112, 'subsample': 0.8339566274875563, 'colsample_bytree': 0.8185279913703083, 'gamma': 0.00600470425948234}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:31:24,791] Trial 18 finished with value: 0.9897249901882078 and parameters: {'n_estimators': 981, 'max_depth': 7, 'learning_rate': 0.06703321718420552, 'subsample': 0.9613173861661694, 'colsample_bytree': 0.937279660865051, 'gamma': 1.807166320512223e-05}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:31:37,130] Trial 19 finished with value: 0.9895246943961896 and parameters: {'n_estimators': 638, 'max_depth': 6, 'learning_rate': 0.026450433114778276, 'subsample': 0.9956798039630753, 'colsample_bytree': 0.8872338700217017, 'gamma': 0.006247672933493882}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:31:50,960] Trial 20 finished with value: 0.9898449987348127 and parameters: {'n_estimators': 845, 'max_depth': 8, 'learning_rate': 0.055826114441274916, 'subsample': 0.8348142286097185, 'colsample_bytree': 0.8156152547822555, 'gamma': 0.9746149580114106}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:32:10,473] Trial 21 finished with value: 0.9899072971084752 and parameters: {'n_estimators': 790, 'max_depth': 8, 'learning_rate': 0.01478548214169061, 'subsample': 0.7552448013466985, 'colsample_bytree': 0.9969891453315805, 'gamma': 0.07060619986578454}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:32:27,304] Trial 22 finished with value: 0.9898477886875967 and parameters: {'n_estimators': 752, 'max_depth': 8, 'learning_rate': 0.016834075201075292, 'subsample': 0.9352514436187431, 'colsample_bytree': 0.9487723657519296, 'gamma': 0.02123263294276723}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:32:47,065] Trial 23 finished with value: 0.9897077045934626 and parameters: {'n_estimators': 927, 'max_depth': 7, 'learning_rate': 0.011423980403048204, 'subsample': 0.7600513270049807, 'colsample_bytree': 0.9313809771005572, 'gamma': 0.10330454511546616}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:33:04,327] Trial 24 finished with value: 0.9899642538503085 and parameters: {'n_estimators': 777, 'max_depth': 8, 'learning_rate': 0.02373508025321369, 'subsample': 0.8942231023549064, 'colsample_bytree': 0.8942699253085069, 'gamma': 0.026544318312726654}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:33:19,080] Trial 25 finished with value: 0.989817933435293 and parameters: {'n_estimators': 713, 'max_depth': 7, 'learning_rate': 0.02513862521111988, 'subsample': 0.9098613733969981, 'colsample_bytree': 0.8909859368126328, 'gamma': 0.0044847970994835226}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:33:38,600] Trial 26 finished with value: 0.989970767270949 and parameters: {'n_estimators': 849, 'max_depth': 8, 'learning_rate': 0.034703151429421965, 'subsample': 0.8872716053604965, 'colsample_bytree': 0.8360859446075712, 'gamma': 0.2908922357869512}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:33:52,781] Trial 27 finished with value: 0.989630288755539 and parameters: {'n_estimators': 854, 'max_depth': 5, 'learning_rate': 0.03907416819926667, 'subsample': 0.9340879172846047, 'colsample_bytree': 0.8324821378340755, 'gamma': 0.25108886295785693}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:34:11,760] Trial 28 finished with value: 0.9898733610542698 and parameters: {'n_estimators': 939, 'max_depth': 7, 'learning_rate': 0.03548745624767908, 'subsample': 0.8850757450570208, 'colsample_bytree': 0.7720967436436578, 'gamma': 0.001364310092174565}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:34:24,727] Trial 29 finished with value: 0.9898257565517159 and parameters: {'n_estimators': 549, 'max_depth': 8, 'learning_rate': 0.019013218199970237, 'subsample': 0.8379951931924521, 'colsample_bytree': 0.7928173697266265, 'gamma': 0.2219767603633573}. Best is trial 12 with value: 0.9899879252560814.\n",
      "[I 2025-07-15 00:34:24,737] A new study created in memory with name: no-name-9007f6d4-196f-42b6-9bc9-24516d8f6c8e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuning Regressor with Optuna ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-15 00:34:30,143] Trial 0 finished with value: 0.9575587392614255 and parameters: {'n_estimators': 971, 'max_depth': 4, 'learning_rate': 0.054708732513392304, 'subsample': 0.8704366213692751, 'colsample_bytree': 0.9425269589406653}. Best is trial 0 with value: 0.9575587392614255.\n",
      "[I 2025-07-15 00:34:34,981] Trial 1 finished with value: 0.9764607085497653 and parameters: {'n_estimators': 865, 'max_depth': 4, 'learning_rate': 0.02645182855018343, 'subsample': 0.9242054867729493, 'colsample_bytree': 0.9298378923730929}. Best is trial 0 with value: 0.9575587392614255.\n",
      "[I 2025-07-15 00:34:39,945] Trial 2 finished with value: 0.9586431807515525 and parameters: {'n_estimators': 850, 'max_depth': 4, 'learning_rate': 0.059911212521302375, 'subsample': 0.8322016660765699, 'colsample_bytree': 0.8934360540316002}. Best is trial 0 with value: 0.9575587392614255.\n",
      "[I 2025-07-15 00:34:46,227] Trial 3 finished with value: 0.9474746229601186 and parameters: {'n_estimators': 799, 'max_depth': 6, 'learning_rate': 0.021559661673819286, 'subsample': 0.9750060000405275, 'colsample_bytree': 0.8295138437684503}. Best is trial 3 with value: 0.9474746229601186.\n",
      "[I 2025-07-15 00:34:52,107] Trial 4 finished with value: 0.9761707796470697 and parameters: {'n_estimators': 894, 'max_depth': 5, 'learning_rate': 0.011776959979011108, 'subsample': 0.864664753886581, 'colsample_bytree': 0.8628008641160769}. Best is trial 3 with value: 0.9474746229601186.\n",
      "[I 2025-07-15 00:35:02,011] Trial 5 finished with value: 0.8369959558851441 and parameters: {'n_estimators': 851, 'max_depth': 8, 'learning_rate': 0.057716795515032934, 'subsample': 0.9857291811087822, 'colsample_bytree': 0.8896716718727508}. Best is trial 5 with value: 0.8369959558851441.\n",
      "[I 2025-07-15 00:35:09,711] Trial 6 finished with value: 0.9306381334166405 and parameters: {'n_estimators': 780, 'max_depth': 7, 'learning_rate': 0.015573774023722451, 'subsample': 0.8091711525388764, 'colsample_bytree': 0.9546370503449176}. Best is trial 5 with value: 0.8369959558851441.\n",
      "[I 2025-07-15 00:35:14,326] Trial 7 finished with value: 0.9189049679155168 and parameters: {'n_estimators': 522, 'max_depth': 6, 'learning_rate': 0.06171895666414167, 'subsample': 0.8878987461528036, 'colsample_bytree': 0.7726572479188366}. Best is trial 5 with value: 0.8369959558851441.\n",
      "[I 2025-07-15 00:35:17,635] Trial 8 finished with value: 0.9977119642189651 and parameters: {'n_estimators': 485, 'max_depth': 4, 'learning_rate': 0.012555135004066251, 'subsample': 0.8723664757070777, 'colsample_bytree': 0.7486339760567299}. Best is trial 5 with value: 0.8369959558851441.\n",
      "[I 2025-07-15 00:35:21,194] Trial 9 finished with value: 0.9730244107411897 and parameters: {'n_estimators': 564, 'max_depth': 4, 'learning_rate': 0.05363381672584371, 'subsample': 0.947875828529244, 'colsample_bytree': 0.8602260060910726}. Best is trial 5 with value: 0.8369959558851441.\n",
      "[I 2025-07-15 00:35:30,117] Trial 10 finished with value: 0.8363731892760715 and parameters: {'n_estimators': 671, 'max_depth': 8, 'learning_rate': 0.09333113356573272, 'subsample': 0.740012967097108, 'colsample_bytree': 0.801432368809919}. Best is trial 10 with value: 0.8363731892760715.\n",
      "[I 2025-07-15 00:35:38,493] Trial 11 finished with value: 0.8400104756123046 and parameters: {'n_estimators': 657, 'max_depth': 8, 'learning_rate': 0.09602534909793298, 'subsample': 0.7034482715585829, 'colsample_bytree': 0.7741847890225386}. Best is trial 10 with value: 0.8363731892760715.\n",
      "[I 2025-07-15 00:35:46,878] Trial 12 finished with value: 0.8380159829814231 and parameters: {'n_estimators': 673, 'max_depth': 8, 'learning_rate': 0.09828481090738539, 'subsample': 0.7508215884846015, 'colsample_bytree': 0.7013621706847855}. Best is trial 10 with value: 0.8363731892760715.\n",
      "[I 2025-07-15 00:35:51,475] Trial 13 finished with value: 0.9265026267947194 and parameters: {'n_estimators': 403, 'max_depth': 7, 'learning_rate': 0.03364081446822017, 'subsample': 0.784544648553292, 'colsample_bytree': 0.9952602906948325}. Best is trial 10 with value: 0.8363731892760715.\n",
      "[I 2025-07-15 00:35:58,601] Trial 14 finished with value: 0.9001263052655728 and parameters: {'n_estimators': 746, 'max_depth': 7, 'learning_rate': 0.039500473841117836, 'subsample': 0.9923460947340751, 'colsample_bytree': 0.8138370757884872}. Best is trial 10 with value: 0.8363731892760715.\n",
      "[I 2025-07-15 00:36:07,328] Trial 15 finished with value: 0.8427391709543965 and parameters: {'n_estimators': 609, 'max_depth': 8, 'learning_rate': 0.07973925257148473, 'subsample': 0.7025164192217701, 'colsample_bytree': 0.9038255928258423}. Best is trial 10 with value: 0.8363731892760715.\n",
      "[I 2025-07-15 00:36:16,969] Trial 16 finished with value: 0.8664872290514797 and parameters: {'n_estimators': 987, 'max_depth': 7, 'learning_rate': 0.044501674326570054, 'subsample': 0.7486021069861211, 'colsample_bytree': 0.8089927909928187}. Best is trial 10 with value: 0.8363731892760715.\n",
      "[I 2025-07-15 00:36:26,103] Trial 17 finished with value: 0.8322209860061556 and parameters: {'n_estimators': 722, 'max_depth': 8, 'learning_rate': 0.07318013909178274, 'subsample': 0.9151719463700474, 'colsample_bytree': 0.8847412783296691}. Best is trial 17 with value: 0.8322209860061556.\n",
      "[I 2025-07-15 00:36:34,793] Trial 18 finished with value: 0.8323931207912583 and parameters: {'n_estimators': 712, 'max_depth': 8, 'learning_rate': 0.0743934832496018, 'subsample': 0.911774499966071, 'colsample_bytree': 0.7192659886965181}. Best is trial 17 with value: 0.8322209860061556.\n",
      "[I 2025-07-15 00:36:39,930] Trial 19 finished with value: 0.9298951903654724 and parameters: {'n_estimators': 747, 'max_depth': 5, 'learning_rate': 0.07350903286754308, 'subsample': 0.9092210537351604, 'colsample_bytree': 0.7364575168061775}. Best is trial 17 with value: 0.8322209860061556.\n",
      "[I 2025-07-15 00:36:45,861] Trial 20 finished with value: 0.8737699117496319 and parameters: {'n_estimators': 592, 'max_depth': 7, 'learning_rate': 0.07395311636635059, 'subsample': 0.9448052012798285, 'colsample_bytree': 0.703562947775374}. Best is trial 17 with value: 0.8322209860061556.\n",
      "[I 2025-07-15 00:36:54,628] Trial 21 finished with value: 0.8287613991850631 and parameters: {'n_estimators': 704, 'max_depth': 8, 'learning_rate': 0.08939366516283126, 'subsample': 0.9003269754712145, 'colsample_bytree': 0.7851949734160162}. Best is trial 21 with value: 0.8287613991850631.\n",
      "[I 2025-07-15 00:37:03,286] Trial 22 finished with value: 0.8514841218817447 and parameters: {'n_estimators': 715, 'max_depth': 8, 'learning_rate': 0.04340367033515343, 'subsample': 0.9057801290812412, 'colsample_bytree': 0.7366578333894476}. Best is trial 21 with value: 0.8287613991850631.\n",
      "[I 2025-07-15 00:37:12,841] Trial 23 finished with value: 0.8279892955397934 and parameters: {'n_estimators': 802, 'max_depth': 8, 'learning_rate': 0.0767788004174469, 'subsample': 0.9495214334980825, 'colsample_bytree': 0.7750031401966515}. Best is trial 23 with value: 0.8279892955397934.\n",
      "[I 2025-07-15 00:37:20,459] Trial 24 finished with value: 0.853055203983054 and parameters: {'n_estimators': 799, 'max_depth': 7, 'learning_rate': 0.08106087876470143, 'subsample': 0.951565752986572, 'colsample_bytree': 0.7736751183033739}. Best is trial 23 with value: 0.8279892955397934.\n",
      "[I 2025-07-15 00:37:31,469] Trial 25 finished with value: 0.8375405268351432 and parameters: {'n_estimators': 931, 'max_depth': 8, 'learning_rate': 0.047631931888664, 'subsample': 0.9606458492175379, 'colsample_bytree': 0.7881648510667578}. Best is trial 23 with value: 0.8279892955397934.\n",
      "[I 2025-07-15 00:37:37,105] Trial 26 finished with value: 0.9049785483602436 and parameters: {'n_estimators': 635, 'max_depth': 6, 'learning_rate': 0.06805868594544924, 'subsample': 0.8457640139041477, 'colsample_bytree': 0.85061069748482}. Best is trial 23 with value: 0.8279892955397934.\n",
      "[I 2025-07-15 00:37:44,709] Trial 27 finished with value: 0.8947390955067865 and parameters: {'n_estimators': 765, 'max_depth': 7, 'learning_rate': 0.03347029549464742, 'subsample': 0.8956338997721258, 'colsample_bytree': 0.8321379546301348}. Best is trial 23 with value: 0.8279892955397934.\n",
      "[I 2025-07-15 00:37:54,314] Trial 28 finished with value: 0.8265673275964784 and parameters: {'n_estimators': 805, 'max_depth': 8, 'learning_rate': 0.08487089201332512, 'subsample': 0.9256692396083404, 'colsample_bytree': 0.7588832874675819}. Best is trial 28 with value: 0.8265673275964784.\n",
      "[I 2025-07-15 00:38:01,500] Trial 29 finished with value: 0.8977448427000514 and parameters: {'n_estimators': 939, 'max_depth': 6, 'learning_rate': 0.05301399922895351, 'subsample': 0.9351591875728497, 'colsample_bytree': 0.7586200507207166}. Best is trial 28 with value: 0.8265673275964784.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Final Models with Best Parameters ---\n",
      "\n",
      "--- Generating Final Kaggle Submission ---\n",
      "Submission file 'submission_final.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "#Version 3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: ADVANCED FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "\n",
    "class AdvancedFeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Creates a rich set of features for the model.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Date/Time Engineering\n",
    "        X_copy['date'] = pd.to_datetime(X_copy['date'], format='%Y%m%d')\n",
    "        X_copy['sessionYear'] = X_copy['date'].dt.year\n",
    "        X_copy['sessionMonth'] = X_copy['date'].dt.month\n",
    "        X_copy['sessionDayOfWeek'] = X_copy['date'].dt.dayofweek\n",
    "        X_copy['sessionHour'] = pd.to_datetime(X_copy['sessionStart'], unit='s').dt.hour\n",
    "        X_copy['is_weekend'] = (X_copy['sessionDayOfWeek'] >= 5).astype(int)\n",
    "        \n",
    "        # Interaction Features\n",
    "        X_copy['month_day_interaction'] = X_copy['sessionMonth'].astype(str) + '_' + X_copy['sessionDayOfWeek'].astype(str)\n",
    "        X_copy['browser_os_interaction'] = X_copy['browser'].astype(str) + '_' + X_copy['os'].astype(str)\n",
    "        \n",
    "        # Ratio Features (handle division by zero)\n",
    "        X_copy['hits_per_pageview'] = X_copy['totalHits'] / (X_copy['pageViews'] + 1e-6)\n",
    "        \n",
    "        # Binning AdWords Page\n",
    "        X_copy['ad_page_binned'] = X_copy['trafficSource.adwordsClickInfo.page'].apply(\n",
    "            lambda p: 1 if p == 1.0 else (2 if pd.notna(p) else 0)\n",
    "        )\n",
    "        \n",
    "        cols_to_drop = ['date', 'sessionStart', 'userId', 'sessionId', 'trafficSource.adwordsClickInfo.page']\n",
    "        X_copy = X_copy.drop(columns=cols_to_drop, errors='ignore')\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "# TargetEncoder class remains the same\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns; self.mappings_ = {}; self.global_mean_ = 0\n",
    "    def fit(self, X, y):\n",
    "        X_fit, y_fit = X.copy(), y.copy()\n",
    "        self.global_mean_ = np.mean(y_fit)\n",
    "        for col in self.columns:\n",
    "            X_fit[col] = X_fit[col].fillna('missing')\n",
    "            self.mappings_[col] = y_fit.groupby(X_fit[col]).mean().to_dict()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transform = X.copy()\n",
    "        for col in self.columns:\n",
    "            X_transform[col] = X_transform[col].fillna('missing')\n",
    "            X_transform[col] = X_transform[col].map(self.mappings_[col]).fillna(self.global_mean_)\n",
    "        return X_transform\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: DATA PREPARATION\n",
    "# ==============================================================================\n",
    "TRAIN_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/train_data.csv'\n",
    "TEST_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/test_data.csv'\n",
    "\n",
    "print(\"Loading and preparing data...\")\n",
    "df = pd.read_csv(TRAIN_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "one_value_cols = [col for col in df.columns if df[col].nunique(dropna=False) == 1]\n",
    "df = df.drop(columns=one_value_cols)\n",
    "\n",
    "df['purchaseValue'] = df['purchaseValue'].fillna(0) / 1e6\n",
    "df['made_purchase'] = (df['purchaseValue'] > 0).astype(int)\n",
    "df['log_purchaseValue'] = np.log1p(df['purchaseValue'])\n",
    "\n",
    "X = df.drop(columns=['purchaseValue', 'made_purchase', 'log_purchaseValue'])\n",
    "y = df[['made_purchase', 'log_purchaseValue']]\n",
    "\n",
    "# --- Define Column Groups for Preprocessing ---\n",
    "temp_engineered_df = AdvancedFeatureEngineering().fit_transform(X)\n",
    "numerical_cols = ['sessionNumber', 'pageViews', 'totalHits', 'sessionYear', 'sessionMonth', 'sessionDayOfWeek', 'sessionHour', 'hits_per_pageview']\n",
    "categorical_cols = [col for col in temp_engineered_df.columns if col not in numerical_cols]\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: HYPERPARAMETER TUNING WITH OPTUNA\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Tuning the Classifier ---\n",
    "print(\"\\n--- Tuning Classifier with Optuna ---\")\n",
    "X_engineered = AdvancedFeatureEngineering().fit_transform(X)\n",
    "y_clf_target = y['made_purchase']\n",
    "\n",
    "def objective_clf(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic', 'eval_metric': 'logloss', 'random_state': 42,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "    }\n",
    "    \n",
    "    cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_engineered):\n",
    "        X_train, X_val = X_engineered.iloc[train_idx], X_engineered.iloc[val_idx]\n",
    "        y_train, y_val = y_clf_target.iloc[train_idx], y_clf_target.iloc[val_idx]\n",
    "\n",
    "        preprocessor = ColumnTransformer([('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols), ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)], remainder='drop')\n",
    "        \n",
    "        preprocessor.fit(X_train, y_train)\n",
    "        X_train_processed = preprocessor.transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        preds = model.predict_proba(X_val_processed)[:, 1]\n",
    "        cv_scores.append(roc_auc_score(y_val, preds))\n",
    "        \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study_clf = optuna.create_study(direction='maximize')\n",
    "study_clf.optimize(objective_clf, n_trials=30) # Run 30 trials\n",
    "best_clf_params = study_clf.best_params\n",
    "\n",
    "# --- Tuning the Regressor ---\n",
    "print(\"\\n--- Tuning Regressor with Optuna ---\")\n",
    "X_buyers_engineered = X_engineered[y['made_purchase'] == 1]\n",
    "y_reg_target = y.loc[y['made_purchase'] == 1, 'log_purchaseValue']\n",
    "\n",
    "def objective_reg(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'random_state': 42,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_buyers_engineered):\n",
    "        X_train, X_val = X_buyers_engineered.iloc[train_idx], X_buyers_engineered.iloc[val_idx]\n",
    "        y_train, y_val = y_reg_target.iloc[train_idx], y_reg_target.iloc[val_idx]\n",
    "\n",
    "        preprocessor = ColumnTransformer([('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols), ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)], remainder='drop')\n",
    "\n",
    "        preprocessor.fit(X_train, y_train)\n",
    "        X_train_processed = preprocessor.transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        preds = model.predict(X_val_processed)\n",
    "        cv_scores.append(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study_reg = optuna.create_study(direction='minimize')\n",
    "study_reg.optimize(objective_reg, n_trials=30) # Run 30 trials\n",
    "best_reg_params = study_reg.best_params\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: FINAL MODEL TRAINING AND SUBMISSION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Training Final Models with Best Parameters ---\")\n",
    "# Build and fit the final classifier pipeline on ALL training data\n",
    "final_clf_pipeline = Pipeline([\n",
    "    ('engineering', AdvancedFeatureEngineering()),\n",
    "    ('preprocessing', ColumnTransformer([('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols), ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)], remainder='drop')),\n",
    "    ('classifier', xgb.XGBClassifier(**best_clf_params, random_state=42))\n",
    "])\n",
    "final_clf_pipeline.fit(X, y['made_purchase'])\n",
    "\n",
    "# Build and fit the final regressor pipeline on ALL buyer data\n",
    "final_reg_pipeline = Pipeline([\n",
    "    ('engineering', AdvancedFeatureEngineering()),\n",
    "    ('preprocessing', ColumnTransformer([('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols), ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)], remainder='drop')),\n",
    "    ('regressor', xgb.XGBRegressor(**best_reg_params, random_state=42))\n",
    "])\n",
    "final_reg_pipeline.fit(X[y['made_purchase'] == 1], y.loc[y['made_purchase'] == 1, 'log_purchaseValue'])\n",
    "\n",
    "print(\"\\n--- Generating Final Kaggle Submission ---\")\n",
    "try:\n",
    "    kaggle_test_df = pd.read_csv(TEST_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "    \n",
    "    kaggle_prob_purchase = final_clf_pipeline.predict_proba(kaggle_test_df)[:, 1]\n",
    "    kaggle_log_value_pred = final_reg_pipeline.predict(kaggle_test_df)\n",
    "    \n",
    "    kaggle_value_pred = np.expm1(kaggle_log_value_pred)\n",
    "    kaggle_final_predictions_dollars = kaggle_prob_purchase * kaggle_value_pred\n",
    "    kaggle_final_predictions_dollars[kaggle_final_predictions_dollars < 0] = 0\n",
    "    \n",
    "    kaggle_final_predictions_scaled = kaggle_final_predictions_dollars * 1e6\n",
    "    \n",
    "    submission_df = pd.DataFrame({'ID': kaggle_test_df.index, 'purchaseValue': kaggle_final_predictions_scaled})\n",
    "    submission_df.to_csv('submission_final.csv', index=False)\n",
    "    print(\"Submission file 'submission_final.csv' created successfully.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nKaggle '{TEST_FILE_PATH}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during submission generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3368f87a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
