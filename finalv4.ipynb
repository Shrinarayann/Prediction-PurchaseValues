{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7928839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Creating user-level features...\n",
      "User-level features created and merged.\n",
      "\n",
      "Identified 13 numerical features.\n",
      "Identified 36 categorical features.\n",
      "\n",
      "--- Tuning Classifier with Optuna (20 trials) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-15 13:52:39,498] A new study created in memory with name: no-name-7db65dbb-8cfd-4337-90aa-a4d7e4e7b52b\n",
      "[I 2025-07-15 13:52:58,948] Trial 0 finished with value: 0.9997992123761983 and parameters: {'n_estimators': 1177, 'max_depth': 8, 'learning_rate': 0.06364487553493839, 'subsample': 0.8581157869329789, 'colsample_bytree': 0.7352708468974155, 'gamma': 0.11854878045838543}. Best is trial 0 with value: 0.9997992123761983.\n",
      "[I 2025-07-15 13:53:13,591] Trial 1 finished with value: 0.9998096543775419 and parameters: {'n_estimators': 746, 'max_depth': 5, 'learning_rate': 0.018393932913427637, 'subsample': 0.9630656697122516, 'colsample_bytree': 0.894917280636048, 'gamma': 0.019787482458871124}. Best is trial 1 with value: 0.9998096543775419.\n",
      "[I 2025-07-15 13:53:26,545] Trial 2 finished with value: 0.9998090919023246 and parameters: {'n_estimators': 555, 'max_depth': 5, 'learning_rate': 0.04524996394918677, 'subsample': 0.6985366176149823, 'colsample_bytree': 0.9052815949365604, 'gamma': 1.828144942802844e-06}. Best is trial 1 with value: 0.9998096543775419.\n",
      "[I 2025-07-15 13:53:44,194] Trial 3 finished with value: 0.9997982973162912 and parameters: {'n_estimators': 817, 'max_depth': 9, 'learning_rate': 0.08002928082428114, 'subsample': 0.6563112097628021, 'colsample_bytree': 0.868381600844238, 'gamma': 0.22493734196863652}. Best is trial 1 with value: 0.9998096543775419.\n",
      "[I 2025-07-15 13:54:09,333] Trial 4 finished with value: 0.9998080999495803 and parameters: {'n_estimators': 917, 'max_depth': 10, 'learning_rate': 0.02202261178583719, 'subsample': 0.6743292266567362, 'colsample_bytree': 0.8244695463972223, 'gamma': 9.171760451136015e-08}. Best is trial 1 with value: 0.9998096543775419.\n",
      "[I 2025-07-15 13:54:25,359] Trial 5 finished with value: 0.9998101982073545 and parameters: {'n_estimators': 951, 'max_depth': 4, 'learning_rate': 0.016252540135004562, 'subsample': 0.8062792516909107, 'colsample_bytree': 0.6766649405550559, 'gamma': 5.983640908211345e-07}. Best is trial 5 with value: 0.9998101982073545.\n",
      "[I 2025-07-15 13:54:40,358] Trial 6 finished with value: 0.9997864305634059 and parameters: {'n_estimators': 634, 'max_depth': 5, 'learning_rate': 0.08546549592711958, 'subsample': 0.6550946115807522, 'colsample_bytree': 0.9619191346825801, 'gamma': 0.00458040568410828}. Best is trial 5 with value: 0.9998101982073545.\n",
      "[I 2025-07-15 13:54:58,707] Trial 7 finished with value: 0.9998104541961873 and parameters: {'n_estimators': 937, 'max_depth': 5, 'learning_rate': 0.014765710239784806, 'subsample': 0.8815787971972054, 'colsample_bytree': 0.716547311142271, 'gamma': 5.483911265107526e-08}. Best is trial 7 with value: 0.9998104541961873.\n",
      "[I 2025-07-15 13:55:11,861] Trial 8 finished with value: 0.9998082597782161 and parameters: {'n_estimators': 749, 'max_depth': 4, 'learning_rate': 0.01588015534092288, 'subsample': 0.923938812192402, 'colsample_bytree': 0.8623536614947228, 'gamma': 0.0004897455544427763}. Best is trial 7 with value: 0.9998104541961873.\n",
      "[I 2025-07-15 13:55:31,641] Trial 9 finished with value: 0.9998054402833901 and parameters: {'n_estimators': 886, 'max_depth': 6, 'learning_rate': 0.0397314161137042, 'subsample': 0.8690115609401897, 'colsample_bytree': 0.7408052874274657, 'gamma': 0.016783416616918914}. Best is trial 7 with value: 0.9998104541961873.\n",
      "[I 2025-07-15 13:55:43,789] Trial 10 finished with value: 0.9997902561417797 and parameters: {'n_estimators': 426, 'max_depth': 7, 'learning_rate': 0.010084499586373898, 'subsample': 0.7500560863956199, 'colsample_bytree': 0.6061073642907508, 'gamma': 1.0150556869606614e-08}. Best is trial 7 with value: 0.9998104541961873.\n",
      "[I 2025-07-15 13:56:01,482] Trial 11 finished with value: 0.9998060401572384 and parameters: {'n_estimators': 1034, 'max_depth': 4, 'learning_rate': 0.011409060899906686, 'subsample': 0.7991428730656397, 'colsample_bytree': 0.6398175222420975, 'gamma': 1.8344521089629205e-06}. Best is trial 7 with value: 0.9998104541961873.\n",
      "[I 2025-07-15 13:56:24,945] Trial 12 finished with value: 0.9998075918546541 and parameters: {'n_estimators': 1056, 'max_depth': 6, 'learning_rate': 0.024707635707303922, 'subsample': 0.8489127874854664, 'colsample_bytree': 0.6990425193791583, 'gamma': 5.290372996453893e-06}. Best is trial 7 with value: 0.9998104541961873.\n",
      "[I 2025-07-15 13:56:42,347] Trial 13 finished with value: 0.9998083386897849 and parameters: {'n_estimators': 1010, 'max_depth': 4, 'learning_rate': 0.014444501699363834, 'subsample': 0.7959734385367, 'colsample_bytree': 0.6750920859682942, 'gamma': 7.98211762080274e-08}. Best is trial 7 with value: 0.9998104541961873.\n",
      "[I 2025-07-15 13:57:02,008] Trial 14 finished with value: 0.9998090278572817 and parameters: {'n_estimators': 942, 'max_depth': 6, 'learning_rate': 0.02688541234906926, 'subsample': 0.9190913369265931, 'colsample_bytree': 0.7585217365959837, 'gamma': 2.2720547388881638e-05}. Best is trial 7 with value: 0.9998104541961873.\n",
      "[I 2025-07-15 13:57:27,817] Trial 15 finished with value: 0.9998132029523126 and parameters: {'n_estimators': 1196, 'max_depth': 7, 'learning_rate': 0.013544806271289402, 'subsample': 0.7544708390806673, 'colsample_bytree': 0.7791493409108223, 'gamma': 1.2448851774679061e-07}. Best is trial 15 with value: 0.9998132029523126.\n",
      "[I 2025-07-15 13:57:54,103] Trial 16 finished with value: 0.9998139265244457 and parameters: {'n_estimators': 1160, 'max_depth': 8, 'learning_rate': 0.013454431636761167, 'subsample': 0.7380881205076005, 'colsample_bytree': 0.793814381626606, 'gamma': 0.00016543684837720262}. Best is trial 16 with value: 0.9998139265244457.\n",
      "[I 2025-07-15 13:58:22,327] Trial 17 finished with value: 0.9997999532243749 and parameters: {'n_estimators': 1194, 'max_depth': 8, 'learning_rate': 0.03246715258041587, 'subsample': 0.6034283453486566, 'colsample_bytree': 0.7974989110587237, 'gamma': 0.0002959965795077974}. Best is trial 16 with value: 0.9998139265244457.\n",
      "[I 2025-07-15 13:58:46,650] Trial 18 finished with value: 0.9998142982837979 and parameters: {'n_estimators': 1118, 'max_depth': 8, 'learning_rate': 0.011980959547758949, 'subsample': 0.7315077366869531, 'colsample_bytree': 0.7890778094662402, 'gamma': 4.5589351871742715e-05}. Best is trial 18 with value: 0.9998142982837979.\n",
      "[I 2025-07-15 13:59:13,797] Trial 19 finished with value: 0.9998082358826762 and parameters: {'n_estimators': 1112, 'max_depth': 9, 'learning_rate': 0.021087971817897004, 'subsample': 0.723831489137616, 'colsample_bytree': 0.8236903303839828, 'gamma': 4.548835059143552e-05}. Best is trial 18 with value: 0.9998142982837979.\n",
      "[I 2025-07-15 13:59:13,798] A new study created in memory with name: no-name-4760bcfb-d913-4ee0-bcb1-cff4cfd43af1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuning Regressor with Optuna (20 trials) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-15 13:59:41,306] Trial 0 finished with value: 0.32131670355637076 and parameters: {'n_estimators': 1200, 'max_depth': 10, 'learning_rate': 0.03008227921110725, 'subsample': 0.9960032474756653, 'colsample_bytree': 0.7314583931481613}. Best is trial 0 with value: 0.32131670355637076.\n",
      "[I 2025-07-15 13:59:47,525] Trial 1 finished with value: 0.311307064881424 and parameters: {'n_estimators': 502, 'max_depth': 7, 'learning_rate': 0.021206995531355397, 'subsample': 0.8613872518251569, 'colsample_bytree': 0.9826650187444967}. Best is trial 1 with value: 0.311307064881424.\n",
      "[I 2025-07-15 13:59:57,660] Trial 2 finished with value: 0.3248924072685753 and parameters: {'n_estimators': 432, 'max_depth': 10, 'learning_rate': 0.06240998578832945, 'subsample': 0.7964398185228835, 'colsample_bytree': 0.6186295110290995}. Best is trial 1 with value: 0.311307064881424.\n",
      "[I 2025-07-15 14:00:15,723] Trial 3 finished with value: 0.32574040353289463 and parameters: {'n_estimators': 1024, 'max_depth': 10, 'learning_rate': 0.07853837783962439, 'subsample': 0.8870366232377276, 'colsample_bytree': 0.6493534758916621}. Best is trial 1 with value: 0.311307064881424.\n",
      "[I 2025-07-15 14:00:38,976] Trial 4 finished with value: 0.31480837720128085 and parameters: {'n_estimators': 1127, 'max_depth': 10, 'learning_rate': 0.012283832800543975, 'subsample': 0.722922939813667, 'colsample_bytree': 0.6865929206520391}. Best is trial 1 with value: 0.311307064881424.\n",
      "[I 2025-07-15 14:00:55,963] Trial 5 finished with value: 0.3218782822269347 and parameters: {'n_estimators': 1003, 'max_depth': 10, 'learning_rate': 0.0954914867396769, 'subsample': 0.7966483451793225, 'colsample_bytree': 0.9822369356694793}. Best is trial 1 with value: 0.311307064881424.\n",
      "[I 2025-07-15 14:01:03,469] Trial 6 finished with value: 0.31052813206860164 and parameters: {'n_estimators': 871, 'max_depth': 6, 'learning_rate': 0.013059073616224456, 'subsample': 0.9402512215516188, 'colsample_bytree': 0.6706521050716538}. Best is trial 6 with value: 0.31052813206860164.\n",
      "[I 2025-07-15 14:01:08,079] Trial 7 finished with value: 0.30535508482226575 and parameters: {'n_estimators': 666, 'max_depth': 4, 'learning_rate': 0.011835687571612878, 'subsample': 0.605628666963347, 'colsample_bytree': 0.7169944725011749}. Best is trial 7 with value: 0.30535508482226575.\n",
      "[I 2025-07-15 14:01:13,833] Trial 8 finished with value: 0.30735630398851893 and parameters: {'n_estimators': 741, 'max_depth': 5, 'learning_rate': 0.014252903459531245, 'subsample': 0.9812558772442069, 'colsample_bytree': 0.8318333063029645}. Best is trial 7 with value: 0.30535508482226575.\n",
      "[I 2025-07-15 14:01:34,085] Trial 9 finished with value: 0.31762225394418675 and parameters: {'n_estimators': 799, 'max_depth': 10, 'learning_rate': 0.012478886015940473, 'subsample': 0.9255292873425244, 'colsample_bytree': 0.9202274978774748}. Best is trial 7 with value: 0.30535508482226575.\n",
      "[I 2025-07-15 14:01:38,349] Trial 10 finished with value: 0.31118298383292525 and parameters: {'n_estimators': 603, 'max_depth': 4, 'learning_rate': 0.04526677479379097, 'subsample': 0.6116592724167369, 'colsample_bytree': 0.7976431188606923}. Best is trial 7 with value: 0.30535508482226575.\n",
      "[I 2025-07-15 14:01:43,351] Trial 11 finished with value: 0.30798240540327454 and parameters: {'n_estimators': 696, 'max_depth': 4, 'learning_rate': 0.019280414398531395, 'subsample': 0.6004206970071744, 'colsample_bytree': 0.8331682246643644}. Best is trial 7 with value: 0.30535508482226575.\n",
      "[I 2025-07-15 14:01:48,839] Trial 12 finished with value: 0.3105331461338345 and parameters: {'n_estimators': 674, 'max_depth': 5, 'learning_rate': 0.01877466859996451, 'subsample': 0.6830026229889098, 'colsample_bytree': 0.8029343520920555}. Best is trial 7 with value: 0.30535508482226575.\n",
      "[I 2025-07-15 14:01:55,145] Trial 13 finished with value: 0.31236674898915817 and parameters: {'n_estimators': 818, 'max_depth': 5, 'learning_rate': 0.027279992952947325, 'subsample': 0.7281795550677672, 'colsample_bytree': 0.7446100817775492}. Best is trial 7 with value: 0.30535508482226575.\n",
      "[I 2025-07-15 14:02:04,108] Trial 14 finished with value: 0.3159628047730601 and parameters: {'n_estimators': 570, 'max_depth': 8, 'learning_rate': 0.010382299371902527, 'subsample': 0.9916593102795173, 'colsample_bytree': 0.8717856006742133}. Best is trial 7 with value: 0.30535508482226575.\n",
      "[I 2025-07-15 14:02:09,648] Trial 15 finished with value: 0.30697189515626694 and parameters: {'n_estimators': 717, 'max_depth': 5, 'learning_rate': 0.015334212060407646, 'subsample': 0.846084204039597, 'colsample_bytree': 0.7402315476570803}. Best is trial 7 with value: 0.30535508482226575.\n",
      "[I 2025-07-15 14:02:17,523] Trial 16 finished with value: 0.3175394301259949 and parameters: {'n_estimators': 935, 'max_depth': 6, 'learning_rate': 0.04141757515837449, 'subsample': 0.863413575688342, 'colsample_bytree': 0.7342616178174265}. Best is trial 7 with value: 0.30535508482226575.\n",
      "[I 2025-07-15 14:02:21,887] Trial 17 finished with value: 0.30507898081099005 and parameters: {'n_estimators': 589, 'max_depth': 4, 'learning_rate': 0.016193434852273397, 'subsample': 0.6655372342309316, 'colsample_bytree': 0.60125948996143}. Best is trial 17 with value: 0.30507898081099005.\n",
      "[I 2025-07-15 14:02:26,036] Trial 18 finished with value: 0.3063272619661309 and parameters: {'n_estimators': 586, 'max_depth': 4, 'learning_rate': 0.02185790585755468, 'subsample': 0.6510098372310416, 'colsample_bytree': 0.6007995782322522}. Best is trial 17 with value: 0.30507898081099005.\n",
      "[I 2025-07-15 14:02:32,634] Trial 19 finished with value: 0.31291693095321477 and parameters: {'n_estimators': 400, 'max_depth': 8, 'learning_rate': 0.010375148150408242, 'subsample': 0.6565568867849881, 'colsample_bytree': 0.695539343533834}. Best is trial 17 with value: 0.30507898081099005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Final Models with Best Parameters ---\n",
      "Final Classifier trained.\n",
      "Final Regressor trained.\n",
      "\n",
      "--- Generating Final Kaggle Submission ---\n",
      "Submission file 'submission_improved.csv' created successfully.\n",
      "   ID  purchaseValue\n",
      "0   0   3.247984e+07\n",
      "1   1   1.561607e+03\n",
      "2   2   4.326103e+02\n",
      "3   3   2.941875e+05\n",
      "4   4   9.960168e+02\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# VERSION 4.1: Fix for older XGBoost versions by removing early stopping during tuning\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Define File Paths ---\n",
    "TRAIN_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/train_data.csv'\n",
    "TEST_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/test_data.csv'\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: ADVANCED FEATURE ENGINEERING TRANSFORMER\n",
    "# ==============================================================================\n",
    "\n",
    "class AdvancedFeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Creates a rich set of session-level features for the model.\n",
    "    NOTE: userId is intentionally NOT dropped here, as it's needed for GroupKFold.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        # Date/Time Engineering\n",
    "        X_copy['date'] = pd.to_datetime(X_copy['date'], format='%Y%m%d')\n",
    "        X_copy['sessionYear'] = X_copy['date'].dt.year\n",
    "        X_copy['sessionMonth'] = X_copy['date'].dt.month\n",
    "        X_copy['sessionDayOfWeek'] = X_copy['date'].dt.dayofweek\n",
    "        X_copy['sessionHour'] = pd.to_datetime(X_copy['sessionStart'], unit='s').dt.hour\n",
    "        X_copy['is_weekend'] = (X_copy['sessionDayOfWeek'] >= 5).astype(int)\n",
    "\n",
    "        # Interaction Features\n",
    "        X_copy['month_day_interaction'] = X_copy['sessionMonth'].astype(str) + '_' + X_copy['sessionDayOfWeek'].astype(str)\n",
    "        X_copy['browser_os_interaction'] = X_copy['browser'].astype(str) + '_' + X_copy['os'].astype(str)\n",
    "        X_copy['geo_channel_interaction'] = X_copy['geoNetwork.continent'].astype(str) + '_' + X_copy['userChannel'].astype(str)\n",
    "        X_copy['device_channel_interaction'] = X_copy['deviceType'].astype(str) + '_' + X_copy['userChannel'].astype(str)\n",
    "\n",
    "        # Ratio Features (handle division by zero)\n",
    "        X_copy['hits_per_pageview'] = X_copy['totalHits'] / (X_copy['pageViews'] + 1e-6)\n",
    "\n",
    "        # Binning AdWords Page\n",
    "        X_copy['ad_page_binned'] = X_copy['trafficSource.adwordsClickInfo.page'].apply(\n",
    "            lambda p: 1 if p == 1.0 else (2 if pd.notna(p) else 0)\n",
    "        )\n",
    "\n",
    "        # Drop original columns that are now redundant or identifiers not used as features\n",
    "        cols_to_drop = ['date', 'sessionStart', 'sessionId', 'trafficSource.adwordsClickInfo.page']\n",
    "        X_copy = X_copy.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "        return X_copy\n",
    "\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Target encodes categorical features. Adds smoothing to prevent overfitting.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=None, smoothing=10):\n",
    "        self.columns = columns\n",
    "        self.smoothing = smoothing\n",
    "        self.mappings_ = {}\n",
    "        self.global_mean_ = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_fit, y_fit = X.copy(), y.copy()\n",
    "        self.global_mean_ = np.mean(y_fit)\n",
    "\n",
    "        for col in self.columns:\n",
    "            X_fit[col] = X_fit[col].fillna('missing')\n",
    "            agg = y_fit.groupby(X_fit[col]).agg(['mean', 'count'])\n",
    "            counts = agg['count']\n",
    "            means = agg['mean']\n",
    "            \n",
    "            # Apply smoothing\n",
    "            smooth_mean = (counts * means + self.smoothing * self.global_mean_) / (counts + self.smoothing)\n",
    "            self.mappings_[col] = smooth_mean.to_dict()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transform = X.copy()\n",
    "        for col in self.columns:\n",
    "            X_transform[col] = X_transform[col].fillna('missing')\n",
    "            X_transform[col] = X_transform[col].map(self.mappings_[col]).fillna(self.global_mean_)\n",
    "        return X_transform\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: DATA PREPARATION (NOW WITH USER-LEVEL FEATURES)\n",
    "# ==============================================================================\n",
    "\n",
    "def create_user_level_features(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Combines train and test to create consistent user-level aggregate features.\n",
    "    \"\"\"\n",
    "    print(\"Creating user-level features...\")\n",
    "    # Create target-related columns ONLY on the training set before combination\n",
    "    df_train['purchaseValue'] = df_train['purchaseValue'].fillna(0) / 1e6\n",
    "    df_train['made_purchase'] = (df_train['purchaseValue'] > 0).astype(int)\n",
    "\n",
    "    # Combine for consistent aggregation\n",
    "    combined_df = pd.concat([df_train.drop(['purchaseValue', 'made_purchase'], axis=1), df_test], axis=0)\n",
    "\n",
    "    # Use the original df_train with target info to create aggregates\n",
    "    user_aggregates = df_train.groupby('userId').agg(\n",
    "        user_session_count=('sessionId', 'nunique'),\n",
    "        user_total_hits=('totalHits', 'sum'),\n",
    "        user_avg_hits=('totalHits', 'mean'),\n",
    "        user_total_pageviews=('pageViews', 'sum'),\n",
    "        user_avg_pageviews=('pageViews', 'mean'),\n",
    "        user_purchase_count=('made_purchase', 'sum'),\n",
    "        user_total_purchase_value=('purchaseValue', 'sum'),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate user conversion rate and average purchase value\n",
    "    user_aggregates['user_conversion_rate'] = user_aggregates['user_purchase_count'] / user_aggregates['user_session_count']\n",
    "    user_aggregates['user_avg_purchase_value'] = user_aggregates['user_total_purchase_value'] / (user_aggregates['user_purchase_count'] + 1e-6)\n",
    "\n",
    "    # Merge these features back into the original dataframes\n",
    "    df_train = pd.merge(df_train, user_aggregates, on='userId', how='left')\n",
    "    df_test = pd.merge(df_test, user_aggregates, on='userId', how='left')\n",
    "    \n",
    "    print(\"User-level features created and merged.\")\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "print(\"Loading and preparing data...\")\n",
    "df_train = pd.read_csv(TRAIN_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "df_test = pd.read_csv(TEST_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "\n",
    "# Drop constant value columns from training data and apply to test data\n",
    "one_value_cols = [col for col in df_train.columns if df_train[col].nunique(dropna=False) == 1]\n",
    "df_train = df_train.drop(columns=one_value_cols)\n",
    "df_test = df_test.drop(columns=[c for c in one_value_cols if c in df_test.columns], errors='ignore')\n",
    "\n",
    "# Create user-level features\n",
    "df_train, df_test = create_user_level_features(df_train, df_test)\n",
    "\n",
    "# Finalize target variable for training\n",
    "df_train['log_purchaseValue'] = np.log1p(df_train['purchaseValue'])\n",
    "X = df_train.drop(columns=['purchaseValue', 'made_purchase', 'log_purchaseValue'])\n",
    "y = df_train[['made_purchase', 'log_purchaseValue']]\n",
    "\n",
    "# --- Define Column Groups for Preprocessing (after all features are created) ---\n",
    "temp_engineered_df = AdvancedFeatureEngineering().fit_transform(X)\n",
    "user_level_numerical = [\n",
    "    'user_session_count', 'user_total_hits', 'user_avg_hits',\n",
    "    'user_total_pageviews', 'user_avg_pageviews', 'user_purchase_count',\n",
    "    'user_total_purchase_value', 'user_conversion_rate', 'user_avg_purchase_value'\n",
    "]\n",
    "session_level_numerical = ['sessionNumber', 'pageViews', 'totalHits', 'hits_per_pageview']\n",
    "numerical_cols = session_level_numerical + user_level_numerical\n",
    "\n",
    "# All other columns (including time features) are treated as categorical. userId is excluded.\n",
    "categorical_cols = [\n",
    "    col for col in temp_engineered_df.columns\n",
    "    if col not in numerical_cols and col != 'userId'\n",
    "]\n",
    "\n",
    "print(f\"\\nIdentified {len(numerical_cols)} numerical features.\")\n",
    "print(f\"Identified {len(categorical_cols)} categorical features.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: HYPERPARAMETER TUNING WITH OPTUNA (USING GROUPKFOLD)\n",
    "# ==============================================================================\n",
    "\n",
    "def objective_clf(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic', 'eval_metric': 'logloss', 'random_state': 42, 'n_jobs': -1,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 400, 1200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "    }\n",
    "    \n",
    "    cv = GroupKFold(n_splits=4)\n",
    "    groups = X_engineered['userId']\n",
    "    y_target = y['made_purchase']\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_engineered, y_target, groups):\n",
    "        X_train, X_val = X_engineered.iloc[train_idx], X_engineered.iloc[val_idx]\n",
    "        y_train, y_val = y_target.iloc[train_idx], y_target.iloc[val_idx]\n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols),\n",
    "            ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)\n",
    "        ], remainder='drop')\n",
    "        \n",
    "        preprocessor.fit(X_train, y_train)\n",
    "        X_train_processed = preprocessor.transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        # !!!!! MAJOR CHANGE HERE: Removed early stopping arguments for compatibility !!!!!\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        preds = model.predict_proba(X_val_processed)[:, 1]\n",
    "        cv_scores.append(roc_auc_score(y_val, preds))\n",
    "        \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "def objective_reg(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'random_state': 42, 'n_jobs': -1,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 400, 1200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "    }\n",
    "\n",
    "    X_buyers_engineered = X_engineered[y['made_purchase'] == 1]\n",
    "    y_reg_target = y.loc[y['made_purchase'] == 1, 'log_purchaseValue']\n",
    "\n",
    "    cv = GroupKFold(n_splits=4)\n",
    "    groups = X_buyers_engineered['userId']\n",
    "    cv_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_buyers_engineered, y_reg_target, groups):\n",
    "        X_train, X_val = X_buyers_engineered.iloc[train_idx], X_buyers_engineered.iloc[val_idx]\n",
    "        y_train, y_val = y_reg_target.iloc[train_idx], y_reg_target.iloc[val_idx]\n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols),\n",
    "            ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)\n",
    "        ], remainder='drop')\n",
    "\n",
    "        preprocessor.fit(X_train, y_train)\n",
    "        X_train_processed = preprocessor.transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        # !!!!! MAJOR CHANGE HERE: Removed early stopping arguments for compatibility !!!!!\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        preds = model.predict(X_val_processed)\n",
    "        cv_scores.append(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "\n",
    "# --- Run Optuna Studies ---\n",
    "N_TRIALS = 20\n",
    "print(f\"\\n--- Tuning Classifier with Optuna ({N_TRIALS} trials) ---\")\n",
    "X_engineered = AdvancedFeatureEngineering().fit_transform(X)\n",
    "study_clf = optuna.create_study(direction='maximize')\n",
    "study_clf.optimize(objective_clf, n_trials=N_TRIALS)\n",
    "best_clf_params = study_clf.best_params\n",
    "\n",
    "print(f\"\\n--- Tuning Regressor with Optuna ({N_TRIALS} trials) ---\")\n",
    "study_reg = optuna.create_study(direction='minimize')\n",
    "study_reg.optimize(objective_reg, n_trials=N_TRIALS)\n",
    "best_reg_params = study_reg.best_params\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: FINAL MODEL TRAINING AND SUBMISSION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Training Final Models with Best Parameters ---\")\n",
    "final_clf_pipeline = Pipeline([\n",
    "    ('engineering', AdvancedFeatureEngineering()),\n",
    "    ('preprocessing', ColumnTransformer([\n",
    "        ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols),\n",
    "        ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)\n",
    "    ], remainder='drop')),\n",
    "    ('classifier', xgb.XGBClassifier(**best_clf_params, random_state=42, n_jobs=-1))\n",
    "])\n",
    "final_clf_pipeline.fit(X, y['made_purchase'])\n",
    "print(\"Final Classifier trained.\")\n",
    "\n",
    "X_buyers = X[y['made_purchase'] == 1]\n",
    "y_buyers_log_value = y.loc[y['made_purchase'] == 1, 'log_purchaseValue']\n",
    "\n",
    "final_reg_pipeline = Pipeline([\n",
    "    ('engineering', AdvancedFeatureEngineering()),\n",
    "    ('preprocessing', ColumnTransformer([\n",
    "        ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols),\n",
    "        ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)\n",
    "    ], remainder='drop')),\n",
    "    ('regressor', xgb.XGBRegressor(**best_reg_params, random_state=42, n_jobs=-1))\n",
    "])\n",
    "final_reg_pipeline.fit(X_buyers, y_buyers_log_value)\n",
    "print(\"Final Regressor trained.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Generating Final Kaggle Submission ---\")\n",
    "try:\n",
    "    train_cols = X.columns\n",
    "    for col in train_cols:\n",
    "        if col not in df_test.columns:\n",
    "            df_test[col] = np.nan\n",
    "    df_test = df_test[train_cols]\n",
    "\n",
    "    kaggle_prob_purchase = final_clf_pipeline.predict_proba(df_test)[:, 1]\n",
    "    kaggle_log_value_pred = final_reg_pipeline.predict(df_test)\n",
    "    kaggle_value_pred = np.expm1(kaggle_log_value_pred)\n",
    "    kaggle_final_predictions_dollars = kaggle_prob_purchase * kaggle_value_pred\n",
    "    kaggle_final_predictions_dollars[kaggle_final_predictions_dollars < 0] = 0\n",
    "    kaggle_final_predictions_scaled = kaggle_final_predictions_dollars * 1e6\n",
    "\n",
    "    submission_df = pd.DataFrame({'ID': df_test.index, 'purchaseValue': kaggle_final_predictions_scaled})\n",
    "    submission_df.to_csv('submission_improved.csv', index=False)\n",
    "    print(\"Submission file 'submission_improved.csv' created successfully.\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nKaggle test file at '{TEST_FILE_PATH}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during submission generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909d78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
