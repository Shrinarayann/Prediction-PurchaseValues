{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c312c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Creating user-level features...\n",
      "User-level features created and merged.\n",
      "\n",
      "Identified 13 numerical features.\n",
      "Identified 36 categorical features.\n",
      "\n",
      "--- Tuning Single Regressor with Optuna (30 trials) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 19:48:20,707] A new study created in memory with name: no-name-87af6514-f769-4dde-bd9c-1faff056b00e\n",
      "[I 2025-07-16 19:48:34,011] Trial 0 finished with value: 0.30318617901507333 and parameters: {'n_estimators': 414, 'max_depth': 8, 'learning_rate': 0.06624010103191053, 'subsample': 0.8928469657991477, 'colsample_bytree': 0.7131835896032724}. Best is trial 0 with value: 0.30318617901507333.\n",
      "[I 2025-07-16 19:48:45,041] Trial 1 finished with value: 0.30672136290848784 and parameters: {'n_estimators': 640, 'max_depth': 4, 'learning_rate': 0.055980537907327654, 'subsample': 0.6421132696192603, 'colsample_bytree': 0.6909041657074161}. Best is trial 0 with value: 0.30318617901507333.\n",
      "[I 2025-07-16 19:49:22,214] Trial 2 finished with value: 0.3057044515132742 and parameters: {'n_estimators': 1110, 'max_depth': 10, 'learning_rate': 0.011824912923344663, 'subsample': 0.8420187968526013, 'colsample_bytree': 0.6319182021918974}. Best is trial 0 with value: 0.30318617901507333.\n",
      "[I 2025-07-16 19:49:37,285] Trial 3 finished with value: 0.3103183373912327 and parameters: {'n_estimators': 1107, 'max_depth': 4, 'learning_rate': 0.08348167755410026, 'subsample': 0.6952256231235575, 'colsample_bytree': 0.8397490621065337}. Best is trial 0 with value: 0.30318617901507333.\n",
      "[I 2025-07-16 19:49:59,439] Trial 4 finished with value: 0.3079644874651676 and parameters: {'n_estimators': 962, 'max_depth': 8, 'learning_rate': 0.022633696502174467, 'subsample': 0.9922181090542642, 'colsample_bytree': 0.9897160853827132}. Best is trial 0 with value: 0.30318617901507333.\n",
      "[I 2025-07-16 19:50:12,710] Trial 5 finished with value: 0.3020531722227942 and parameters: {'n_estimators': 573, 'max_depth': 7, 'learning_rate': 0.03927531664116429, 'subsample': 0.6134660341495729, 'colsample_bytree': 0.7030430547958316}. Best is trial 5 with value: 0.3020531722227942.\n",
      "[I 2025-07-16 19:50:23,837] Trial 6 finished with value: 0.31015642458728515 and parameters: {'n_estimators': 537, 'max_depth': 4, 'learning_rate': 0.021964970261517225, 'subsample': 0.7522082926302994, 'colsample_bytree': 0.6584729796818934}. Best is trial 5 with value: 0.3020531722227942.\n",
      "[I 2025-07-16 19:50:54,842] Trial 7 finished with value: 0.3172030827963178 and parameters: {'n_estimators': 922, 'max_depth': 10, 'learning_rate': 0.09439476180513785, 'subsample': 0.9511183467985849, 'colsample_bytree': 0.9032506117127286}. Best is trial 5 with value: 0.3020531722227942.\n",
      "[I 2025-07-16 19:51:10,787] Trial 8 finished with value: 0.30139180086322254 and parameters: {'n_estimators': 613, 'max_depth': 8, 'learning_rate': 0.02030253604863135, 'subsample': 0.6732522926493172, 'colsample_bytree': 0.9455661018418445}. Best is trial 8 with value: 0.30139180086322254.\n",
      "[I 2025-07-16 19:51:24,413] Trial 9 finished with value: 0.2994641381685496 and parameters: {'n_estimators': 463, 'max_depth': 8, 'learning_rate': 0.01754999954347204, 'subsample': 0.7209883112489311, 'colsample_bytree': 0.6622399670140559}. Best is trial 9 with value: 0.2994641381685496.\n",
      "[I 2025-07-16 19:51:40,565] Trial 10 finished with value: 0.299819999121502 and parameters: {'n_estimators': 796, 'max_depth': 6, 'learning_rate': 0.010166766039612321, 'subsample': 0.7685468333044272, 'colsample_bytree': 0.7827318447632273}. Best is trial 9 with value: 0.2994641381685496.\n",
      "[I 2025-07-16 19:51:54,952] Trial 11 finished with value: 0.2996207913819358 and parameters: {'n_estimators': 758, 'max_depth': 6, 'learning_rate': 0.010104061154894496, 'subsample': 0.7781463534665515, 'colsample_bytree': 0.76703600745187}. Best is trial 9 with value: 0.2994641381685496.\n",
      "[I 2025-07-16 19:52:09,374] Trial 12 finished with value: 0.29841073230493714 and parameters: {'n_estimators': 768, 'max_depth': 6, 'learning_rate': 0.014742441219301808, 'subsample': 0.8345936063378105, 'colsample_bytree': 0.766894200572058}. Best is trial 12 with value: 0.29841073230493714.\n",
      "[I 2025-07-16 19:52:20,033] Trial 13 finished with value: 0.3009691929483753 and parameters: {'n_estimators': 407, 'max_depth': 6, 'learning_rate': 0.015258198214590854, 'subsample': 0.8499266102940737, 'colsample_bytree': 0.8431549978756204}. Best is trial 12 with value: 0.29841073230493714.\n",
      "[I 2025-07-16 19:52:51,075] Trial 14 finished with value: 0.3041362232693826 and parameters: {'n_estimators': 923, 'max_depth': 9, 'learning_rate': 0.032651947747393155, 'subsample': 0.7185009114172097, 'colsample_bytree': 0.6044027642363986}. Best is trial 12 with value: 0.29841073230493714.\n",
      "[I 2025-07-16 19:53:12,293] Trial 15 finished with value: 0.29805109799443924 and parameters: {'n_estimators': 695, 'max_depth': 7, 'learning_rate': 0.014913926436753957, 'subsample': 0.8169829168707167, 'colsample_bytree': 0.7457366614643987}. Best is trial 15 with value: 0.29805109799443924.\n",
      "[I 2025-07-16 19:53:26,267] Trial 16 finished with value: 0.3024216563218856 and parameters: {'n_estimators': 685, 'max_depth': 5, 'learning_rate': 0.014295545789564823, 'subsample': 0.832986747599443, 'colsample_bytree': 0.7470409963256809}. Best is trial 15 with value: 0.29805109799443924.\n",
      "[I 2025-07-16 19:53:45,039] Trial 17 finished with value: 0.29950441392586 and parameters: {'n_estimators': 734, 'max_depth': 7, 'learning_rate': 0.02697626853812534, 'subsample': 0.9039372900563459, 'colsample_bytree': 0.8277241593458524}. Best is trial 15 with value: 0.29805109799443924.\n",
      "[I 2025-07-16 19:54:00,939] Trial 18 finished with value: 0.3013533104162055 and parameters: {'n_estimators': 857, 'max_depth': 5, 'learning_rate': 0.013803183845133146, 'subsample': 0.803882151284061, 'colsample_bytree': 0.8885939335201476}. Best is trial 15 with value: 0.29805109799443924.\n",
      "[I 2025-07-16 19:54:17,560] Trial 19 finished with value: 0.301709645208242 and parameters: {'n_estimators': 1026, 'max_depth': 5, 'learning_rate': 0.042247785699863025, 'subsample': 0.8950191115152277, 'colsample_bytree': 0.7414141189109288}. Best is trial 15 with value: 0.29805109799443924.\n",
      "[I 2025-07-16 19:54:35,649] Trial 20 finished with value: 0.29924732345594995 and parameters: {'n_estimators': 838, 'max_depth': 7, 'learning_rate': 0.01790697742365661, 'subsample': 0.9389510441650768, 'colsample_bytree': 0.8079036392921618}. Best is trial 15 with value: 0.29805109799443924.\n",
      "[I 2025-07-16 19:54:54,667] Trial 21 finished with value: 0.29981640389831776 and parameters: {'n_estimators': 853, 'max_depth': 7, 'learning_rate': 0.017594545353916646, 'subsample': 0.9440812848908124, 'colsample_bytree': 0.8046243875649943}. Best is trial 15 with value: 0.29805109799443924.\n",
      "[I 2025-07-16 19:55:09,165] Trial 22 finished with value: 0.29750306077736816 and parameters: {'n_estimators': 690, 'max_depth': 6, 'learning_rate': 0.0264455404973215, 'subsample': 0.8107357552502429, 'colsample_bytree': 0.8010232310476227}. Best is trial 22 with value: 0.29750306077736816.\n",
      "[I 2025-07-16 19:55:23,940] Trial 23 finished with value: 0.29814392639343323 and parameters: {'n_estimators': 695, 'max_depth': 6, 'learning_rate': 0.0271677935700943, 'subsample': 0.8130688846707097, 'colsample_bytree': 0.8783463319266023}. Best is trial 22 with value: 0.29750306077736816.\n",
      "[I 2025-07-16 19:55:38,878] Trial 24 finished with value: 0.29813401511413207 and parameters: {'n_estimators': 685, 'max_depth': 6, 'learning_rate': 0.028184820952141518, 'subsample': 0.7905917952383615, 'colsample_bytree': 0.8871913141200166}. Best is trial 22 with value: 0.29750306077736816.\n",
      "[I 2025-07-16 19:55:51,200] Trial 25 finished with value: 0.3001837756554686 and parameters: {'n_estimators': 547, 'max_depth': 5, 'learning_rate': 0.036599030171363726, 'subsample': 0.8768434980089856, 'colsample_bytree': 0.9339372943365492}. Best is trial 22 with value: 0.29750306077736816.\n",
      "[I 2025-07-16 19:56:27,811] Trial 26 finished with value: 0.30236694106807666 and parameters: {'n_estimators': 664, 'max_depth': 7, 'learning_rate': 0.052341988647597086, 'subsample': 0.7898220777740957, 'colsample_bytree': 0.8525840857304708}. Best is trial 22 with value: 0.29750306077736816.\n",
      "[I 2025-07-16 19:57:01,656] Trial 27 finished with value: 0.29854609548767225 and parameters: {'n_estimators': 604, 'max_depth': 6, 'learning_rate': 0.026715543378895392, 'subsample': 0.7398398734946658, 'colsample_bytree': 0.9350969073669131}. Best is trial 22 with value: 0.29750306077736816.\n",
      "[I 2025-07-16 19:57:36,951] Trial 28 finished with value: 0.30478711333741143 and parameters: {'n_estimators': 720, 'max_depth': 9, 'learning_rate': 0.03201529677241101, 'subsample': 0.8635446215057347, 'colsample_bytree': 0.740083121091745}. Best is trial 22 with value: 0.29750306077736816.\n",
      "[I 2025-07-16 19:57:49,456] Trial 29 finished with value: 0.30134957042104793 and parameters: {'n_estimators': 466, 'max_depth': 7, 'learning_rate': 0.04584238322084981, 'subsample': 0.8035941147382, 'colsample_bytree': 0.8669412704687618}. Best is trial 22 with value: 0.29750306077736816.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Final Single Regressor Model with Best Parameters ---\n",
      "Final Single Regressor trained.\n",
      "\n",
      "--- Generating Final Kaggle Submission ---\n",
      "Submission file 'submission_single_regressor.csv' created successfully.\n",
      "   ID  purchaseValue\n",
      "0   0   2.559731e+07\n",
      "1   1   1.676521e+03\n",
      "2   2   0.000000e+00\n",
      "3   3   1.066896e+06\n",
      "4   4   0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Define File Paths ---\n",
    "TRAIN_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/train_data.csv'\n",
    "TEST_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/test_data.csv'\n",
    "\n",
    "\n",
    "class AdvancedFeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['date'] = pd.to_datetime(X_copy['date'], format='%Y%m%d')\n",
    "        X_copy['sessionYear'] = X_copy['date'].dt.year\n",
    "        X_copy['sessionMonth'] = X_copy['date'].dt.month\n",
    "        X_copy['sessionDayOfWeek'] = X_copy['date'].dt.dayofweek\n",
    "        X_copy['sessionHour'] = pd.to_datetime(X_copy['sessionStart'], unit='s').dt.hour\n",
    "        X_copy['is_weekend'] = (X_copy['sessionDayOfWeek'] >= 5).astype(int)\n",
    "        X_copy['month_day_interaction'] = X_copy['sessionMonth'].astype(str) + '_' + X_copy['sessionDayOfWeek'].astype(str)\n",
    "        X_copy['browser_os_interaction'] = X_copy['browser'].astype(str) + '_' + X_copy['os'].astype(str)\n",
    "        X_copy['geo_channel_interaction'] = X_copy['geoNetwork.continent'].astype(str) + '_' + X_copy['userChannel'].astype(str)\n",
    "        X_copy['device_channel_interaction'] = X_copy['deviceType'].astype(str) + '_' + X_copy['userChannel'].astype(str)\n",
    "        X_copy['hits_per_pageview'] = X_copy['totalHits'] / (X_copy['pageViews'] + 1e-6)\n",
    "        X_copy['ad_page_binned'] = X_copy['trafficSource.adwordsClickInfo.page'].apply(lambda p: 1 if p == 1.0 else (2 if pd.notna(p) else 0))\n",
    "        cols_to_drop = ['date', 'sessionStart', 'sessionId', 'trafficSource.adwordsClickInfo.page']\n",
    "        X_copy = X_copy.drop(columns=cols_to_drop, errors='ignore')\n",
    "        return X_copy\n",
    "\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None, smoothing=10):\n",
    "        self.columns, self.smoothing = columns, smoothing\n",
    "        self.mappings_, self.global_mean_ = {}, 0\n",
    "    def fit(self, X, y):\n",
    "        X_fit, y_fit = X.copy(), y.copy()\n",
    "        self.global_mean_ = np.mean(y_fit)\n",
    "        for col in self.columns:\n",
    "            X_fit[col] = X_fit[col].fillna('missing')\n",
    "            agg = y_fit.groupby(X_fit[col]).agg(['mean', 'count'])\n",
    "            smooth_mean = (agg['count'] * agg['mean'] + self.smoothing * self.global_mean_) / (agg['count'] + self.smoothing)\n",
    "            self.mappings_[col] = smooth_mean.to_dict()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transform = X.copy()\n",
    "        for col in self.columns:\n",
    "            X_transform[col] = X_transform[col].fillna('missing')\n",
    "            X_transform[col] = X_transform[col].map(self.mappings_[col]).fillna(self.global_mean_)\n",
    "        return X_transform\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: DATA PREPARATION (Unchanged)\n",
    "# ==============================================================================\n",
    "\n",
    "def create_user_level_features(df_train, df_test):\n",
    "    print(\"Creating user-level features...\")\n",
    "    df_train['purchaseValue'] = df_train['purchaseValue'].fillna(0) / 1e6\n",
    "    df_train['made_purchase'] = (df_train['purchaseValue'] > 0).astype(int)\n",
    "    combined_df = pd.concat([df_train.drop(['purchaseValue', 'made_purchase'], axis=1), df_test], axis=0)\n",
    "    user_aggregates = df_train.groupby('userId').agg(\n",
    "        user_session_count=('sessionId', 'nunique'), user_total_hits=('totalHits', 'sum'),\n",
    "        user_avg_hits=('totalHits', 'mean'), user_total_pageviews=('pageViews', 'sum'),\n",
    "        user_avg_pageviews=('pageViews', 'mean'), user_purchase_count=('made_purchase', 'sum'),\n",
    "        user_total_purchase_value=('purchaseValue', 'sum'),\n",
    "    ).reset_index()\n",
    "    user_aggregates['user_conversion_rate'] = user_aggregates['user_purchase_count'] / user_aggregates['user_session_count']\n",
    "    user_aggregates['user_avg_purchase_value'] = user_aggregates['user_total_purchase_value'] / (user_aggregates['user_purchase_count'] + 1e-6)\n",
    "    df_train = pd.merge(df_train, user_aggregates, on='userId', how='left')\n",
    "    df_test = pd.merge(df_test, user_aggregates, on='userId', how='left')\n",
    "    print(\"User-level features created and merged.\")\n",
    "    return df_train, df_test\n",
    "\n",
    "print(\"Loading and preparing data...\")\n",
    "df_train = pd.read_csv(TRAIN_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "df_test = pd.read_csv(TEST_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "one_value_cols = [col for col in df_train.columns if df_train[col].nunique(dropna=False) == 1]\n",
    "df_train = df_train.drop(columns=one_value_cols)\n",
    "df_test = df_test.drop(columns=[c for c in one_value_cols if c in df_test.columns], errors='ignore')\n",
    "df_train, df_test = create_user_level_features(df_train, df_test)\n",
    "df_train['log_purchaseValue'] = np.log1p(df_train['purchaseValue'])\n",
    "X = df_train.drop(columns=['purchaseValue', 'made_purchase', 'log_purchaseValue'])\n",
    "y = df_train['log_purchaseValue'] # Our target is now just the log value\n",
    "\n",
    "temp_engineered_df = AdvancedFeatureEngineering().fit_transform(X)\n",
    "user_level_numerical = [\n",
    "    'user_session_count', 'user_total_hits', 'user_avg_hits', 'user_total_pageviews',\n",
    "    'user_avg_pageviews', 'user_purchase_count', 'user_total_purchase_value',\n",
    "    'user_conversion_rate', 'user_avg_purchase_value'\n",
    "]\n",
    "session_level_numerical = ['sessionNumber', 'pageViews', 'totalHits', 'hits_per_pageview']\n",
    "numerical_cols = session_level_numerical + user_level_numerical\n",
    "categorical_cols = [col for col in temp_engineered_df.columns if col not in numerical_cols and col != 'userId']\n",
    "print(f\"\\nIdentified {len(numerical_cols)} numerical features.\")\n",
    "print(f\"Identified {len(categorical_cols)} categorical features.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: HYPERPARAMETER TUNING FOR SINGLE REGRESSOR\n",
    "# ==============================================================================\n",
    "\n",
    "def objective_reg(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'random_state': 42, 'n_jobs': -1,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 400, 1200),#2000\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),#16\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),#0.0125\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),#0.85\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),#0.85\n",
    "    }\n",
    "\n",
    "    # Using the ENTIRE dataset for tuning, not just buyers\n",
    "    cv = GroupKFold(n_splits=4)\n",
    "    groups = X_engineered['userId']\n",
    "    y_reg_target = y # y is already defined as df_train['log_purchaseValue']\n",
    "    cv_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_engineered, y_reg_target, groups):\n",
    "        X_train, X_val = X_engineered.iloc[train_idx], X_engineered.iloc[val_idx]\n",
    "        y_train, y_val = y_reg_target.iloc[train_idx], y_reg_target.iloc[val_idx]\n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols),\n",
    "            ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)\n",
    "        ], remainder='drop')\n",
    "\n",
    "        preprocessor.fit(X_train, y_train)\n",
    "        X_train_processed = preprocessor.transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X_train_processed, y_train) # Removed early stopping for compatibility\n",
    "        preds = model.predict(X_val_processed)\n",
    "        cv_scores.append(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# --- Run Optuna Study ---\n",
    "N_TRIALS = 30 # Increased trials slightly for a more thorough search\n",
    "print(f\"\\n--- Tuning Single Regressor with Optuna ({N_TRIALS} trials) ---\")\n",
    "X_engineered = AdvancedFeatureEngineering().fit_transform(X)\n",
    "study_reg = optuna.create_study(direction='minimize')\n",
    "study_reg.optimize(objective_reg, n_trials=N_TRIALS)\n",
    "best_reg_params = study_reg.best_params\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: FINAL MODEL TRAINING AND SUBMISSION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Training Final Single Regressor Model with Best Parameters ---\")\n",
    "# Build and fit the final regressor pipeline on ALL training data\n",
    "final_reg_pipeline = Pipeline([\n",
    "    ('engineering', AdvancedFeatureEngineering()),\n",
    "    ('preprocessing', ColumnTransformer([\n",
    "        ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols),\n",
    "        ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)\n",
    "    ], remainder='drop')),\n",
    "    ('regressor', xgb.XGBRegressor(**best_reg_params, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Train on ALL of X and y (log_purchaseValue)\n",
    "final_reg_pipeline.fit(X, y)\n",
    "print(\"Final Single Regressor trained.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Generating Final Kaggle Submission ---\")\n",
    "try:\n",
    "    # Align columns of test set with train set\n",
    "    train_cols = X.columns\n",
    "    for col in train_cols:\n",
    "        if col not in df_test.columns:\n",
    "            df_test[col] = np.nan\n",
    "    df_test = df_test[train_cols]\n",
    "\n",
    "    # Predict log purchase value directly\n",
    "    kaggle_log_value_pred = final_reg_pipeline.predict(df_test)\n",
    "    \n",
    "    # Transform prediction back to original scale\n",
    "    kaggle_value_pred = np.expm1(kaggle_log_value_pred)\n",
    "    \n",
    "    # Ensure no negative predictions (can happen with model noise)\n",
    "    kaggle_value_pred[kaggle_value_pred < 0] = 0\n",
    "\n",
    "    # Scale back to the original submission format (multiply by 1e6)\n",
    "    kaggle_final_predictions_scaled = kaggle_value_pred * 1e6\n",
    "    \n",
    "    # Create submission file\n",
    "    submission_df = pd.DataFrame({'ID': df_test.index, 'purchaseValue': kaggle_final_predictions_scaled})\n",
    "    submission_df.to_csv('submission_single_regressor.csv', index=False)\n",
    "    print(\"Submission file 'submission_single_regressor.csv' created successfully.\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nKaggle test file at '{TEST_FILE_PATH}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during submission generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f8caf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
