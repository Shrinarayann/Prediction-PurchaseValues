{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f4fbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURATION ---\n",
      "\n",
      "--- DATA LOADING & PREPARATION ---\n",
      "  > Starting feature engineering...\n",
      "  > Feature engineering complete.\n",
      "\n",
      "--- PREPROCESSING SETUP ---\n",
      "\n",
      "--- MODEL TRAINING (K-FOLD ENSEMBLE) ---\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 140\u001b[0m\n\u001b[1;32m    137\u001b[0m oof_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(X))\n\u001b[1;32m    138\u001b[0m gkf \u001b[38;5;241m=\u001b[39m GroupKFold(n_splits\u001b[38;5;241m=\u001b[39mN_SPLITS)\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgkf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muserId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_SPLITS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining Folds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    141\u001b[0m     X_train, X_val \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[train_idx], X\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[1;32m    142\u001b[0m     y_train, y_val \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_idx], y\u001b[38;5;241m.\u001b[39miloc[val_idx]\n",
      "File \u001b[0;32m~/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/tqdm/notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/tqdm/notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# VERSION 6: ROBUST, POWERFUL, FROM-SCRATCH SCRIPT\n",
    "# - Single, powerful Tweedie Regressor.\n",
    "# - Extensive feature engineering (User-level, Lag, Rolling).\n",
    "# - K-Fold ensemble training for stability and performance.\n",
    "# - Pre-selected robust hyperparameters (no Optuna).\n",
    "# - Includes TQDM progress bar.\n",
    "# - Written for maximum library compatibility.\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter, or just tqdm for scripts\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "print(\"--- CONFIGURATION ---\")\n",
    "TRAIN_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/train_data.csv'\n",
    "TEST_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/test_data.csv'\n",
    "N_SPLITS = 5  # Using 5 folds for our robust training\n",
    "# A pre-selected, robust set of hyperparameters for the Tweedie Regressor\n",
    "XGB_PARAMS = {\n",
    "    'objective': 'reg:tweedie',\n",
    "    'tweedie_variance_power': 1.6,\n",
    "    'n_estimators': 1500,\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 5,\n",
    "    'eval_metric': 'rmse',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "# --- 2. FEATURE ENGINEERING FUNCTION ---\n",
    "def create_all_features(df):\n",
    "    \"\"\"Master function to create all features on the combined dataframe.\"\"\"\n",
    "    print(\"  > Starting feature engineering...\")\n",
    "    \n",
    "    # Date & Time Features\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')\n",
    "    df['sessionHour'] = pd.to_datetime(df['sessionStart'], unit='s').dt.hour\n",
    "    df['sessionDayOfWeek'] = df['date'].dt.dayofweek\n",
    "    df['sessionMonth'] = df['date'].dt.month\n",
    "    \n",
    "    # Interaction & Ratio Features\n",
    "    df['hits_per_pageview'] = df['totalHits'] / (df['pageViews'].fillna(0) + 1)\n",
    "    \n",
    "    # User-Level Aggregates\n",
    "    df['userId'] = df['userId'].astype(str)\n",
    "    user_agg = df.groupby('userId').agg(\n",
    "        user_total_hits=('totalHits', 'sum'),\n",
    "        user_avg_hits=('totalHits', 'mean'),\n",
    "        user_session_count=('sessionId', 'nunique'),\n",
    "        user_unique_days=('date', 'nunique')\n",
    "    )\n",
    "    user_agg['user_avg_sessions_per_day'] = user_agg['user_session_count'] / user_agg['user_unique_days']\n",
    "    df = pd.merge(df, user_agg, on='userId', how='left')\n",
    "\n",
    "    # Time-Series Features (Lag & Rolling)\n",
    "    df_sorted = df.sort_values(['userId', 'date', 'sessionStart']).copy()\n",
    "    df_sorted['time_since_last_session'] = df_sorted.groupby('userId')['sessionStart'].diff()\n",
    "    df_sorted['user_rolling_avg_hits_3'] = df_sorted.groupby('userId')['totalHits'].transform(\n",
    "        lambda s: s.rolling(3, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # Drop temporary/redundant columns\n",
    "    cols_to_drop = ['date', 'sessionStart', 'sessionId']\n",
    "    df_sorted = df_sorted.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    print(\"  > Feature engineering complete.\")\n",
    "    gc.collect()\n",
    "    return df_sorted\n",
    "\n",
    "# --- 3. DATA LOADING & PREPARATION ---\n",
    "print(\"\\n--- DATA LOADING & PREPARATION ---\")\n",
    "df_train = pd.read_csv(TRAIN_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "df_test = pd.read_csv(TEST_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "\n",
    "test_indices = df_test.index\n",
    "\n",
    "one_value_cols = [col for col in df_train.columns if df_train[col].nunique(dropna=False) == 1]\n",
    "df_train = df_train.drop(columns=one_value_cols)\n",
    "df_test = df_test.drop(columns=[c for c in one_value_cols if c in df_test.columns], errors='ignore')\n",
    "\n",
    "train_len = len(df_train)\n",
    "combined_df = pd.concat([df_train, df_test], axis=0, sort=False)\n",
    "combined_df_featured = create_all_features(combined_df)\n",
    "\n",
    "# Handle NaNs in the target variable AFTER feature creation\n",
    "combined_df_featured['purchaseValue'] = combined_df_featured['purchaseValue'].fillna(0)\n",
    "\n",
    "# Split back into train and test\n",
    "X = combined_df_featured[:train_len].drop(columns=['purchaseValue'])\n",
    "y = combined_df_featured[:train_len]['purchaseValue'] / 1e6 # Scale target\n",
    "X_test = combined_df_featured[train_len:].drop(columns=['purchaseValue'])\n",
    "\n",
    "# --- 4. PREPROCESSING SETUP ---\n",
    "print(\"\\n--- PREPROCESSING SETUP ---\")\n",
    "# Define column types\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "categorical_cols.remove('userId') # For grouping only\n",
    "\n",
    "# Enforce string type on all categorical columns to prevent mixed-type errors\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].astype(str)\n",
    "    X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "# Create the master preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), numerical_cols),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore')) # Compatible with older sklearn\n",
    "    ]), categorical_cols)\n",
    "], remainder='drop') # Drop any columns not specified (like userId)\n",
    "\n",
    "# --- 5. MODEL TRAINING ---\n",
    "print(\"\\n--- MODEL TRAINING (K-FOLD ENSEMBLE) ---\")\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "oof_predictions = np.zeros(len(X))\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tqdm(gkf.split(X, y, groups=X['userId']), total=N_SPLITS, desc=\"Training Folds\")):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # Fit preprocessor on this fold's training data and transform all sets\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "    \n",
    "    model = xgb.XGBRegressor(**XGB_PARAMS, random_state=42 + fold)\n",
    "    model.fit(X_train_processed, y_train)\n",
    "              \n",
    "    val_preds = model.predict(X_val_processed)\n",
    "    # ROBUSTNESS: Convert any potential NaN predictions from the model to 0\n",
    "    val_preds = np.nan_to_num(val_preds, nan=0.0)\n",
    "    oof_predictions[val_idx] = val_preds\n",
    "    \n",
    "    # Predict on the test set for this fold\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    test_fold_preds = model.predict(X_test_processed)\n",
    "    test_fold_preds = np.nan_to_num(test_fold_preds, nan=0.0)\n",
    "    test_predictions += test_fold_preds / N_SPLITS\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "# --- 6. EVALUATION & SUBMISSION ---\n",
    "print(\"\\n--- EVALUATION & SUBMISSION ---\")\n",
    "oof_r2 = r2_score(y, oof_predictions)\n",
    "print(f\"Overall Out-of-Fold R2 Score: {oof_r2:.5f}\")\n",
    "\n",
    "# Finalize predictions\n",
    "test_predictions[test_predictions < 0] = 0\n",
    "final_predictions_scaled = test_predictions * 1e6\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({'ID': test_indices, 'purchaseValue': final_predictions_scaled})\n",
    "submission_df.to_csv('submission_robust_final.csv', index=False)\n",
    "print(\"Submission file 'submission_robust_final.csv' created successfully.\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b637194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURATION ---\n",
      "\n",
      "--- DATA LOADING & PREPARATION ---\n",
      "  > Starting feature engineering...\n",
      "  > Feature engineering complete.\n",
      "\n",
      "--- PREPROCESSING SETUP ---\n",
      "\n",
      "--- MODEL TRAINING (K-FOLD ENSEMBLE) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:   0%|          | 0/5 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "xgboost.sklearn.XGBRegressor() got multiple values for keyword argument 'random_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 145\u001b[0m\n\u001b[1;32m    142\u001b[0m X_train_processed \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[1;32m    143\u001b[0m X_val_processed \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mtransform(X_val)\n\u001b[0;32m--> 145\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mXGB_PARAMS, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m \u001b[38;5;241m+\u001b[39m fold)\n\u001b[1;32m    146\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_processed, y_train)\n\u001b[1;32m    148\u001b[0m val_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val_processed)\n",
      "\u001b[0;31mTypeError\u001b[0m: xgboost.sklearn.XGBRegressor() got multiple values for keyword argument 'random_state'"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# VERSION 7: FINAL ROBUST SCRIPT\n",
    "# - Uses standard `tqdm` for universal compatibility.\n",
    "# - Single, powerful Tweedie Regressor with robust pre-set parameters.\n",
    "# - Extensive feature engineering.\n",
    "# - K-Fold ensemble training for stability.\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm  # !!!!! MAJOR CHANGE HERE: Using the standard, universal tqdm !!!!!\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "print(\"--- CONFIGURATION ---\")\n",
    "TRAIN_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/train_data.csv'\n",
    "TEST_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/test_data.csv'\n",
    "N_SPLITS = 5  # Using 5 folds for our robust training\n",
    "XGB_PARAMS = {\n",
    "    'objective': 'reg:tweedie',\n",
    "    'tweedie_variance_power': 1.6,\n",
    "    'n_estimators': 1500,\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 5,\n",
    "    'eval_metric': 'rmse',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "# --- 2. FEATURE ENGINEERING FUNCTION ---\n",
    "def create_all_features(df):\n",
    "    \"\"\"Master function to create all features on the combined dataframe.\"\"\"\n",
    "    print(\"  > Starting feature engineering...\")\n",
    "    \n",
    "    # Date & Time Features\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')\n",
    "    df['sessionHour'] = pd.to_datetime(df['sessionStart'], unit='s').dt.hour\n",
    "    df['sessionDayOfWeek'] = df['date'].dt.dayofweek\n",
    "    df['sessionMonth'] = df['date'].dt.month\n",
    "    \n",
    "    # Interaction & Ratio Features\n",
    "    df['hits_per_pageview'] = df['totalHits'] / (df['pageViews'].fillna(0) + 1)\n",
    "    \n",
    "    # User-Level Aggregates\n",
    "    df['userId'] = df['userId'].astype(str)\n",
    "    user_agg = df.groupby('userId').agg(\n",
    "        user_total_hits=('totalHits', 'sum'),\n",
    "        user_avg_hits=('totalHits', 'mean'),\n",
    "        user_session_count=('sessionId', 'nunique'),\n",
    "        user_unique_days=('date', 'nunique')\n",
    "    )\n",
    "    user_agg['user_avg_sessions_per_day'] = user_agg['user_session_count'] / user_agg['user_unique_days']\n",
    "    df = pd.merge(df, user_agg, on='userId', how='left')\n",
    "\n",
    "    # Time-Series Features (Lag & Rolling)\n",
    "    df_sorted = df.sort_values(['userId', 'date', 'sessionStart']).copy()\n",
    "    df_sorted['time_since_last_session'] = df_sorted.groupby('userId')['sessionStart'].diff()\n",
    "    df_sorted['user_rolling_avg_hits_3'] = df_sorted.groupby('userId')['totalHits'].transform(\n",
    "        lambda s: s.rolling(3, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # Drop temporary/redundant columns\n",
    "    cols_to_drop = ['date', 'sessionStart', 'sessionId']\n",
    "    df_sorted = df_sorted.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    print(\"  > Feature engineering complete.\")\n",
    "    gc.collect()\n",
    "    return df_sorted\n",
    "\n",
    "# --- 3. DATA LOADING & PREPARATION ---\n",
    "print(\"\\n--- DATA LOADING & PREPARATION ---\")\n",
    "df_train = pd.read_csv(TRAIN_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "df_test = pd.read_csv(TEST_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "\n",
    "test_indices = df_test.index\n",
    "\n",
    "one_value_cols = [col for col in df_train.columns if df_train[col].nunique(dropna=False) == 1]\n",
    "df_train = df_train.drop(columns=one_value_cols)\n",
    "df_test = df_test.drop(columns=[c for c in one_value_cols if c in df_test.columns], errors='ignore')\n",
    "\n",
    "train_len = len(df_train)\n",
    "combined_df = pd.concat([df_train, df_test], axis=0, sort=False)\n",
    "combined_df_featured = create_all_features(combined_df)\n",
    "\n",
    "# Handle NaNs in the target variable AFTER feature creation\n",
    "combined_df_featured['purchaseValue'] = combined_df_featured['purchaseValue'].fillna(0)\n",
    "\n",
    "# Split back into train and test\n",
    "X = combined_df_featured[:train_len].drop(columns=['purchaseValue'])\n",
    "y = combined_df_featured[:train_len]['purchaseValue'] / 1e6 # Scale target\n",
    "X_test = combined_df_featured[train_len:].drop(columns=['purchaseValue'])\n",
    "\n",
    "# --- 4. PREPROCESSING SETUP ---\n",
    "print(\"\\n--- PREPROCESSING SETUP ---\")\n",
    "# Define column types\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "categorical_cols.remove('userId') # For grouping only\n",
    "\n",
    "# Enforce string type on all categorical columns to prevent mixed-type errors\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].astype(str)\n",
    "    X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "# Create the master preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), numerical_cols),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]), categorical_cols)\n",
    "], remainder='drop') # Drop any columns not specified (like userId)\n",
    "\n",
    "# --- 5. MODEL TRAINING ---\n",
    "print(\"\\n--- MODEL TRAINING (K-FOLD ENSEMBLE) ---\")\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "oof_predictions = np.zeros(len(X))\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tqdm(gkf.split(X, y, groups=X['userId']), total=N_SPLITS, desc=\"Training Folds\")):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # Fit preprocessor on this fold's training data and transform all sets\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "    \n",
    "    model = xgb.XGBRegressor(**XGB_PARAMS, random_state=42 + fold)\n",
    "    model.fit(X_train_processed, y_train)\n",
    "              \n",
    "    val_preds = model.predict(X_val_processed)\n",
    "    # ROBUSTNESS: Convert any potential NaN predictions from the model to 0\n",
    "    val_preds = np.nan_to_num(val_preds, nan=0.0)\n",
    "    oof_predictions[val_idx] = val_preds\n",
    "    \n",
    "    # Predict on the test set for this fold\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    test_fold_preds = model.predict(X_test_processed)\n",
    "    test_fold_preds = np.nan_to_num(test_fold_preds, nan=0.0)\n",
    "    test_predictions += test_fold_preds / N_SPLITS\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "# --- 6. EVALUATION & SUBMISSION ---\n",
    "print(\"\\n--- EVALUATION & SUBMISSION ---\")\n",
    "oof_r2 = r2_score(y, oof_predictions)\n",
    "print(f\"Overall Out-of-Fold R2 Score: {oof_r2:.5f}\")\n",
    "\n",
    "# Finalize predictions\n",
    "test_predictions[test_predictions < 0] = 0\n",
    "final_predictions_scaled = test_predictions * 1e6\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({'ID': test_indices, 'purchaseValue': final_predictions_scaled})\n",
    "submission_df.to_csv('submission_robust_final.csv', index=False)\n",
    "print(\"Submission file 'submission_robust_final.csv' created successfully.\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88722f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURATION ---\n",
      "\n",
      "--- DATA LOADING & PREPARATION ---\n",
      "  > Starting feature engineering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x14d5ea040>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/tqdm/notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Feature engineering complete.\n",
      "\n",
      "--- PREPROCESSING SETUP ---\n",
      "\n",
      "--- MODEL TRAINING (K-FOLD ENSEMBLE) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [01:08<00:00, 13.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EVALUATION & SUBMISSION ---\n",
      "Overall Out-of-Fold R2 Score: 0.05332\n",
      "Submission file 'submission_robust_final.csv' created successfully.\n",
      "   ID  purchaseValue\n",
      "0   0   1.406660e-07\n",
      "1   1   2.875419e-05\n",
      "2   2   1.180671e-05\n",
      "3   3   1.739838e-05\n",
      "4   4   1.378051e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# VERSION 7.1: FINAL ROBUST SCRIPT (Corrected)\n",
    "# - Fixed \"multiple values for keyword argument\" TypeError.\n",
    "# - This is the final, robust, and logically sound version.\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "print(\"--- CONFIGURATION ---\")\n",
    "TRAIN_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/train_data.csv'\n",
    "TEST_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/test_data.csv'\n",
    "N_SPLITS = 5  # Using 5 folds for our robust training\n",
    "\n",
    "# !!!!! MAJOR CHANGE HERE: Removed 'random_state' from the dictionary !!!!!\n",
    "XGB_PARAMS = {\n",
    "    'objective': 'reg:tweedie',\n",
    "    'tweedie_variance_power': 1.6,\n",
    "    'n_estimators': 1500,\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 5,\n",
    "    'eval_metric': 'rmse',\n",
    "    'n_jobs': -1,\n",
    "}\n",
    "\n",
    "# --- 2. FEATURE ENGINEERING FUNCTION ---\n",
    "def create_all_features(df):\n",
    "    \"\"\"Master function to create all features on the combined dataframe.\"\"\"\n",
    "    print(\"  > Starting feature engineering...\")\n",
    "    \n",
    "    # Date & Time Features\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')\n",
    "    df['sessionHour'] = pd.to_datetime(df['sessionStart'], unit='s').dt.hour\n",
    "    df['sessionDayOfWeek'] = df['date'].dt.dayofweek\n",
    "    df['sessionMonth'] = df['date'].dt.month\n",
    "    \n",
    "    # Interaction & Ratio Features\n",
    "    df['hits_per_pageview'] = df['totalHits'] / (df['pageViews'].fillna(0) + 1)\n",
    "    \n",
    "    # User-Level Aggregates\n",
    "    df['userId'] = df['userId'].astype(str)\n",
    "    user_agg = df.groupby('userId').agg(\n",
    "        user_total_hits=('totalHits', 'sum'),\n",
    "        user_avg_hits=('totalHits', 'mean'),\n",
    "        user_session_count=('sessionId', 'nunique'),\n",
    "        user_unique_days=('date', 'nunique')\n",
    "    )\n",
    "    user_agg['user_avg_sessions_per_day'] = user_agg['user_session_count'] / user_agg['user_unique_days']\n",
    "    df = pd.merge(df, user_agg, on='userId', how='left')\n",
    "\n",
    "    # Time-Series Features (Lag & Rolling)\n",
    "    df_sorted = df.sort_values(['userId', 'date', 'sessionStart']).copy()\n",
    "    df_sorted['time_since_last_session'] = df_sorted.groupby('userId')['sessionStart'].diff()\n",
    "    df_sorted['user_rolling_avg_hits_3'] = df_sorted.groupby('userId')['totalHits'].transform(\n",
    "        lambda s: s.rolling(3, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # Drop temporary/redundant columns\n",
    "    cols_to_drop = ['date', 'sessionStart', 'sessionId']\n",
    "    df_sorted = df_sorted.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    print(\"  > Feature engineering complete.\")\n",
    "    gc.collect()\n",
    "    return df_sorted\n",
    "\n",
    "# --- 3. DATA LOADING & PREPARATION ---\n",
    "print(\"\\n--- DATA LOADING & PREPARATION ---\")\n",
    "df_train = pd.read_csv(TRAIN_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "df_test = pd.read_csv(TEST_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "\n",
    "test_indices = df_test.index\n",
    "\n",
    "one_value_cols = [col for col in df_train.columns if df_train[col].nunique(dropna=False) == 1]\n",
    "df_train = df_train.drop(columns=one_value_cols)\n",
    "df_test = df_test.drop(columns=[c for c in one_value_cols if c in df_test.columns], errors='ignore')\n",
    "\n",
    "train_len = len(df_train)\n",
    "combined_df = pd.concat([df_train, df_test], axis=0, sort=False)\n",
    "combined_df_featured = create_all_features(combined_df)\n",
    "\n",
    "# Handle NaNs in the target variable AFTER feature creation\n",
    "combined_df_featured['purchaseValue'] = combined_df_featured['purchaseValue'].fillna(0)\n",
    "\n",
    "# Split back into train and test\n",
    "X = combined_df_featured[:train_len].drop(columns=['purchaseValue'])\n",
    "y = combined_df_featured[:train_len]['purchaseValue'] / 1e6 # Scale target\n",
    "X_test = combined_df_featured[train_len:].drop(columns=['purchaseValue'])\n",
    "\n",
    "# --- 4. PREPROCESSING SETUP ---\n",
    "print(\"\\n--- PREPROCESSING SETUP ---\")\n",
    "# Define column types\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "categorical_cols.remove('userId') # For grouping only\n",
    "\n",
    "# Enforce string type on all categorical columns to prevent mixed-type errors\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].astype(str)\n",
    "    X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "# Create the master preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), numerical_cols),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]), categorical_cols)\n",
    "], remainder='drop') # Drop any columns not specified (like userId)\n",
    "\n",
    "# --- 5. MODEL TRAINING ---\n",
    "print(\"\\n--- MODEL TRAINING (K-FOLD ENSEMBLE) ---\")\n",
    "test_predictions = np.zeros(len(X_test))\n",
    "oof_predictions = np.zeros(len(X))\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tqdm(gkf.split(X, y, groups=X['userId']), total=N_SPLITS, desc=\"Training Folds\")):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # Fit preprocessor on this fold's training data and transform all sets\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "    \n",
    "    # The 'random_state' is now correctly passed only once\n",
    "    model = xgb.XGBRegressor(**XGB_PARAMS, random_state=42 + fold)\n",
    "    model.fit(X_train_processed, y_train)\n",
    "              \n",
    "    val_preds = model.predict(X_val_processed)\n",
    "    val_preds = np.nan_to_num(val_preds, nan=0.0)\n",
    "    oof_predictions[val_idx] = val_preds\n",
    "    \n",
    "    # Predict on the test set for this fold\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    test_fold_preds = model.predict(X_test_processed)\n",
    "    test_fold_preds = np.nan_to_num(test_fold_preds, nan=0.0)\n",
    "    test_predictions += test_fold_preds / N_SPLITS\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "# --- 6. EVALUATION & SUBMISSION ---\n",
    "print(\"\\n--- EVALUATION & SUBMISSION ---\")\n",
    "oof_r2 = r2_score(y, oof_predictions)\n",
    "print(f\"Overall Out-of-Fold R2 Score: {oof_r2:.5f}\")\n",
    "\n",
    "# Finalize predictions\n",
    "test_predictions[test_predictions < 0] = 0\n",
    "final_predictions_scaled = test_predictions * 1e6\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({'ID': test_indices, 'purchaseValue': final_predictions_scaled})\n",
    "submission_df.to_csv('submission_robust_final.csv', index=False)\n",
    "print(\"Submission file 'submission_robust_final.csv' created successfully.\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f919b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
