{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ad0e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "\n",
      "--- Starting Cross-Validation with GroupKFold to Prevent Leakage ---\n",
      "\n",
      "===== FOLD 1/5 =====\n",
      "Training model for this fold...\n",
      "Evaluating model...\n",
      "Fold 1 Validation -> R²: -0.0088 | RMSE: 384,823,578.71 | MAE: 36,009,596.62\n",
      "\n",
      "===== FOLD 2/5 =====\n",
      "Training model for this fold...\n",
      "Evaluating model...\n",
      "Fold 2 Validation -> R²: -0.0286 | RMSE: 143,972,054.07 | MAE: 23,993,710.31\n",
      "\n",
      "===== FOLD 3/5 =====\n",
      "Training model for this fold...\n",
      "Evaluating model...\n",
      "Fold 3 Validation -> R²: -0.0444 | RMSE: 114,073,719.64 | MAE: 23,523,686.19\n",
      "\n",
      "===== FOLD 4/5 =====\n",
      "Training model for this fold...\n",
      "Evaluating model...\n",
      "Fold 4 Validation -> R²: -0.0407 | RMSE: 124,791,395.26 | MAE: 24,687,749.40\n",
      "\n",
      "===== FOLD 5/5 =====\n",
      "Training model for this fold...\n",
      "Evaluating model...\n",
      "Fold 5 Validation -> R²: -0.0378 | RMSE: 128,920,897.59 | MAE: 24,604,742.17\n",
      "\n",
      "\n",
      "--- Overall Cross-Validation Results ---\n",
      "Average R² Score: -0.0321 (± 0.0127)\n",
      "Average RMSE:     179,316,329.05 (± 103,200,497.52)\n",
      "Average MAE:      26,563,896.94 (± 4,741,902.58)\n",
      "\n",
      "Total Out-of-Fold R² Score: -0.0168\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# ----------------------- File Paths -----------------------\n",
    "# --- Use your actual file paths here ---\n",
    "TRAIN_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/train_data.csv'\n",
    "TEST_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/test_data.csv'\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CUSTOM TRANSFORMERS\n",
    "# ==============================================================================\n",
    "\n",
    "class SessionFeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Creates time-based and interaction features at the session level.\n",
    "    This transformer does NOT use the target variable, so it's safe to use early.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        # Date and time features\n",
    "        X_copy['date'] = pd.to_datetime(X_copy['date'], errors='coerce', format='%Y%m%d')\n",
    "        X_copy['sessionYear'] = X_copy['date'].dt.year\n",
    "        X_copy['sessionMonth'] = X_copy['date'].dt.month\n",
    "        X_copy['sessionDayOfWeek'] = X_copy['date'].dt.dayofweek\n",
    "        X_copy['sessionHour'] = pd.to_datetime(X_copy['sessionStart'], unit='s').dt.hour\n",
    "        X_copy['is_weekend'] = (X_copy['sessionDayOfWeek'] >= 5).astype(int)\n",
    "\n",
    "        # Interaction features\n",
    "        X_copy['browser_os_interaction'] = X_copy['browser'].astype(str) + '_' + X_copy['os'].astype(str)\n",
    "        X_copy['geo_channel_interaction'] = X_copy['geoNetwork.continent'].astype(str) + '_' + X_copy['userChannel'].astype(str)\n",
    "\n",
    "        # Ratio features\n",
    "        X_copy['hits_per_pageview'] = X_copy['totalHits'] / (X_copy['pageViews'] + 1e-6)\n",
    "\n",
    "        # Drop original columns that are no longer needed\n",
    "        X_copy = X_copy.drop(columns=['date', 'sessionStart', 'sessionId'], errors='ignore')\n",
    "        return X_copy\n",
    "\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Target encodes categorical features. Uses smoothing to prevent overfitting to rare categories.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=None, smoothing=10):\n",
    "        self.columns = columns\n",
    "        self.smoothing = smoothing\n",
    "        self.mappings_ = {}\n",
    "        self.global_mean_ = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Calculate global mean from the training target data\n",
    "        self.global_mean_ = np.mean(y)\n",
    "\n",
    "        for col in self.columns:\n",
    "            # Create a temporary dataframe for aggregation\n",
    "            df = pd.DataFrame({'feature': X[col], 'target': y})\n",
    "            df['feature'] = df['feature'].fillna('missing')\n",
    "\n",
    "            # Calculate mean and count for each category\n",
    "            agg = df.groupby('feature')['target'].agg(['mean', 'count'])\n",
    "            \n",
    "            # Calculate smoothed mean\n",
    "            smooth_mean = (agg['count'] * agg['mean'] + self.smoothing * self.global_mean_) / (agg['count'] + self.smoothing)\n",
    "            \n",
    "            # Store the mapping\n",
    "            self.mappings_[col] = smooth_mean.to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for col in self.columns:\n",
    "            # Apply the learned mapping\n",
    "            X_transformed[col] = X_transformed[col].fillna('missing').map(self.mappings_[col])\n",
    "            # Fill any new/unseen categories with the global mean\n",
    "            X_transformed[col] = X_transformed[col].fillna(self.global_mean_)\n",
    "        return X_transformed\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DATA LOADING AND INITIAL PREPARATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading and preparing data...\")\n",
    "df_train = pd.read_csv(TRAIN_FILE_PATH, dtype={'userId': 'str', 'sessionId': 'str'})\n",
    "\n",
    "# --- Target Variable Transformation ---\n",
    "# The target is heavily skewed. Using log1p is standard practice.\n",
    "# We predict the log value and convert it back for evaluation.\n",
    "df_train['purchaseValue'] = df_train['purchaseValue'].fillna(0).astype(float)\n",
    "df_train['log_purchaseValue'] = np.log1p(df_train['purchaseValue'])\n",
    "\n",
    "# --- Initial Feature Cleaning ---\n",
    "# Remove columns with only one unique value, as they provide no information\n",
    "one_value_cols = [col for col in df_train.columns if df_train[col].nunique(dropna=False) == 1]\n",
    "df_train = df_train.drop(columns=one_value_cols)\n",
    "\n",
    "# --- Define Features (X), Target (y), and Groups ---\n",
    "X = df_train.drop(columns=['purchaseValue', 'log_purchaseValue'])\n",
    "y = df_train['log_purchaseValue']\n",
    "groups = df_train['userId'] # For GroupKFold\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ROBUST CROSS-VALIDATION AND MODELING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Starting Cross-Validation with GroupKFold to Prevent Leakage ---\")\n",
    "\n",
    "# --- Cross-Validation Setup ---\n",
    "N_SPLITS = 5\n",
    "gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "oof_preds = np.zeros(len(df_train))\n",
    "oof_rmse_scores, oof_r2_scores, oof_mae_scores = [], [], []\n",
    "\n",
    "# --- Fixed Hyperparams for XGBoost ---\n",
    "# In a real project, you would tune these using Optuna/GridSearchCV within the CV loop\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 1500,  # Reduced for faster demonstration\n",
    "    'max_depth': 12,      # Slightly reduced\n",
    "    'learning_rate': 0.015,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=groups)):\n",
    "    print(f\"\\n===== FOLD {fold+1}/{N_SPLITS} =====\")\n",
    "\n",
    "    # --- Split data for this fold ---\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # --- LEAKAGE-PROOF FEATURE ENGINEERING ---\n",
    "    # Step A: Create user-level aggregates using ONLY the training data of this fold\n",
    "    train_df_fold = pd.concat([X_train, y_train], axis=1)\n",
    "    train_df_fold['made_purchase'] = (np.expm1(train_df_fold['log_purchaseValue']) > 0).astype(int)\n",
    "    \n",
    "    user_agg = train_df_fold.groupby('userId').agg(\n",
    "        user_session_count=('sessionId', 'nunique'),\n",
    "        user_avg_hits=('totalHits', 'mean'),\n",
    "        user_total_pageviews=('pageViews', 'sum'),\n",
    "        user_purchase_count=('made_purchase', 'sum')\n",
    "    )\n",
    "    user_agg['user_conversion_rate'] = user_agg['user_purchase_count'] / user_agg['user_session_count']\n",
    "    \n",
    "    # Step B: Merge these safe aggregates onto the train and validation sets\n",
    "    X_train = pd.merge(X_train, user_agg, on='userId', how='left')\n",
    "    X_val = pd.merge(X_val, user_agg, on='userId', how='left')\n",
    "    \n",
    "    # --- Define column types for the pipeline ---\n",
    "    # These must be defined *after* creating the new user-level features\n",
    "    user_level_numerical = list(user_agg.columns)\n",
    "    session_level_numerical = ['sessionNumber', 'pageViews', 'totalHits'] # Base numericals\n",
    "    \n",
    "    # Run session-level feature engineering to get final feature set\n",
    "    temp_engineer = SessionFeatureEngineering()\n",
    "    X_train_engineered = temp_engineer.fit_transform(X_train)\n",
    "    \n",
    "    numerical_cols = session_level_numerical + user_level_numerical + ['hits_per_pageview']\n",
    "    categorical_cols = [\n",
    "        col for col in X_train_engineered.columns \n",
    "        if col not in numerical_cols and col != 'userId' and X_train_engineered[col].dtype == 'object'\n",
    "    ]\n",
    "\n",
    "    # --- Create the Preprocessing and Modeling Pipeline for this Fold ---\n",
    "    # The TargetEncoder is fit ONLY on (X_train, y_train) inside this pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', RobustScaler())\n",
    "            ]), numerical_cols),\n",
    "            ('cat', TargetEncoder(columns=categorical_cols, smoothing=20), categorical_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    model_pipeline = Pipeline([\n",
    "        ('session_eng', SessionFeatureEngineering()),\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('regressor', xgb.XGBRegressor(**xgb_params))\n",
    "    ])\n",
    "\n",
    "    # --- Train the model for this fold ---\n",
    "    print(\"Training model for this fold...\")\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # --- Evaluate on the validation set ---\n",
    "    print(\"Evaluating model...\")\n",
    "    val_preds_log = model_pipeline.predict(X_val)\n",
    "    oof_preds[val_idx] = val_preds_log # Store out-of-fold predictions\n",
    "\n",
    "    # Convert predictions and true values back from log scale to real scale for metrics\n",
    "    val_preds_real = np.expm1(val_preds_log)\n",
    "    val_preds_real[val_preds_real < 0] = 0 # Ensure non-negativity\n",
    "    y_val_real = np.expm1(y_val)\n",
    "\n",
    "    # Calculate and store metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_real, val_preds_real))\n",
    "    r2 = r2_score(y_val_real, val_preds_real)\n",
    "    mae = mean_absolute_error(y_val_real, val_preds_real)\n",
    "    \n",
    "    oof_rmse_scores.append(rmse)\n",
    "    oof_r2_scores.append(r2)\n",
    "    oof_mae_scores.append(mae)\n",
    "\n",
    "    print(f\"Fold {fold+1} Validation -> R²: {r2:.4f} | RMSE: {rmse:,.2f} | MAE: {mae:,.2f}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. FINAL RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\\n--- Overall Cross-Validation Results ---\")\n",
    "print(f\"Average R² Score: {np.mean(oof_r2_scores):.4f} (± {np.std(oof_r2_scores):.4f})\")\n",
    "print(f\"Average RMSE:     {np.mean(oof_rmse_scores):,.2f} (± {np.std(oof_rmse_scores):,.2f})\")\n",
    "print(f\"Average MAE:      {np.mean(oof_mae_scores):,.2f} (± {np.std(oof_mae_scores):,.2f})\")\n",
    "\n",
    "# You can also evaluate the full set of out-of-fold predictions\n",
    "oof_preds_real = np.expm1(oof_preds)\n",
    "oof_preds_real[oof_preds_real < 0] = 0\n",
    "y_real = np.expm1(y)\n",
    "total_oof_r2 = r2_score(y_real, oof_preds_real)\n",
    "print(f\"\\nTotal Out-of-Fold R² Score: {total_oof_r2:.4f}\")\n",
    "\n",
    "# --- FINAL MODEL TRAINING (for submission) ---\n",
    "# After finding the best hyperparameters and features through CV, you would train a final\n",
    "# model on ALL the training data to make predictions on the actual test set.\n",
    "\n",
    "# print(\"\\n--- Training Final Model on All Data ---\")\n",
    "# final_pipeline.fit(X, y) \n",
    "# print(\"Final model ready for test set predictions.\")\n",
    "# test_preds = final_pipeline.predict(X_test)\n",
    "# ... etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810d1e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Creating user-level aggregates from full training data...\n",
      "Applying all features to train and test sets...\n",
      "\n",
      "--- Splitting data for local validation ---\n",
      "Preprocessing data for validation...\n",
      "Training validation model with Early Stopping...\n",
      "Evaluating model on hold-out validation set...\n",
      "\n",
      "--- Validation Set Performance ---\n",
      "R² Score: 0.5462\n",
      "MAE:      6,615,512.16\n",
      "RMSE:     146,460,977.65\n",
      "\n",
      "\n",
      "--- Training Final Model on 100% of the Data ---\n",
      "Preprocessing full training data...\n",
      "Training final model for 756 rounds...\n",
      "Final model trained.\n",
      "\n",
      "--- Generating predictions on the test set ---\n",
      "\n",
      "Submission file created successfully at: 'submission.csv'\n",
      "Top 5 rows of the submission file:\n",
      "   id  purchaseValue\n",
      "0   0   3.376664e+07\n",
      "1   1   2.715588e-04\n",
      "2   2   2.715588e-04\n",
      "3   3   0.000000e+00\n",
      "4   4   2.715588e-04\n"
     ]
    }
   ],
   "source": [
    "#7.1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# ----------------------- File Paths -----------------------\n",
    "TRAIN_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/train_data.csv'\n",
    "TEST_FILE_PATH = '/Users/shrinarayan/Desktop/Prediction-PurchaseValues/dataset/test_data.csv'\n",
    "SUBMISSION_FILE_PATH = 'submission.csv'\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DATA LOADING AND INITIAL PREPARATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df_train = pd.read_csv(TRAIN_FILE_PATH, dtype={'userId': 'str', 'sessionId': 'str'})\n",
    "df_test = pd.read_csv(TEST_FILE_PATH, dtype={'userId': 'str', 'sessionId': 'str'})\n",
    "test_session_ids = df_test['sessionId']\n",
    "\n",
    "# --- Initial Feature Cleaning (Common for Train and Test) ---\n",
    "one_value_cols = [col for col in df_train.columns if df_train[col].nunique(dropna=False) == 1]\n",
    "df_train = df_train.drop(columns=one_value_cols)\n",
    "df_test = df_test.drop(columns=[c for c in one_value_cols if c in df_test.columns], errors='ignore')\n",
    "\n",
    "# --- Target Variable Transformation ---\n",
    "df_train['purchaseValue'] = df_train['purchaseValue'].fillna(0).astype(float)\n",
    "df_train['log_purchaseValue'] = np.log1p(df_train['purchaseValue'])\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. \"COMPETITION-STYLE\" FEATURE ENGINEERING (APPLIED BEFORE SPLITTING)\n",
    "# ==============================================================================\n",
    "\n",
    "def create_features(df, user_agg_map=None):\n",
    "    \"\"\"Applies session-level and user-level features.\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Session-level features\n",
    "    df_copy['date'] = pd.to_datetime(df_copy['date'], errors='coerce', format='%Y%m%d')\n",
    "    df_copy['sessionMonth'] = df_copy['date'].dt.month\n",
    "    df_copy['sessionDayOfWeek'] = df_copy['date'].dt.dayofweek\n",
    "    df_copy['sessionHour'] = pd.to_datetime(df_copy['sessionStart'], unit='s').dt.hour\n",
    "    df_copy['browser_os_interaction'] = df_copy['browser'].astype(str) + '_' + df_copy['os'].astype(str)\n",
    "    df_copy['hits_per_pageview'] = df_copy['totalHits'] / (df_copy['pageViews'].fillna(0) + 1e-6)\n",
    "    \n",
    "    # Merge pre-calculated user-level features\n",
    "    if user_agg_map is not None:\n",
    "        df_copy = pd.merge(df_copy, user_agg_map, on='userId', how='left')\n",
    "\n",
    "    # Drop original/intermediate columns\n",
    "    df_copy = df_copy.drop(columns=['date', 'sessionStart'], errors='ignore')\n",
    "    return df_copy\n",
    "\n",
    "print(\"Creating user-level aggregates from full training data...\")\n",
    "# --- Create User Aggregates from the ENTIRE training set ---\n",
    "df_train['made_purchase'] = (df_train['purchaseValue'] > 0).astype(int)\n",
    "user_aggregates = df_train.groupby('userId').agg(\n",
    "    user_session_count=('sessionId', 'nunique'),\n",
    "    user_total_hits=('totalHits', 'sum'),\n",
    "    user_avg_hits=('totalHits', 'mean'),\n",
    "    user_total_pageviews=('pageViews', 'sum'),\n",
    "    user_avg_pageviews=('pageViews', 'mean'),\n",
    "    user_purchase_count=('made_purchase', 'sum'),\n",
    "    user_total_purchase_value=('purchaseValue', 'sum'),\n",
    ").reset_index()\n",
    "user_aggregates['user_conversion_rate'] = user_aggregates['user_purchase_count'] / user_aggregates['user_session_count']\n",
    "user_aggregates['user_avg_purchase_value'] = user_aggregates['user_total_purchase_value'] / (user_aggregates['user_purchase_count'] + 1e-6)\n",
    "\n",
    "print(\"Applying all features to train and test sets...\")\n",
    "# --- Apply all features to the full train and test sets ---\n",
    "X_full_engineered = create_features(df_train.drop(columns=['purchaseValue', 'log_purchaseValue', 'made_purchase']), user_aggregates)\n",
    "X_test_engineered = create_features(df_test, user_aggregates)\n",
    "y = df_train['log_purchaseValue']\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PIPELINE DEFINITION (PREPROCESSING ONLY)\n",
    "# ==============================================================================\n",
    "\n",
    "# Define column types based on the engineered features\n",
    "numerical_cols = X_full_engineered.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = [col for col in X_full_engineered.select_dtypes(include=['object', 'category']).columns if col not in ['userId', 'sessionId']]\n",
    "\n",
    "# We use a custom TargetEncoder similar to your original implementation\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None, smoothing=10):\n",
    "        self.columns, self.smoothing = columns, smoothing\n",
    "        self.mappings_, self.global_mean_ = {}, 0\n",
    "    def fit(self, X, y):\n",
    "        self.global_mean_ = np.mean(y)\n",
    "        for col in self.columns:\n",
    "            df = pd.DataFrame({'feature': X[col], 'target': y})\n",
    "            agg = df.groupby('feature')['target'].agg(['mean', 'count'])\n",
    "            smooth_mean = (agg['count'] * agg['mean'] + self.smoothing * self.global_mean_) / (agg['count'] + self.smoothing)\n",
    "            self.mappings_[col] = smooth_mean.to_dict()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            X_copy[col] = X_copy[col].fillna('missing').map(self.mappings_).fillna(self.global_mean_)\n",
    "        return X_copy\n",
    "\n",
    "# The pipeline now only handles preprocessing\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('col_transformer', ColumnTransformer([\n",
    "        ('target_encoder', TargetEncoder(columns=categorical_cols), categorical_cols),\n",
    "        ('numerical_scaler', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', RobustScaler())\n",
    "        ]), numerical_cols)\n",
    "    ], remainder='drop'))\n",
    "])\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. VALIDATION ON A HOLD-OUT SET\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Splitting data for local validation ---\")\n",
    "# Split the ALREADY ENGINEERED data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full_engineered, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Preprocessing data for validation...\")\n",
    "X_train_processed = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "X_val_processed = preprocessing_pipeline.transform(X_val)\n",
    "\n",
    "print(\"Training validation model with Early Stopping...\")\n",
    "xgb_params_val = {\n",
    "    'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'random_state': 42,\n",
    "    'n_jobs': -1, 'n_estimators': 2000, 'max_depth': 16, 'learning_rate': 0.0125,\n",
    "    'subsample': 0.85, 'colsample_bytree': 0.85, 'early_stopping_rounds': 50\n",
    "}\n",
    "validation_model = xgb.XGBRegressor(**xgb_params_val)\n",
    "validation_model.fit(X_train_processed, y_train,\n",
    "                     eval_set=[(X_val_processed, y_val)],\n",
    "                     verbose=False)\n",
    "\n",
    "print(\"Evaluating model on hold-out validation set...\")\n",
    "val_preds_log = validation_model.predict(X_val_processed)\n",
    "val_preds_real = np.expm1(val_preds_log)\n",
    "val_preds_real[val_preds_real < 0] = 0\n",
    "y_val_real = np.expm1(y_val)\n",
    "r2 = r2_score(y_val_real, val_preds_real)\n",
    "mae = mean_absolute_error(y_val_real, val_preds_real)\n",
    "rmse = np.sqrt(mean_squared_error(y_val_real, val_preds_real))\n",
    "\n",
    "print(f\"\\n--- Validation Set Performance ---\")\n",
    "print(f\"R² Score: {r2:.4f}\") # THIS SHOULD BE HIGH NOW\n",
    "print(f\"MAE:      {mae:,.2f}\")\n",
    "print(f\"RMSE:     {rmse:,.2f}\")\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 5. TRAIN FINAL MODEL ON ALL DATA AND CREATE SUBMISSION\n",
    "# # ==============================================================================\n",
    "\n",
    "# print(\"\\n\\n--- Training Final Model on 100% of the Data ---\")\n",
    "# print(\"Preprocessing full training data...\")\n",
    "# X_full_processed = preprocessing_pipeline.fit_transform(X_full_engineered, y)\n",
    "\n",
    "# # Use parameters WITHOUT early stopping for the final model\n",
    "# xgb_params_final = {\n",
    "#     'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'random_state': 42,\n",
    "#     'n_jobs': -1, 'n_estimators': validation_model.best_iteration, # Use best iteration from validation\n",
    "#     'max_depth': 16, 'learning_rate': 0.0125, 'subsample': 0.85, 'colsample_bytree': 0.85\n",
    "# }\n",
    "# final_model = xgb.XGBRegressor(**xgb_params_final)\n",
    "# final_model.fit(X_full_processed, y)\n",
    "# print(\"Final model trained.\")\n",
    "\n",
    "# print(\"\\n--- Generating predictions on the test set ---\")\n",
    "# X_test_processed = preprocessing_pipeline.transform(X_test_engineered)\n",
    "# test_preds_log = final_model.predict(X_test_processed)\n",
    "\n",
    "# test_preds_real = np.expm1(test_preds_log)\n",
    "# test_preds_real[test_preds_real < 0] = 0\n",
    "\n",
    "# submission_df = pd.DataFrame({'sessionId': test_session_ids, 'purchaseValue': test_preds_real})\n",
    "# submission_df.to_csv(SUBMISSION_FILE_PATH, index=False)\n",
    "# print(f\"\\nSubmission file created successfully at: '{SUBMISSION_FILE_PATH}'\")\n",
    "# print(\"Top 5 rows of submission file:\")\n",
    "# print(submission_df.head())\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. TRAIN FINAL MODEL ON ALL DATA AND CREATE SUBMISSION (CORRECTED)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\\n--- Training Final Model on 100% of the Data ---\")\n",
    "print(\"Preprocessing full training data...\")\n",
    "# The preprocessing pipeline is already fit on the training data during the validation step.\n",
    "# We will refit it on the ENTIRE training set (X_full_engineered, y) to learn from all data.\n",
    "X_full_processed = preprocessing_pipeline.fit_transform(X_full_engineered, y)\n",
    "\n",
    "# Use parameters WITHOUT early stopping for the final model\n",
    "# Using validation_model.best_iteration ensures we train for the optimal number of rounds.\n",
    "xgb_params_final = {\n",
    "    'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'random_state': 42,\n",
    "    'n_jobs': -1, 'n_estimators': validation_model.best_iteration, \n",
    "    'max_depth': 16, 'learning_rate': 0.0125, 'subsample': 0.85, 'colsample_bytree': 0.85\n",
    "}\n",
    "\n",
    "print(f\"Training final model for {validation_model.best_iteration} rounds...\")\n",
    "final_model = xgb.XGBRegressor(**xgb_params_final)\n",
    "final_model.fit(X_full_processed, y)\n",
    "print(\"Final model trained.\")\n",
    "\n",
    "print(\"\\n--- Generating predictions on the test set ---\")\n",
    "# Preprocess the test data using the pipeline fitted on the full training data\n",
    "X_test_processed = preprocessing_pipeline.transform(X_test_engineered)\n",
    "test_preds_log = final_model.predict(X_test_processed)\n",
    "\n",
    "# Post-processing: Convert from log scale and ensure non-negativity\n",
    "test_preds_real = np.expm1(test_preds_log)\n",
    "test_preds_real[test_preds_real < 0] = 0\n",
    "\n",
    "# --- Create Submission File in the CORRECT Format ---\n",
    "# The 'id' column should be the index of the original test dataframe.\n",
    "# The 'purchaseValue' column must have the correct casing.\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': df_test.index,\n",
    "    'purchaseValue': test_preds_real\n",
    "})\n",
    "\n",
    "submission_df.to_csv(SUBMISSION_FILE_PATH, index=False)\n",
    "print(f\"\\nSubmission file created successfully at: '{SUBMISSION_FILE_PATH}'\")\n",
    "print(\"Top 5 rows of the submission file:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a6a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
