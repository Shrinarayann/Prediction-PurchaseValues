{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790eb5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-24 14:33:27,778] A new study created in memory with name: no-name-ee7a4fca-f155-4b52-b55b-9e32ad939688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Dummy data created as files not found.\n",
      "Creating user-level aggregates from full training data...\n",
      "Applying all features to train and test sets...\n",
      "\n",
      "--- Splitting data for hyperparameter tuning ---\n",
      "Preprocessing data for tuning...\n",
      "\n",
      "--- Starting Hyperparameter Tuning with Optuna ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing Hyperparameters:   0%|          | 0/30 [00:00<?, ?it/s][W 2025-07-24 14:33:27,793] Trial 0 failed with parameters: {'booster': 'dart', 'learning_rate': 0.026646935925302404, 'max_depth': 6, 'subsample': 0.7962727479528825, 'colsample_bytree': 0.5153529832016173, 'gamma': 2.7270689315288854e-06, 'lambda': 0.00017150648028317985, 'alpha': 0.11222788594653742} because of the following error: TypeError(\"fit() got an unexpected keyword argument 'early_stopping_rounds'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/dm/98lvl5s96xn5qcs0j_dnn9l80000gn/T/ipykernel_54457/4253649522.py\", line 150, in objective\n",
      "    model.fit(X_train_processed, y_train,\n",
      "  File \"/Users/shrinarayan/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "TypeError: fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "[W 2025-07-24 14:33:27,806] Trial 0 failed with value None.\n",
      "Optimizing Hyperparameters:   0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 163\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtqdm_callback\u001b[39m(study, trial):\n\u001b[1;32m    162\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtqdm_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Optuna Study Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of finished trials: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/optuna/study/study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/optuna/study/_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/optuna/study/_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/optuna/study/_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    252\u001b[0m ):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/optuna/study/_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[3], line 150\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    138\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_metric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbooster\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbooster\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgbtree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdart\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1e-8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    148\u001b[0m }\n\u001b[1;32m    149\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 150\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m          \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val_processed)\n\u001b[1;32m    154\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_val, preds))\n",
      "File \u001b[0;32m~/Desktop/Prediction-PurchaseValues/venv/lib/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# ----------------------- File Paths -----------------------\n",
    "TRAIN_FILE_PATH = 'train.csv'\n",
    "TEST_FILE_PATH = 'test.csv'\n",
    "SUBMISSION_FILE_PATH = 'submission.csv'\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DATA LOADING AND INITIAL PREPARATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df_train = pd.read_csv(TRAIN_FILE_PATH, dtype={'userId': 'str', 'sessionId': 'str'})\n",
    "    df_test = pd.read_csv(TEST_FILE_PATH, dtype={'userId': 'str', 'sessionId': 'str'})\n",
    "except FileNotFoundError:\n",
    "    print(\"Dummy data created as files not found.\")\n",
    "    train_data = {'date': ['20230101']*1000, 'sessionStart': np.random.randint(1672531200, 1672617600, 1000),\n",
    "                  'browser': np.random.choice(['Chrome', 'Safari', 'Firefox'], 1000), 'os': np.random.choice(['Windows', 'Macintosh', 'Linux'], 1000),\n",
    "                  'totalHits': np.random.randint(1, 50, 1000), 'pageViews': np.random.randint(1, 30, 1000),\n",
    "                  'userId': [f'user_{i}' for i in np.random.randint(0, 100, 1000)], 'sessionId': [f'session_{i}' for i in range(1000)],\n",
    "                  'purchaseValue': [np.random.rand()*100 if np.random.rand() > 0.9 else 0 for _ in range(1000)]}\n",
    "    df_train = pd.DataFrame(train_data)\n",
    "    test_data = train_data.copy()\n",
    "    del test_data['purchaseValue']\n",
    "    test_data['sessionId'] = [f'session_test_{i}' for i in range(1000)]\n",
    "    df_test = pd.DataFrame(test_data)\n",
    "\n",
    "test_session_ids = df_test['sessionId']\n",
    "\n",
    "# --- THIS IS THE CORRECTED LINE ---\n",
    "# Exclude essential columns from being dropped, even if they have only one value\n",
    "essential_cols = ['date', 'sessionStart']\n",
    "one_value_cols = [col for col in df_train.columns if df_train[col].nunique(dropna=False) == 1 and col not in essential_cols]\n",
    "# --- END OF CORRECTION ---\n",
    "\n",
    "df_train = df_train.drop(columns=one_value_cols)\n",
    "df_test = df_test.drop(columns=[c for c in one_value_cols if c in df_test.columns], errors='ignore')\n",
    "\n",
    "df_train['purchaseValue'] = df_train['purchaseValue'].fillna(0).astype(float)\n",
    "df_train['log_purchaseValue'] = np.log1p(df_train['purchaseValue'])\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "\n",
    "def create_features(df, user_agg_map=None):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['date'] = pd.to_datetime(df_copy['date'], errors='coerce', format='%Y%m%d')\n",
    "    df_copy['sessionMonth'] = df_copy['date'].dt.month\n",
    "    df_copy['sessionDayOfWeek'] = df_copy['date'].dt.dayofweek\n",
    "    df_copy['sessionHour'] = pd.to_datetime(df_copy['sessionStart'], unit='s').dt.hour\n",
    "    df_copy['browser_os_interaction'] = df_copy['browser'].astype(str) + '_' + df_copy['os'].astype(str)\n",
    "    df_copy['hits_per_pageview'] = df_copy['totalHits'] / (df_copy['pageViews'].fillna(0) + 1e-6)\n",
    "    if user_agg_map is not None:\n",
    "        df_copy = pd.merge(df_copy, user_agg_map, on='userId', how='left')\n",
    "    df_copy = df_copy.drop(columns=['date', 'sessionStart'], errors='ignore')\n",
    "    return df_copy\n",
    "\n",
    "print(\"Creating user-level aggregates from full training data...\")\n",
    "df_train['made_purchase'] = (df_train['purchaseValue'] > 0).astype(int)\n",
    "user_aggregates = df_train.groupby('userId').agg(\n",
    "    user_session_count=('sessionId', 'nunique'), user_total_hits=('totalHits', 'sum'),\n",
    "    user_avg_hits=('totalHits', 'mean'), user_total_pageviews=('pageViews', 'sum'),\n",
    "    user_avg_pageviews=('pageViews', 'mean'), user_purchase_count=('made_purchase', 'sum'),\n",
    "    user_total_purchase_value=('purchaseValue', 'sum'),\n",
    ").reset_index()\n",
    "user_aggregates['user_conversion_rate'] = user_aggregates['user_purchase_count'] / user_aggregates['user_session_count']\n",
    "user_aggregates['user_avg_purchase_value'] = user_aggregates['user_total_purchase_value'] / (user_aggregates['user_purchase_count'] + 1e-6)\n",
    "\n",
    "print(\"Applying all features to train and test sets...\")\n",
    "X_full_engineered = create_features(df_train.drop(columns=['purchaseValue', 'log_purchaseValue', 'made_purchase']), user_aggregates)\n",
    "X_test_engineered = create_features(df_test, user_aggregates)\n",
    "y = df_train['log_purchaseValue']\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PIPELINE DEFINITION (No changes here)\n",
    "# ==============================================================================\n",
    "numerical_cols = X_full_engineered.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = [col for col in X_full_engineered.select_dtypes(include=['object', 'category']).columns if col not in ['userId', 'sessionId']]\n",
    "\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None, smoothing=10):\n",
    "        self.columns, self.smoothing = columns, smoothing\n",
    "        self.mappings_, self.global_mean_ = {}, 0\n",
    "    def fit(self, X, y):\n",
    "        self.global_mean_ = np.mean(y)\n",
    "        for col in self.columns:\n",
    "            df = pd.DataFrame({'feature': X[col], 'target': y})\n",
    "            agg = df.groupby('feature')['target'].agg(['mean', 'count'])\n",
    "            smooth_mean = (agg['count'] * agg['mean'] + self.smoothing * self.global_mean_) / (agg['count'] + self.smoothing)\n",
    "            self.mappings_[col] = smooth_mean.to_dict()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            X_copy[col] = X_copy[col].fillna('missing').map(self.mappings_).fillna(self.global_mean_)\n",
    "        return X_copy\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('col_transformer', ColumnTransformer([\n",
    "        ('target_encoder', TargetEncoder(columns=categorical_cols), categorical_cols),\n",
    "        ('numerical_scaler', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', RobustScaler())\n",
    "        ]), numerical_cols)\n",
    "    ], remainder='drop'))\n",
    "])\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. HYPERPARAMETER TUNING WITH OPTUNA and TQDM (No changes here)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Splitting data for hyperparameter tuning ---\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full_engineered, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Preprocessing data for tuning...\")\n",
    "X_train_processed = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "X_val_processed = preprocessing_pipeline.transform(X_val)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'n_jobs': -1,\n",
    "        'random_state': 42, 'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "    }\n",
    "    model = xgb.XGBRegressor(n_estimators=2000, **params)\n",
    "    model.fit(X_train_processed, y_train,\n",
    "              eval_set=[(X_val_processed, y_val)],\n",
    "              early_stopping_rounds=50, verbose=False)\n",
    "    preds = model.predict(X_val_processed)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    return rmse\n",
    "\n",
    "print(\"\\n--- Starting Hyperparameter Tuning with Optuna ---\")\n",
    "study = optuna.create_study(direction='minimize')\n",
    "N_TRIALS = 30\n",
    "with tqdm(total=N_TRIALS, desc=\"Optimizing Hyperparameters\") as pbar:\n",
    "    def tqdm_callback(study, trial):\n",
    "        pbar.update(1)\n",
    "    study.optimize(objective, n_trials=N_TRIALS, callbacks=[tqdm_callback])\n",
    "\n",
    "print(f\"\\n--- Optuna Study Complete ---\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"Best trial's RMSE: {study.best_value:.4f}\")\n",
    "print(\"Best trial's parameters: \")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. TRAIN FINAL MODEL WITH BEST PARAMETERS (No changes here)\n",
    "# ==============================================================================\n",
    "print(\"\\n\\n--- Training Final Model on 100% of the Data with Best Parameters ---\")\n",
    "print(\"Preprocessing full training data...\")\n",
    "X_full_processed = preprocessing_pipeline.fit_transform(X_full_engineered, y)\n",
    "\n",
    "best_params = study.best_params\n",
    "final_xgb_params = {\n",
    "    'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'random_state': 42,\n",
    "    'n_jobs': -1, **best_params\n",
    "}\n",
    "\n",
    "print(\"Determining optimal number of estimators with early stopping...\")\n",
    "temp_model = xgb.XGBRegressor(n_estimators=2000, **final_xgb_params)\n",
    "temp_model.fit(X_train_processed, y_train,\n",
    "               eval_set=[(X_val_processed, y_val)],\n",
    "               early_stopping_rounds=50, verbose=False)\n",
    "optimal_n_estimators = temp_model.best_iteration\n",
    "\n",
    "print(f\"Optimal number of estimators found: {optimal_n_estimators}\")\n",
    "final_xgb_params['n_estimators'] = optimal_n_estimators\n",
    "\n",
    "print(f\"Training final model for {optimal_n_estimators} rounds...\")\n",
    "final_model = xgb.XGBRegressor(**final_xgb_params)\n",
    "final_model.fit(X_full_processed, y)\n",
    "print(\"Final model trained.\")\n",
    "\n",
    "print(\"\\n--- Generating predictions on the test set ---\")\n",
    "X_test_processed = preprocessing_pipeline.transform(X_test_engineered)\n",
    "test_preds_log = final_model.predict(X_test_processed)\n",
    "\n",
    "test_preds_real = np.expm1(test_preds_log)\n",
    "test_preds_real[test_preds_real < 0] = 0\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': df_test.index,\n",
    "    'purchaseValue': test_preds_real\n",
    "})\n",
    "\n",
    "submission_df.to_csv(SUBMISSION_FILE_PATH, index=False)\n",
    "print(f\"\\nSubmission file created successfully at: '{SUBMISSION_FILE_PATH}'\")\n",
    "print(\"Top 5 rows of the submission file:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5de216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
