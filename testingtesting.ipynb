{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e08788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================\n",
      "Comparing Robust Model ('submission.csv') vs. Leaky Model ('submission-3.csv')\n",
      "Assuming the Leaky Model's predictions are the 'Ground Truth'...\n",
      "\n",
      "R² Score: 0.3932\n",
      "Mean Squared Error (MSE): 12,569,394,226,424,738.00\n",
      "Mean Absolute Error (MAE): 13,856,449.45\n",
      "=======================================================================\n",
      "\n",
      "What does this R² score mean?\n",
      "The calculated R² score is 0.3932, which is very low.\n",
      "This indicates a very weak correlation between the two models' predictions.\n",
      "\n",
      "Based on this analysis, the Kaggle score for 'submission.csv' will almost certainly be very low, likely close to zero or negative, just as your validation score predicted.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# --- File Paths ---\n",
    "# Assumes the files are in the same directory as the script\n",
    "LEAKY_SUB_PATH = 'submission-3.csv'  # The one that scored 0.69\n",
    "ROBUST_SUB_PATH = 'submission.csv'    # The one you want to evaluate\n",
    "\n",
    "# --- Load the submission files ---\n",
    "try:\n",
    "    df_leaky = pd.read_csv(LEAKY_SUB_PATH)\n",
    "    df_robust = pd.read_csv(ROBUST_SUB_PATH)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Make sure both submission files are in the same directory as this script.\")\n",
    "    exit()\n",
    "\n",
    "# --- Merge the dataframes to align predictions by 'id' ---\n",
    "# This is crucial to ensure we compare the correct rows.\n",
    "df_comparison = pd.merge(df_leaky, df_robust, on='id', suffixes=('_leaky_0.69', '_robust_unknown'))\n",
    "\n",
    "# --- Extract the prediction columns to act as y_true and y_pred ---\n",
    "# We use the high-scoring leaky submission as a proxy for the true values.\n",
    "y_true_proxy = df_comparison['purchaseValue_leaky_0.69']\n",
    "y_pred_robust = df_comparison['purchaseValue_robust_unknown']\n",
    "\n",
    "# --- Calculate the metrics ---\n",
    "# R² Score\n",
    "# This will show how much of the \"variance\" in the good model is explained by the bad model.\n",
    "# A negative score means the model is worse than just predicting the average.\n",
    "r2 = r2_score(y_true_proxy, y_pred_robust)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "# This will be enormous because R² penalizes large errors quadratically.\n",
    "mse = mean_squared_error(y_true_proxy, y_pred_robust)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "# This shows the average dollar amount your prediction was off by.\n",
    "mae = mean_absolute_error(y_true_proxy, y_pred_robust)\n",
    "\n",
    "\n",
    "# --- Print the results ---\n",
    "print(\"=======================================================================\")\n",
    "print(\"Comparing Robust Model ('submission.csv') vs. Leaky Model ('submission-3.csv')\")\n",
    "print(\"Assuming the Leaky Model's predictions are the 'Ground Truth'...\\n\")\n",
    "\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:,.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:,.2f}\")\n",
    "print(\"=======================================================================\\n\")\n",
    "\n",
    "\n",
    "# --- Interpretation of the Results ---\n",
    "print(\"What does this R² score mean?\")\n",
    "if r2 < 0:\n",
    "    print(f\"The calculated R² score is {r2:.4f}, which is NEGATIVE.\")\n",
    "    print(\"This indicates that the robust model's predictions are extremely poor compared to the leaky model.\")\n",
    "    print(\"It performs WORSE than a naive model that simply predicts the average purchase value of the leaky model.\")\n",
    "elif r2 < 0.5:\n",
    "    print(f\"The calculated R² score is {r2:.4f}, which is very low.\")\n",
    "    print(\"This indicates a very weak correlation between the two models' predictions.\")\n",
    "else:\n",
    "    print(f\"The calculated R² score is {r2:.4f}, indicating a moderate to strong correlation.\")\n",
    "\n",
    "print(\"\\nBased on this analysis, the Kaggle score for 'submission.csv' will almost certainly be very low, likely close to zero or negative, just as your validation score predicted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044ed53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb9cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
