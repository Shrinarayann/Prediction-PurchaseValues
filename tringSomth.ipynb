{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47045ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "\n",
      "--- Tuning Classifier with Optuna ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 20:15:28,425] A new study created in memory with name: no-name-5cf6981e-7351-466a-ad9f-9af1ddf872e3\n",
      "[I 2025-07-22 20:15:39,136] Trial 0 finished with value: 0.9895436234394619 and parameters: {'n_estimators': 497, 'max_depth': 5, 'learning_rate': 0.045867062470863355, 'subsample': 0.8646651669650627, 'colsample_bytree': 0.7662024815888056, 'gamma': 5.712279949855512e-08}. Best is trial 0 with value: 0.9895436234394619.\n",
      "[I 2025-07-22 20:15:48,539] Trial 1 finished with value: 0.9891107746816205 and parameters: {'n_estimators': 442, 'max_depth': 5, 'learning_rate': 0.020557432052152273, 'subsample': 0.9862210971509013, 'colsample_bytree': 0.7687252505865382, 'gamma': 1.5415536274811217e-05}. Best is trial 0 with value: 0.9895436234394619.\n",
      "[I 2025-07-22 20:16:00,308] Trial 2 finished with value: 0.9896494527815003 and parameters: {'n_estimators': 498, 'max_depth': 6, 'learning_rate': 0.028312955743009086, 'subsample': 0.7516512280047092, 'colsample_bytree': 0.8916307887417505, 'gamma': 4.7188236644267336e-07}. Best is trial 2 with value: 0.9896494527815003.\n",
      "[I 2025-07-22 20:16:16,193] Trial 3 finished with value: 0.9899708379896138 and parameters: {'n_estimators': 644, 'max_depth': 8, 'learning_rate': 0.03666758412903508, 'subsample': 0.8102205073628125, 'colsample_bytree': 0.7812719112850632, 'gamma': 4.0234011061416905e-08}. Best is trial 3 with value: 0.9899708379896138.\n",
      "[I 2025-07-22 20:16:25,515] Trial 4 finished with value: 0.9894014459588075 and parameters: {'n_estimators': 415, 'max_depth': 5, 'learning_rate': 0.038278299132259934, 'subsample': 0.9626009224180655, 'colsample_bytree': 0.7008791575996708, 'gamma': 0.04048980454316295}. Best is trial 3 with value: 0.9899708379896138.\n",
      "[I 2025-07-22 20:16:38,182] Trial 5 finished with value: 0.9897204812639149 and parameters: {'n_estimators': 511, 'max_depth': 7, 'learning_rate': 0.05862876794889727, 'subsample': 0.7372592801437279, 'colsample_bytree': 0.7025169144926965, 'gamma': 2.2591778299863337e-06}. Best is trial 3 with value: 0.9899708379896138.\n",
      "[I 2025-07-22 20:16:55,428] Trial 6 finished with value: 0.9897906961938725 and parameters: {'n_estimators': 878, 'max_depth': 6, 'learning_rate': 0.036967663650559354, 'subsample': 0.8822297461112102, 'colsample_bytree': 0.8495970750231586, 'gamma': 1.3095841056150257e-07}. Best is trial 3 with value: 0.9899708379896138.\n",
      "[I 2025-07-22 20:17:11,900] Trial 7 finished with value: 0.9898853064616117 and parameters: {'n_estimators': 638, 'max_depth': 8, 'learning_rate': 0.01856418095558083, 'subsample': 0.8336911016437983, 'colsample_bytree': 0.7259403723373427, 'gamma': 1.8437467062942256e-06}. Best is trial 3 with value: 0.9899708379896138.\n",
      "[I 2025-07-22 20:17:30,900] Trial 8 finished with value: 0.9898144157935289 and parameters: {'n_estimators': 796, 'max_depth': 8, 'learning_rate': 0.011675653678430457, 'subsample': 0.7209140397620145, 'colsample_bytree': 0.9966705409000225, 'gamma': 0.004599763378819628}. Best is trial 3 with value: 0.9899708379896138.\n",
      "[I 2025-07-22 20:17:47,408] Trial 9 finished with value: 0.9896115585260153 and parameters: {'n_estimators': 591, 'max_depth': 5, 'learning_rate': 0.0435922221263175, 'subsample': 0.703052958682848, 'colsample_bytree': 0.7287574434170787, 'gamma': 1.565503251736249e-07}. Best is trial 3 with value: 0.9899708379896138.\n",
      "[I 2025-07-22 20:18:10,170] Trial 10 finished with value: 0.9895962000747178 and parameters: {'n_estimators': 744, 'max_depth': 7, 'learning_rate': 0.09469896781636618, 'subsample': 0.80820627439712, 'colsample_bytree': 0.8479381512353841, 'gamma': 0.0004526785224526063}. Best is trial 3 with value: 0.9899708379896138.\n",
      "[I 2025-07-22 20:18:29,490] Trial 11 finished with value: 0.9898551637784019 and parameters: {'n_estimators': 644, 'max_depth': 8, 'learning_rate': 0.017213524518075136, 'subsample': 0.8172163269744993, 'colsample_bytree': 0.7883936057742365, 'gamma': 1.3446563209211026e-08}. Best is trial 3 with value: 0.9899708379896138.\n",
      "[I 2025-07-22 20:18:48,683] Trial 12 finished with value: 0.9899001748800275 and parameters: {'n_estimators': 668, 'max_depth': 8, 'learning_rate': 0.022769656172943152, 'subsample': 0.9115317468717437, 'colsample_bytree': 0.8058107892613745, 'gamma': 1.8752200558323732e-05}. Best is trial 3 with value: 0.9899708379896138.\n",
      "[I 2025-07-22 20:19:06,512] Trial 13 finished with value: 0.9898112651549922 and parameters: {'n_estimators': 808, 'max_depth': 7, 'learning_rate': 0.029805790298301695, 'subsample': 0.924298269937931, 'colsample_bytree': 0.811548997131991, 'gamma': 0.7106500464745629}. Best is trial 3 with value: 0.9899708379896138.\n",
      "[I 2025-07-22 20:19:22,646] Trial 14 finished with value: 0.9895013232868384 and parameters: {'n_estimators': 972, 'max_depth': 4, 'learning_rate': 0.06316814743201776, 'subsample': 0.9098855441888721, 'colsample_bytree': 0.9010419870377226, 'gamma': 4.0395149394575556e-05}. Best is trial 3 with value: 0.9899708379896138.\n",
      "[I 2025-07-22 20:19:40,495] Trial 15 finished with value: 0.9899933897688971 and parameters: {'n_estimators': 697, 'max_depth': 8, 'learning_rate': 0.023502318350237437, 'subsample': 0.7719538182197907, 'colsample_bytree': 0.814857013094328, 'gamma': 0.0002077026649634292}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:19:55,893] Trial 16 finished with value: 0.9896807815577515 and parameters: {'n_estimators': 737, 'max_depth': 7, 'learning_rate': 0.013217437128848347, 'subsample': 0.7744329680000082, 'colsample_bytree': 0.923834246980811, 'gamma': 0.0005126864253442942}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:20:10,290] Trial 17 finished with value: 0.9899209892559284 and parameters: {'n_estimators': 561, 'max_depth': 8, 'learning_rate': 0.02574870520544093, 'subsample': 0.7925358869737946, 'colsample_bytree': 0.8304250650378454, 'gamma': 0.008128268899784161}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:20:29,639] Trial 18 finished with value: 0.9897452640282216 and parameters: {'n_estimators': 834, 'max_depth': 7, 'learning_rate': 0.013851642764759855, 'subsample': 0.7786594914470035, 'colsample_bytree': 0.8825287304079339, 'gamma': 0.0002805662392749859}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:20:45,441] Trial 19 finished with value: 0.9897284146724623 and parameters: {'n_estimators': 691, 'max_depth': 6, 'learning_rate': 0.06690037987823967, 'subsample': 0.8366265341608339, 'colsample_bytree': 0.7534343089166858, 'gamma': 1.0289017618379728e-08}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:21:09,189] Trial 20 finished with value: 0.9896275405766188 and parameters: {'n_estimators': 920, 'max_depth': 8, 'learning_rate': 0.0983141713139741, 'subsample': 0.7633233011637943, 'colsample_bytree': 0.9705207855934994, 'gamma': 3.2276717369584284e-06}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:21:28,968] Trial 21 finished with value: 0.9899425500215897 and parameters: {'n_estimators': 608, 'max_depth': 8, 'learning_rate': 0.02564151309980115, 'subsample': 0.7991307522329437, 'colsample_bytree': 0.8241583312255646, 'gamma': 0.017905963860781714}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:21:44,822] Trial 22 finished with value: 0.9899459682503629 and parameters: {'n_estimators': 620, 'max_depth': 8, 'learning_rate': 0.03335820292518716, 'subsample': 0.8142497189410424, 'colsample_bytree': 0.8149376491528532, 'gamma': 0.324352416846565}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:21:58,322] Trial 23 finished with value: 0.9898665234697458 and parameters: {'n_estimators': 736, 'max_depth': 7, 'learning_rate': 0.033853321904100286, 'subsample': 0.8554032990924147, 'colsample_bytree': 0.7916905048771731, 'gamma': 0.9499192111933447}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:22:13,734] Trial 24 finished with value: 0.989876488861225 and parameters: {'n_estimators': 544, 'max_depth': 8, 'learning_rate': 0.049669834671141715, 'subsample': 0.8261578953979428, 'colsample_bytree': 0.868500449851702, 'gamma': 0.10975119589592795}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:22:32,491] Trial 25 finished with value: 0.9898499967728924 and parameters: {'n_estimators': 723, 'max_depth': 7, 'learning_rate': 0.033551315354157826, 'subsample': 0.7377252537632224, 'colsample_bytree': 0.7848068306079113, 'gamma': 0.0018529618824559176}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:22:48,645] Trial 26 finished with value: 0.9898122324017473 and parameters: {'n_estimators': 626, 'max_depth': 8, 'learning_rate': 0.01627894333884102, 'subsample': 0.8739104310032965, 'colsample_bytree': 0.7471520894014485, 'gamma': 0.10038132107531932}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:23:05,372] Trial 27 finished with value: 0.9897078666343312 and parameters: {'n_estimators': 775, 'max_depth': 6, 'learning_rate': 0.021393707793439735, 'subsample': 0.789996627026628, 'colsample_bytree': 0.8355864823754079, 'gamma': 9.0438533492069e-05}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:23:23,561] Trial 28 finished with value: 0.989885477113176 and parameters: {'n_estimators': 695, 'max_depth': 8, 'learning_rate': 0.05213377000110314, 'subsample': 0.753647509287527, 'colsample_bytree': 0.8107650555885473, 'gamma': 0.0021638887610070175}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:23:36,906] Trial 29 finished with value: 0.9898290228315474 and parameters: {'n_estimators': 572, 'max_depth': 7, 'learning_rate': 0.04111514733531471, 'subsample': 0.8595517454355436, 'colsample_bytree': 0.7733195960538586, 'gamma': 7.629439646112035e-06}. Best is trial 15 with value: 0.9899933897688971.\n",
      "[I 2025-07-22 20:23:36,920] A new study created in memory with name: no-name-8011f822-1772-4b37-bf5b-b790b8133688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuning Regressor with Optuna ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 20:23:46,363] Trial 0 finished with value: 0.9417748036024891 and parameters: {'n_estimators': 864, 'max_depth': 7, 'learning_rate': 0.010884057037511748, 'subsample': 0.7401576999868297, 'colsample_bytree': 0.9043608237512015}. Best is trial 0 with value: 0.9417748036024891.\n",
      "[I 2025-07-22 20:23:52,194] Trial 1 finished with value: 0.9804922913372645 and parameters: {'n_estimators': 672, 'max_depth': 5, 'learning_rate': 0.012723902142819248, 'subsample': 0.9509688684519855, 'colsample_bytree': 0.9585241629560958}. Best is trial 0 with value: 0.9417748036024891.\n",
      "[I 2025-07-22 20:24:13,484] Trial 2 finished with value: 0.8268046482477835 and parameters: {'n_estimators': 966, 'max_depth': 8, 'learning_rate': 0.05827460234722526, 'subsample': 0.7588544733288076, 'colsample_bytree': 0.9329530292556752}. Best is trial 2 with value: 0.8268046482477835.\n",
      "[I 2025-07-22 20:24:21,686] Trial 3 finished with value: 0.9913209019796123 and parameters: {'n_estimators': 443, 'max_depth': 4, 'learning_rate': 0.020218789764949417, 'subsample': 0.7324850144798283, 'colsample_bytree': 0.7848942090021593}. Best is trial 2 with value: 0.8268046482477835.\n",
      "[I 2025-07-22 20:24:31,574] Trial 4 finished with value: 0.9583648245042791 and parameters: {'n_estimators': 556, 'max_depth': 5, 'learning_rate': 0.044928342188695874, 'subsample': 0.972590013640395, 'colsample_bytree': 0.9664939211703572}. Best is trial 2 with value: 0.8268046482477835.\n",
      "[I 2025-07-22 20:24:38,007] Trial 5 finished with value: 0.9545614365384992 and parameters: {'n_estimators': 547, 'max_depth': 5, 'learning_rate': 0.04516062493197666, 'subsample': 0.7833485442246081, 'colsample_bytree': 0.8818128044552096}. Best is trial 2 with value: 0.8268046482477835.\n",
      "[I 2025-07-22 20:24:44,457] Trial 6 finished with value: 0.8778191214418579 and parameters: {'n_estimators': 482, 'max_depth': 8, 'learning_rate': 0.04023523533966519, 'subsample': 0.7840885758810785, 'colsample_bytree': 0.8779750962829351}. Best is trial 2 with value: 0.8268046482477835.\n",
      "[I 2025-07-22 20:24:48,996] Trial 7 finished with value: 0.9819047247308623 and parameters: {'n_estimators': 540, 'max_depth': 5, 'learning_rate': 0.014136312272005059, 'subsample': 0.8102692353085936, 'colsample_bytree': 0.9274196658096051}. Best is trial 2 with value: 0.8268046482477835.\n",
      "[I 2025-07-22 20:24:53,691] Trial 8 finished with value: 0.9668745591554353 and parameters: {'n_estimators': 581, 'max_depth': 5, 'learning_rate': 0.02742268308082495, 'subsample': 0.7644620662137388, 'colsample_bytree': 0.761669983016689}. Best is trial 2 with value: 0.8268046482477835.\n",
      "[I 2025-07-22 20:24:57,488] Trial 9 finished with value: 0.9794386766661423 and parameters: {'n_estimators': 574, 'max_depth': 4, 'learning_rate': 0.035801268090527784, 'subsample': 0.9185963859310854, 'colsample_bytree': 0.7121513298914232}. Best is trial 2 with value: 0.8268046482477835.\n",
      "[I 2025-07-22 20:25:10,029] Trial 10 finished with value: 0.8250191210844261 and parameters: {'n_estimators': 979, 'max_depth': 8, 'learning_rate': 0.07634737495760008, 'subsample': 0.8667169985719515, 'colsample_bytree': 0.9954223335680034}. Best is trial 10 with value: 0.8250191210844261.\n",
      "[I 2025-07-22 20:25:24,871] Trial 11 finished with value: 0.822743432945921 and parameters: {'n_estimators': 994, 'max_depth': 8, 'learning_rate': 0.09388117311825207, 'subsample': 0.8512274286007111, 'colsample_bytree': 0.9989001593392778}. Best is trial 11 with value: 0.822743432945921.\n",
      "[I 2025-07-22 20:25:35,976] Trial 12 finished with value: 0.8394692928741652 and parameters: {'n_estimators': 993, 'max_depth': 7, 'learning_rate': 0.09927459765173625, 'subsample': 0.8724810877824736, 'colsample_bytree': 0.9993681983234108}. Best is trial 11 with value: 0.822743432945921.\n",
      "[I 2025-07-22 20:25:46,097] Trial 13 finished with value: 0.8444966177610704 and parameters: {'n_estimators': 852, 'max_depth': 7, 'learning_rate': 0.0934011242610685, 'subsample': 0.8573537834169136, 'colsample_bytree': 0.9945884836455782}. Best is trial 11 with value: 0.822743432945921.\n",
      "[I 2025-07-22 20:25:58,857] Trial 14 finished with value: 0.8259217409698416 and parameters: {'n_estimators': 852, 'max_depth': 8, 'learning_rate': 0.06884105669818208, 'subsample': 0.8962348631692466, 'colsample_bytree': 0.8179443880588313}. Best is trial 11 with value: 0.822743432945921.\n",
      "[I 2025-07-22 20:26:08,646] Trial 15 finished with value: 0.8547465909147702 and parameters: {'n_estimators': 918, 'max_depth': 7, 'learning_rate': 0.06511607917739173, 'subsample': 0.8202954205157955, 'colsample_bytree': 0.8429789208485917}. Best is trial 11 with value: 0.822743432945921.\n",
      "[I 2025-07-22 20:26:15,123] Trial 16 finished with value: 0.8936862867392678 and parameters: {'n_estimators': 723, 'max_depth': 6, 'learning_rate': 0.0787859042912821, 'subsample': 0.9150383016035452, 'colsample_bytree': 0.9668220503037351}. Best is trial 11 with value: 0.822743432945921.\n",
      "[I 2025-07-22 20:26:24,807] Trial 17 finished with value: 0.8724102839939343 and parameters: {'n_estimators': 764, 'max_depth': 8, 'learning_rate': 0.02709931559196339, 'subsample': 0.7019316475806431, 'colsample_bytree': 0.9307997499528172}. Best is trial 11 with value: 0.822743432945921.\n",
      "[I 2025-07-22 20:26:32,918] Trial 18 finished with value: 0.8957443827386165 and parameters: {'n_estimators': 919, 'max_depth': 6, 'learning_rate': 0.051544579321068026, 'subsample': 0.8300204764347872, 'colsample_bytree': 0.9940145670838694}. Best is trial 11 with value: 0.822743432945921.\n",
      "[I 2025-07-22 20:26:40,013] Trial 19 finished with value: 0.8873485440078223 and parameters: {'n_estimators': 801, 'max_depth': 6, 'learning_rate': 0.07888755601720603, 'subsample': 0.8818241331362818, 'colsample_bytree': 0.8919247332098663}. Best is trial 11 with value: 0.822743432945921.\n",
      "[I 2025-07-22 20:26:48,442] Trial 20 finished with value: 0.8990209302759598 and parameters: {'n_estimators': 655, 'max_depth': 8, 'learning_rate': 0.02045351900689265, 'subsample': 0.8420845447974915, 'colsample_bytree': 0.8504588691896721}. Best is trial 11 with value: 0.822743432945921.\n",
      "[I 2025-07-22 20:27:02,043] Trial 21 finished with value: 0.8224107049192441 and parameters: {'n_estimators': 931, 'max_depth': 8, 'learning_rate': 0.07233743664352678, 'subsample': 0.9009833723665834, 'colsample_bytree': 0.8059307245085688}. Best is trial 21 with value: 0.8224107049192441.\n",
      "[I 2025-07-22 20:27:16,207] Trial 22 finished with value: 0.8445207247834647 and parameters: {'n_estimators': 941, 'max_depth': 7, 'learning_rate': 0.08133630384388647, 'subsample': 0.9319681737049956, 'colsample_bytree': 0.7836240414303816}. Best is trial 21 with value: 0.8224107049192441.\n",
      "[I 2025-07-22 20:27:39,422] Trial 23 finished with value: 0.8339541076327313 and parameters: {'n_estimators': 999, 'max_depth': 8, 'learning_rate': 0.05744939783928221, 'subsample': 0.9957813651812335, 'colsample_bytree': 0.733901039561068}. Best is trial 21 with value: 0.8224107049192441.\n",
      "[I 2025-07-22 20:27:51,362] Trial 24 finished with value: 0.8260623115386754 and parameters: {'n_estimators': 893, 'max_depth': 8, 'learning_rate': 0.09565804758447076, 'subsample': 0.8567289088645114, 'colsample_bytree': 0.8097493324742451}. Best is trial 21 with value: 0.8224107049192441.\n",
      "[I 2025-07-22 20:28:00,133] Trial 25 finished with value: 0.853098227975422 and parameters: {'n_estimators': 808, 'max_depth': 7, 'learning_rate': 0.07267473501414809, 'subsample': 0.8958659729551726, 'colsample_bytree': 0.8520344164650304}. Best is trial 21 with value: 0.8224107049192441.\n",
      "[I 2025-07-22 20:28:14,120] Trial 26 finished with value: 0.8260402723901056 and parameters: {'n_estimators': 959, 'max_depth': 8, 'learning_rate': 0.05587396112042425, 'subsample': 0.8781799772271995, 'colsample_bytree': 0.9515492115385284}. Best is trial 21 with value: 0.8224107049192441.\n",
      "[I 2025-07-22 20:28:24,219] Trial 27 finished with value: 0.8464411837249572 and parameters: {'n_estimators': 899, 'max_depth': 7, 'learning_rate': 0.08597099867037099, 'subsample': 0.9457914982572342, 'colsample_bytree': 0.8227730104465271}. Best is trial 21 with value: 0.8224107049192441.\n",
      "[I 2025-07-22 20:28:40,171] Trial 28 finished with value: 0.8263797460156372 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.06681260106043195, 'subsample': 0.8048518971040441, 'colsample_bytree': 0.9778233310598355}. Best is trial 21 with value: 0.8224107049192441.\n",
      "[I 2025-07-22 20:28:50,238] Trial 29 finished with value: 0.8692232851446797 and parameters: {'n_estimators': 870, 'max_depth': 7, 'learning_rate': 0.0469639977557851, 'subsample': 0.9034827273708176, 'colsample_bytree': 0.9145148767814031}. Best is trial 21 with value: 0.8224107049192441.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Final Models with Best Parameters ---\n",
      "\n",
      "--- Generating Final Kaggle Submission ---\n",
      "\n",
      "An error occurred during submission generation: Cannot save file into a non-existent directory: '/kaggle/working'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 1: ADVANCED FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "\n",
    "class AdvancedFeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Creates a rich set of features for the model.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Date/Time Engineering\n",
    "        X_copy['date'] = pd.to_datetime(X_copy['date'], format='%Y%m%d')\n",
    "        X_copy['sessionYear'] = X_copy['date'].dt.year\n",
    "        X_copy['sessionMonth'] = X_copy['date'].dt.month\n",
    "        X_copy['sessionDayOfWeek'] = X_copy['date'].dt.dayofweek\n",
    "        X_copy['sessionHour'] = pd.to_datetime(X_copy['sessionStart'], unit='s').dt.hour\n",
    "        X_copy['is_weekend'] = (X_copy['sessionDayOfWeek'] >= 5).astype(int)\n",
    "        \n",
    "        # Interaction Features\n",
    "        X_copy['month_day_interaction'] = X_copy['sessionMonth'].astype(str) + '_' + X_copy['sessionDayOfWeek'].astype(str)\n",
    "        X_copy['browser_os_interaction'] = X_copy['browser'].astype(str) + '_' + X_copy['os'].astype(str)\n",
    "        \n",
    "        # Ratio Features (handle division by zero)\n",
    "        X_copy['hits_per_pageview'] = X_copy['totalHits'] / (X_copy['pageViews'] + 1e-6)\n",
    "        \n",
    "        # Binning AdWords Page\n",
    "        X_copy['ad_page_binned'] = X_copy['trafficSource.adwordsClickInfo.page'].apply(\n",
    "            lambda p: 1 if p == 1.0 else (2 if pd.notna(p) else 0)\n",
    "        )\n",
    "        \n",
    "        cols_to_drop = ['date', 'sessionStart', 'userId', 'sessionId', 'trafficSource.adwordsClickInfo.page']\n",
    "        X_copy = X_copy.drop(columns=cols_to_drop, errors='ignore')\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "# TargetEncoder class remains the same\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns; self.mappings_ = {}; self.global_mean_ = 0\n",
    "    def fit(self, X, y):\n",
    "        X_fit, y_fit = X.copy(), y.copy()\n",
    "        self.global_mean_ = np.mean(y_fit)\n",
    "        for col in self.columns:\n",
    "            X_fit[col] = X_fit[col].fillna('missing')\n",
    "            self.mappings_[col] = y_fit.groupby(X_fit[col]).mean().to_dict()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_transform = X.copy()\n",
    "        for col in self.columns:\n",
    "            X_transform[col] = X_transform[col].fillna('missing')\n",
    "            X_transform[col] = X_transform[col].map(self.mappings_[col]).fillna(self.global_mean_)\n",
    "        return X_transform\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 2: DATA PREPARATION\n",
    "# ==============================================================================\n",
    "TRAIN_FILE_PATH = './dataset/train_data.csv'\n",
    "TEST_FILE_PATH = './dataset/test_data.csv'\n",
    "\n",
    "print(\"Loading and preparing data...\")\n",
    "df = pd.read_csv(TRAIN_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "one_value_cols = [col for col in df.columns if df[col].nunique(dropna=False) == 1]\n",
    "df = df.drop(columns=one_value_cols)\n",
    "\n",
    "df['purchaseValue'] = df['purchaseValue'].fillna(0) / 1e6\n",
    "df['made_purchase'] = (df['purchaseValue'] > 0).astype(int)\n",
    "df['log_purchaseValue'] = np.log1p(df['purchaseValue'])\n",
    "\n",
    "X = df.drop(columns=['purchaseValue', 'made_purchase', 'log_purchaseValue'])\n",
    "y = df[['made_purchase', 'log_purchaseValue']]\n",
    "\n",
    "# --- Define Column Groups for Preprocessing ---\n",
    "temp_engineered_df = AdvancedFeatureEngineering().fit_transform(X)\n",
    "numerical_cols = ['sessionNumber', 'pageViews', 'totalHits', 'sessionYear', 'sessionMonth', 'sessionDayOfWeek', 'sessionHour', 'hits_per_pageview']\n",
    "categorical_cols = [col for col in temp_engineered_df.columns if col not in numerical_cols]\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 3: HYPERPARAMETER TUNING WITH OPTUNA\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Tuning the Classifier ---\n",
    "print(\"\\n--- Tuning Classifier with Optuna ---\")\n",
    "X_engineered = AdvancedFeatureEngineering().fit_transform(X)\n",
    "y_clf_target = y['made_purchase']\n",
    "\n",
    "def objective_clf(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic', 'eval_metric': 'logloss', 'random_state': 42,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "    }\n",
    "    \n",
    "    cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_engineered):\n",
    "        X_train, X_val = X_engineered.iloc[train_idx], X_engineered.iloc[val_idx]\n",
    "        y_train, y_val = y_clf_target.iloc[train_idx], y_clf_target.iloc[val_idx]\n",
    "\n",
    "        preprocessor = ColumnTransformer([('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols), ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)], remainder='drop')\n",
    "        \n",
    "        preprocessor.fit(X_train, y_train)\n",
    "        X_train_processed = preprocessor.transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        preds = model.predict_proba(X_val_processed)[:, 1]\n",
    "        cv_scores.append(roc_auc_score(y_val, preds))\n",
    "        \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study_clf = optuna.create_study(direction='maximize')\n",
    "study_clf.optimize(objective_clf, n_trials=30) # Run 30 trials\n",
    "best_clf_params = study_clf.best_params\n",
    "\n",
    "# --- Tuning the Regressor ---\n",
    "print(\"\\n--- Tuning Regressor with Optuna ---\")\n",
    "X_buyers_engineered = X_engineered[y['made_purchase'] == 1]\n",
    "y_reg_target = y.loc[y['made_purchase'] == 1, 'log_purchaseValue']\n",
    "\n",
    "def objective_reg(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'random_state': 42,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_buyers_engineered):\n",
    "        X_train, X_val = X_buyers_engineered.iloc[train_idx], X_buyers_engineered.iloc[val_idx]\n",
    "        y_train, y_val = y_reg_target.iloc[train_idx], y_reg_target.iloc[val_idx]\n",
    "\n",
    "        preprocessor = ColumnTransformer([('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols), ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)], remainder='drop')\n",
    "\n",
    "        preprocessor.fit(X_train, y_train)\n",
    "        X_train_processed = preprocessor.transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        preds = model.predict(X_val_processed)\n",
    "        cv_scores.append(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study_reg = optuna.create_study(direction='minimize')\n",
    "study_reg.optimize(objective_reg, n_trials=30) # Run 30 trials\n",
    "best_reg_params = study_reg.best_params\n",
    "\n",
    "# ==============================================================================\n",
    "# PART 4: FINAL MODEL TRAINING AND SUBMISSION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Training Final Models with Best Parameters ---\")\n",
    "# Build and fit the final classifier pipeline on ALL training data\n",
    "final_clf_pipeline = Pipeline([\n",
    "    ('engineering', AdvancedFeatureEngineering()),\n",
    "    ('preprocessing', ColumnTransformer([('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols), ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)], remainder='drop')),\n",
    "    ('classifier', xgb.XGBClassifier(**best_clf_params, random_state=42))\n",
    "])\n",
    "final_clf_pipeline.fit(X, y['made_purchase'])\n",
    "\n",
    "# Build and fit the final regressor pipeline on ALL buyer data\n",
    "final_reg_pipeline = Pipeline([\n",
    "    ('engineering', AdvancedFeatureEngineering()),\n",
    "    ('preprocessing', ColumnTransformer([('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), numerical_cols), ('cat', TargetEncoder(columns=categorical_cols), categorical_cols)], remainder='drop')),\n",
    "    ('regressor', xgb.XGBRegressor(**best_reg_params, random_state=42))\n",
    "])\n",
    "final_reg_pipeline.fit(X[y['made_purchase'] == 1], y.loc[y['made_purchase'] == 1, 'log_purchaseValue'])\n",
    "\n",
    "print(\"\\n--- Generating Final Kaggle Submission ---\")\n",
    "try:\n",
    "    kaggle_test_df = pd.read_csv(TEST_FILE_PATH, dtype={'fullVisitorId': 'str'})\n",
    "    \n",
    "    kaggle_prob_purchase = final_clf_pipeline.predict_proba(kaggle_test_df)[:, 1]\n",
    "    kaggle_log_value_pred = final_reg_pipeline.predict(kaggle_test_df)\n",
    "    \n",
    "    kaggle_value_pred = np.expm1(kaggle_log_value_pred)\n",
    "    kaggle_final_predictions_dollars = kaggle_prob_purchase * kaggle_value_pred\n",
    "    kaggle_final_predictions_dollars[kaggle_final_predictions_dollars < 0] = 0\n",
    "    \n",
    "    kaggle_final_predictions_scaled = kaggle_final_predictions_dollars * 1e6\n",
    "    \n",
    "    # ðŸ‘‡ Changed 'ID' to lowercase 'id'\n",
    "    submission_df = pd.DataFrame({'id': kaggle_test_df.index, 'purchaseValue': kaggle_final_predictions_scaled})\n",
    "    \n",
    "    # Save to /kaggle/working explicitly for Kaggle\n",
    "    submission_df.to_csv('/kaggle/working/submission_final.csv', index=False)\n",
    "    print(\"âœ… Submission file 'submission_final.csv' created successfully.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nKaggle '{TEST_FILE_PATH}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during submission generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf419bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
