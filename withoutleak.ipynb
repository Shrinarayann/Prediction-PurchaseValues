{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ecbf722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "--- Splitting data for local validation (BEFORE feature engineering) ---\n",
      "Creating non-leaky user-level aggregates...\n",
      "Preprocessing data for validation...\n",
      "Training validation model with Early Stopping...\n",
      "Evaluating model on hold-out validation set...\n",
      "\n",
      "--- Validation Set Performance (Robust) ---\n",
      "R² Score: 0.0198\n",
      "MAE:      27,007,723.83\n",
      "RMSE:     215,249,181.42\n",
      "\n",
      "\n",
      "--- Training Final Model on 100% of the Data ---\n",
      "Creating non-leaky user-level aggregates...\n",
      "Preprocessing full training and test data...\n",
      "Training final model for 235 rounds...\n",
      "Final model trained.\n",
      "\n",
      "--- Generating predictions on the test set ---\n",
      "\n",
      "Submission file created successfully at: 'submission.csv'\n",
      "Top 5 rows of the submission file:\n",
      "   id  purchaseValue\n",
      "0   0   6.442287e+06\n",
      "1   1   8.692483e-01\n",
      "2   2   3.447260e-02\n",
      "3   3   0.000000e+00\n",
      "4   4   1.154217e+00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# ----------------------- File Paths -----------------------\n",
    "TRAIN_FILE_PATH = './dataset/train_data.csv'\n",
    "TEST_FILE_PATH = './dataset/test_data.csv'\n",
    "SUBMISSION_FILE_PATH = 'submission.csv'\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DATA LOADING AND INITIAL PREPARATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df_train = pd.read_csv(TRAIN_FILE_PATH, dtype={'userId': 'str', 'sessionId': 'str'})\n",
    "df_test = pd.read_csv(TEST_FILE_PATH, dtype={'userId': 'str', 'sessionId': 'str'})\n",
    "test_session_ids = df_test['sessionId'] # Storing for potential use later, although id is index now\n",
    "\n",
    "# --- Initial Feature Cleaning (Common for Train and Test) ---\n",
    "one_value_cols = [col for col in df_train.columns if df_train[col].nunique(dropna=False) == 1]\n",
    "df_train = df_train.drop(columns=one_value_cols)\n",
    "df_test = df_test.drop(columns=[c for c in one_value_cols if c in df_test.columns], errors='ignore')\n",
    "\n",
    "# --- Target Variable Transformation ---\n",
    "df_train['purchaseValue'] = df_train['purchaseValue'].fillna(0).astype(float)\n",
    "df_train['log_purchaseValue'] = np.log1p(df_train['purchaseValue'])\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. FEATURE ENGINEERING (REVISED & ROBUST)\n",
    "# ==============================================================================\n",
    "\n",
    "def create_user_behavior_aggregates(df):\n",
    "    \"\"\"\n",
    "    Creates user-level aggregates based on BEHAVIOR, not purchase value.\n",
    "    This prevents target leakage.\n",
    "    \"\"\"\n",
    "    print(\"Creating non-leaky user-level aggregates...\")\n",
    "    user_aggregates = df.groupby('userId').agg(\n",
    "        user_session_count=('sessionId', 'nunique'),\n",
    "        user_total_hits=('totalHits', 'sum'),\n",
    "        user_avg_hits_per_session=('totalHits', 'mean'),\n",
    "        user_total_pageviews=('pageViews', 'sum'),\n",
    "        user_avg_pageviews_per_session=('pageViews', 'mean'),\n",
    "        user_unique_days_visited=('date', 'nunique'),\n",
    "        user_first_session_ts=('sessionStart', pd.Series.min),\n",
    "        user_last_session_ts=('sessionStart', pd.Series.max),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate time-based features for the user\n",
    "    user_aggregates['user_activity_span_seconds'] = user_aggregates['user_last_session_ts'] - user_aggregates['user_first_session_ts']\n",
    "    user_aggregates = user_aggregates.drop(columns=['user_first_session_ts', 'user_last_session_ts'])\n",
    "    \n",
    "    return user_aggregates\n",
    "\n",
    "def create_session_and_interaction_features(df, user_agg_map=None):\n",
    "    \"\"\"\n",
    "    Applies session-level features and new interaction features.\n",
    "    Merges pre-calculated user-level aggregates.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Fill NA in key columns for safe string operations\n",
    "    for col in ['browser', 'os', 'userChannel', 'deviceType', 'geoNetwork.continent']:\n",
    "        df_copy[col] = df_copy[col].fillna('missing')\n",
    "\n",
    "    # Session-level time features\n",
    "    df_copy['date'] = pd.to_datetime(df_copy['date'], errors='coerce', format='%Y%m%d')\n",
    "    df_copy['sessionMonth'] = df_copy['date'].dt.month\n",
    "    df_copy['sessionDayOfWeek'] = df_copy['date'].dt.dayofweek\n",
    "    df_copy['sessionHour'] = pd.to_datetime(df_copy['sessionStart'], unit='s').dt.hour\n",
    "    \n",
    "    # Session-level engagement features\n",
    "    df_copy['hits_per_pageview'] = df_copy['totalHits'] / (df_copy['pageViews'].fillna(0) + 1e-6)\n",
    "    \n",
    "    # --- NEW INTERACTION FEATURES ---\n",
    "    df_copy['browser_os_interaction'] = df_copy['browser'].astype(str) + '_' + df_copy['os'].astype(str)\n",
    "    df_copy['channel_device_interaction'] = df_copy['userChannel'].astype(str) + '_' + df_copy['deviceType'].astype(str)\n",
    "    df_copy['continent_browser_interaction'] = df_copy['geoNetwork.continent'].astype(str) + '_' + df_copy['browser'].astype(str)\n",
    "    \n",
    "    # Merge pre-calculated user-level features\n",
    "    if user_agg_map is not None:\n",
    "        df_copy = pd.merge(df_copy, user_agg_map, on='userId', how='left')\n",
    "        \n",
    "        # --- NEW: Features comparing current session to user's history ---\n",
    "        df_copy['session_hits_vs_user_avg'] = df_copy['totalHits'] - df_copy['user_avg_hits_per_session']\n",
    "        df_copy['session_pvs_vs_user_avg'] = df_copy['pageViews'] - df_copy['user_avg_pageviews_per_session']\n",
    "\n",
    "\n",
    "    # Drop original/intermediate columns\n",
    "    df_copy = df_copy.drop(columns=['date', 'sessionStart'], errors='ignore')\n",
    "    return df_copy\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PIPELINE DEFINITION (PREPROCESSING ONLY)\n",
    "# ==============================================================================\n",
    "\n",
    "# Custom TargetEncoder remains the same - it's a great way to handle categoricals\n",
    "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None, smoothing=10):\n",
    "        self.columns, self.smoothing = columns, smoothing\n",
    "        self.mappings_, self.global_mean_ = {}, 0\n",
    "    def fit(self, X, y):\n",
    "        self.global_mean_ = np.mean(y)\n",
    "        for col in self.columns:\n",
    "            df = pd.DataFrame({'feature': X[col], 'target': y})\n",
    "            agg = df.groupby('feature')['target'].agg(['mean', 'count'])\n",
    "            smooth_mean = (agg['count'] * agg['mean'] + self.smoothing * self.global_mean_) / (agg['count'] + self.smoothing)\n",
    "            self.mappings_[col] = smooth_mean.to_dict()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            # Handle potential missing values during transform\n",
    "            X_copy[col] = X_copy[col].fillna('missing').map(self.mappings_).fillna(self.global_mean_)\n",
    "        return X_copy\n",
    "\n",
    "# The preprocessing pipeline definition will be instantiated later, after we know the column names.\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. VALIDATION ON A HOLD-OUT SET (CORRECTED PROCESS)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Splitting data for local validation (BEFORE feature engineering) ---\")\n",
    "# Split the ORIGINAL dataframe to prevent any data leakage during feature creation\n",
    "train_df, val_df = train_test_split(\n",
    "    df_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Feature Engineering on the Split Data ---\n",
    "# 1. Create user aggregates *only from the training part*\n",
    "user_aggregates_val = create_user_behavior_aggregates(train_df)\n",
    "\n",
    "# 2. Apply all features to the train and validation sets\n",
    "X_train = create_session_and_interaction_features(train_df.drop(columns=['purchaseValue', 'log_purchaseValue']), user_aggregates_val)\n",
    "X_val = create_session_and_interaction_features(val_df.drop(columns=['purchaseValue', 'log_purchaseValue']), user_aggregates_val)\n",
    "y_train, y_val = train_df['log_purchaseValue'], val_df['log_purchaseValue']\n",
    "\n",
    "# Define column types based on the engineered features\n",
    "numerical_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = [col for col in X_train.select_dtypes(include=['object', 'category']).columns if col not in ['userId', 'sessionId']]\n",
    "\n",
    "# Instantiate and fit the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('col_transformer', ColumnTransformer([\n",
    "        ('target_encoder', TargetEncoder(columns=categorical_cols), categorical_cols),\n",
    "        ('numerical_scaler', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', RobustScaler())\n",
    "        ]), numerical_cols)\n",
    "    ], remainder='drop'))\n",
    "])\n",
    "\n",
    "print(\"Preprocessing data for validation...\")\n",
    "X_train_processed = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "X_val_processed = preprocessing_pipeline.transform(X_val)\n",
    "\n",
    "print(\"Training validation model with Early Stopping...\")\n",
    "xgb_params_val = {\n",
    "    'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'random_state': 42,\n",
    "    'n_jobs': -1, 'n_estimators': 2000, 'max_depth': 8, 'learning_rate': 0.02, # Slightly more robust params\n",
    "    'subsample': 0.8, 'colsample_bytree': 0.8, 'early_stopping_rounds': 50\n",
    "}\n",
    "validation_model = xgb.XGBRegressor(**xgb_params_val)\n",
    "validation_model.fit(X_train_processed, y_train,\n",
    "                     eval_set=[(X_val_processed, y_val)],\n",
    "                     verbose=False)\n",
    "\n",
    "print(\"Evaluating model on hold-out validation set...\")\n",
    "val_preds_log = validation_model.predict(X_val_processed)\n",
    "val_preds_real = np.expm1(val_preds_log)\n",
    "val_preds_real[val_preds_real < 0] = 0\n",
    "y_val_real = np.expm1(y_val)\n",
    "r2 = r2_score(y_val_real, val_preds_real)\n",
    "mae = mean_absolute_error(y_val_real, val_preds_real)\n",
    "rmse = np.sqrt(mean_squared_error(y_val_real, val_preds_real))\n",
    "\n",
    "print(f\"\\n--- Validation Set Performance (Robust) ---\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"MAE:      {mae:,.2f}\")\n",
    "print(f\"RMSE:     {rmse:,.2f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. TRAIN FINAL MODEL ON ALL DATA AND CREATE SUBMISSION \n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\\n--- Training Final Model on 100% of the Data ---\")\n",
    "# 1. Create user aggregates from the ENTIRE training set for maximum information\n",
    "user_aggregates_full = create_user_behavior_aggregates(df_train)\n",
    "\n",
    "# 2. Apply all features to the full train and test sets\n",
    "X_full_engineered = create_session_and_interaction_features(df_train.drop(columns=['purchaseValue', 'log_purchaseValue']), user_aggregates_full)\n",
    "X_test_engineered = create_session_and_interaction_features(df_test, user_aggregates_full)\n",
    "y = df_train['log_purchaseValue']\n",
    "\n",
    "# 3. Re-fit the preprocessing pipeline on ALL training data\n",
    "print(\"Preprocessing full training and test data...\")\n",
    "X_full_processed = preprocessing_pipeline.fit_transform(X_full_engineered, y)\n",
    "X_test_processed = preprocessing_pipeline.transform(X_test_engineered)\n",
    "\n",
    "# Use parameters WITHOUT early stopping for the final model\n",
    "# Using validation_model.best_iteration ensures we train for the optimal number of rounds.\n",
    "final_n_estimators = validation_model.best_iteration if validation_model.best_iteration > 0 else 500\n",
    "xgb_params_final = {\n",
    "    'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'random_state': 42,\n",
    "    'n_jobs': -1, 'n_estimators': final_n_estimators, \n",
    "    'max_depth': 8, 'learning_rate': 0.02, 'subsample': 0.8, 'colsample_bytree': 0.8\n",
    "}\n",
    "\n",
    "print(f\"Training final model for {final_n_estimators} rounds...\")\n",
    "final_model = xgb.XGBRegressor(**xgb_params_final)\n",
    "final_model.fit(X_full_processed, y)\n",
    "print(\"Final model trained.\")\n",
    "\n",
    "print(\"\\n--- Generating predictions on the test set ---\")\n",
    "test_preds_log = final_model.predict(X_test_processed)\n",
    "\n",
    "# Post-processing: Convert from log scale and ensure non-negativity\n",
    "test_preds_real = np.expm1(test_preds_log)\n",
    "test_preds_real[test_preds_real < 0] = 0\n",
    "\n",
    "# Creating submission file in the specified format\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': df_test.index,\n",
    "    'purchaseValue': test_preds_real\n",
    "})\n",
    "\n",
    "submission_df.to_csv(SUBMISSION_FILE_PATH, index=False)\n",
    "print(f\"\\nSubmission file created successfully at: '{SUBMISSION_FILE_PATH}'\")\n",
    "print(\"Top 5 rows of the submission file:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a57ed52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for user overlap...\n",
      "Number of users in Train: 100499\n",
      "Number of users in Test: 27657\n",
      "Number of users in BOTH Train and Test: 5879\n",
      "Overlap Percentage: 21.26%\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for user overlap...\")\n",
    "train_users = set(df_train['userId'])\n",
    "test_users = set(df_test['userId'])\n",
    "overlap_count = len(train_users.intersection(test_users))\n",
    "print(f\"Number of users in Train: {len(train_users)}\")\n",
    "print(f\"Number of users in Test: {len(test_users)}\")\n",
    "print(f\"Number of users in BOTH Train and Test: {overlap_count}\")\n",
    "print(f\"Overlap Percentage: {100 * overlap_count / len(test_users):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4d0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
